---
Title: What constrains distributional learning in adults?
Author: Dave Kleinschmidt
bibliography: /home/dave/Zotero/library.bib
output:
    html_document:
        code_folding: hide
        dev: png
        fig_retina: 3
        keep_md: true
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-crossref
    pdf_document:
        md_extensions: +implicit_figures
        keep_tex: true
        pandoc_args:
        - --filter
        - pandoc-crossref
---


```{r knitr-setup, warning=FALSE, message=FALSE, error=FALSE, echo=FALSE, results='hide'}

library(knitr)
opts_chunk$set(warning = FALSE,
               message = FALSE,
               error = FALSE,
               cache=TRUE,
               echo=opts_knit$get("rmarkdown.pandoc.to") != 'latex')

options(digits=2)

## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})
## Produce markdown-formatted table captions with anchors for cross-refs.
## Requires pandoc-tablenos to parse the IDs. Refer to tables
## in text with {@tbl:label} or @tbl:label.
## Based partly on http://stackoverflow.com/a/18672268
##
## (see https://github.com/tomduck/pandoc-tablenos)
knit_hooks$set(tbl.cap = function(before, options, envir) {
  if(!before){
    paste0('\n\nTable: ', options$tbl.cap,
           ' {#tbl:', options$label, '}', sep = '')
  }
})

```

```{r preamble, cache=FALSE}

library(tidyverse)
library(beliefupdatr)
library(supunsup)

library(rstan)
library(brms)
library(tidybayes)

library(cowplot)
theme_set(cowplot::theme_cowplot() +
            theme(plot.title=element_text(hjust=0)))

## devtools::install_github('kleinschmidt/daver')
library(daver)
## devtools::install_github('kleinschmidt/phonetic-sup-unsup')
library(supunsup)
## devtools::install_github('kleinschmidt/beliefupdatr')
library(beliefupdatr)

```



# Introduction

# Experiment 1

```{r data-exp1}

data_exp1 <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(trueCat = respCategory,
         subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

conditions_exp1 <-
  data_exp1 %>%
  group_by(bvotCond, trueCat) %>%
  summarise(mean_vot = mean(vot)) %>%
  spread(trueCat, mean_vot) %>%
  transmute(vot_cond = paste(b, p, sep=', '),
            ideal_boundary = (b+p)/2) %>%
  ungroup() %>%
  mutate(vot_cond = factor(vot_cond, levels=vot_cond))

data_exp1 <- left_join(data_exp1, conditions_exp1, by="bvotCond")

```

```{r vot-dists-exp1, fig.width=10, fig.height=3, fig.cap="Each subject heard one of these five synthetic accents, which differ only in the distribution of VOTs of the word-initial stops. Black dashed lines show VOT distributions from a hypothetical typical talker [as estimated by @Kronrod2016]. Note that the 0 and 10ms shifted accents are reasonably close to this typical talker, while the -10, 20, and 30ms shifted accents deviate substantially."}

prior_stats_by_talker <-
  votcorpora::vot %>%
  filter(source %in% c('gva13', 'bbg09', 'buckeye'),
         place == 'lab') %>%
  mutate(source = ifelse(source %in% c('gva13', 'bbg09'),
                         'goldricketal',
                         source)) %>%
  group_by(source, prevoiced, subject, phoneme) %>%
  summarise(mu = mean(vot),
            sigma2 = var(vot),
            sigma = sd(vot),
            n = n()) %>%
  rename(category = phoneme)

prior_stats <-
  prior_stats_by_talker %>%
  filter(source == 'goldricketal', category == 'b') %>%
  group_by(source, prevoiced, category) %>%
  summarise_at(vars(mu, sigma2, sigma, n), funs(mean, var, sum)) %>%
  transmute(category,
            mean = mu_mean,
            sd = sqrt(sigma2_mean),
            n = n_sum) %>%
  bind_rows(supunsup::prior_stats %>%
              filter(source=='kronrod2012') %>%
              mutate(n = 1))

prior_lhood <- 
  prior_stats %>%
  filter(source == 'kronrod2012') %>%
  supunsup::stats_to_lhood()

prior_class <- prior_lhood %>% lhood_to_classification()


exposure_stats <- data_exp1 %>%
  group_by(vot_cond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sd_noise = sqrt(82)

exposure_lhood <- exposure_stats %>%
  group_by(vot_cond) %>%
  do(supunsup::stats_to_lhood(., sd_noise))

data_exp1 %>%
  group_by(vot_cond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=vot_cond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  geom_text(data=data.frame(vot_cond='-10, 30'), x = 10, y = 60,
            label = 'Typical Talker',
            color='black', hjust=0, vjust=0.3, size=3) +
  geom_text(data=data.frame(vot_cond='-10, 30'), x = 40, y = 50,
            label = 'Exposure\nTalker',
            hjust=0, vjust=0.8, size=3,
            lineheight=1) + 
  facet_grid(.~vot_cond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  labs(title="Experiment 1: VOT distribution conditions",
       subtitle="Similarity of exposure talker to typical talker controlled by shifting means of /b/ and /p/ distributions")
  ## scale_fill_discrete('/b/, /p/\nmean VOT') ## +
  ## theme(legend.position='none')

```

## Methods

### Subjects {#sec:subjects}

```{r participants-exp1}

n_subj <- data_exp1 %>% group_by(subject) %>% summarise() %>% tally()

excluded <- supunsup::supunsup_excluded %>%
  filter(supCond == 'unsupervised') %>%
  group_by(subject, bvotCond) %>% 
  summarise() %>% 
  left_join(supunsup::excludes, by="subject") %>%
  select(subject, bvotCond, exclude80PercentAcc, rank)

n_excluded <- nrow(excluded)
n_subj_repeat <- sum(!is.na(excluded$rank))
n_subj_bad <- sum(!is.na(excluded$exclude80PercentAcc))
n_both <- n_subj_repeat + n_subj_bad - n_excluded 

n_total <- n_subj + n_excluded

n_excl_cond <- excluded %>%
  group_by(bvotCond) %>%
  tally() %>%
  arrange(n)

```

`r n_total` subjects were recruited via Amazon's Mechanical Turk. Subjects were
paid \$2.00 for participation, which took about 20 minutes. We excluded subjects
who participated more than once ($n=`r n_subj_repeat`$) or who failed to
classify VOTs reliably ($n=`r n_subj_bad`$; $n=`r n_both`$ for both reasons).
We defined reliable classification as accuracy of at least 80% at 0 and 70ms
VOT. Because some conditions had few stimuli with these VOTs, we extrapolated
subjects' responses using a logistic generalized linear model (GLM).  Excluded
subjects were roughly equally distributed across conditions (maximum of 
`r last(n_excl_cond$n)` in `r last(n_excl_cond$bvotCond)`ms /b/ VOT condition,
and minimum of `r first(n_excl_cond$n)` in `r first(n_excl_cond$bvotCond)`ms /b/
VOT condition). After these exclusions, data from `r n_subj` subjects remained
for analysis.

### Procedure

![Example trial display (beach/peach). Listeners first click on the
    green button to play the word, then click on one picture to indicate what
    they heard.](figure_manual/beach_peach.png){#fig:beach-peach}

The procedure is based on @Clayards2008. Figure -@fig:beach-peach shows an
example trial display.  On each trial, two response option images appeared,
which corresponded to one of three /b/-/p/ minimal pairs (beach/peach,
bees/peas, or beak/peak).  Subjects started each trial by clicking on a button
between the two pictures, which played the corresponding minimal pair word audio
stimulus. Subjects then clicked on the picture to indicate whether they heard
the /b/ or /p/ member of the minimal pair. Subjects performed 222 of these
trials, evenly divided between the three minimal pairs, in random order.

Each trial's word was synthesized with a voice onset time (VOT) that was
randomly drawn from a bimodal distribution, with low and high VOT clusters
implicitly corresponding to /b/ and /p/, respectively. This distribution defined
the *accent* that the subject heard, and each subject was pseudorandomly
assigned to one of five accent conditions ([@Fig:vot-dists-exp1]).

### Materials

The audio and visual stimuli we used were identical to those in
@Clayards2008. Three /b/-/p/ minimal pair audio continua were synthesized using
the 1988 Klatt synthesizer [@Klatt1980], by manipulating VOT in 10ms increments
(either adding voicing before the stop burst to create negative VOTs, or
aspiration after for positive VOTs). Within a /b/-/p/ continuum, the other
parameters were held constant, and modeled on natural tokens of the endpoints
(beach/peach, bees/peas, and beak/peak).

## Results

In order to assess distributional learning, each subject's classification
function is compared to two baselines, corresponding to _no learning_ and
_complete learning_.  The _no learning_ baseline is derived from the category
boundary for a _typical talker_'s VOT distributions [based on @Kronrod2016], and
the category boundary based solely on the exposure talker's distributions.  I
refer to the typical talker's boundary as the "no learning" baseline, and the
exposure talker's boundary as the "complete learning" baseline.

Each subject's classification function was estimated with a Bayesian multilevel
logistic regression, using the `brms` package [@Burkner2017] for `R`
[@RCoreTeam2017].  This approach simultaneously estimates the group-level
effects of the experimental manipulation (VOT shift condition) on the /b/-/p/
category boundary location and sharpness, and each individual subjects' boundary
locations and slopes.  A multilevel approach like this has the benefit of
properly accounting for and balancing the joint uncertainty about the group- and
individual-level effects [@Gelman2007].[^shrinkage]

[^shrinkage]: One effect of using a multilevel approach is that the estimate of
    each subject's boundary is "shrunk" towards the group-level estimate.  This
    reflects the assumption that subjects are drawn from the same population,
    and hence the data from one subject in a group are informative about other
    subjects in that group (and vice versa).  The amount of this shrinkage
    depends on the amount of data available from each individual subject, and
    the consistency of the individual subject estimates.  In this case, each
    subject contributes sufficient data to make shrinkage mild, and an
    alternative analysis that estimates each subjects' boundary with a separate
    logistic GLM produces qualitatively similar effects.

```{r exp1-regression, dependson=c("data-exp1")}

data_exp1_mod <-
  data_exp1 %>%
  select(subject, bvotCond, trial, vot, respP) %>%
  mutate(vot_s = (vot - mean(vot)) / sd(vot),
         trial_s = (trial - mean(trial)) / sd(trial))

# b_logit_exp1 <- brm(respP ~ 1 + bvotCond * vot_s * trial_s + (1 + vot_s | subject),
#                     data=data_exp1_mod, family=bernoulli(), chains=4, iter=1000)
# saveRDS(b_logit_exp1, "models/brm_logistic_exp1.rds")
b_logit_exp1 <- readRDS("models/brm_logistic_exp1.rds")

# extract fitted classification functions:
# first get trials from first, second, and third thirds
exp1_blocks <-
  data_exp1_mod %>%
  group_by(block=ntile(trial, 3)) %>%
  summarise_at(vars(trial, trial_s), mean)
# overall fit (fixed effects):
data_pred <-
  cross_df(list(vot_s = seq(min(data_exp1_mod$vot_s),
                            max(data_exp1_mod$vot_s),
                            length.out=100),
                bvotCond = unique(data_exp1_mod$bvotCond),
                block = 1:3)) %>%
  left_join(exp1_blocks, by="block") %>%
  mutate(vot = vot_s * sd(data_exp1_mod$vot) + mean(data_exp1_mod$vot)) %>%
  left_join(conditions_exp1, by="bvotCond")

expt1_class <-
  fitted(b_logit_exp1, newdata=data_pred, re_formula=NA) %>%
  as_tibble() %>%
  bind_cols(data_pred)

# by-subject fit (fixed+random effects):
data_pred_subj <-
  data_exp1_mod %>%
  group_by(subject, bvotCond) %>%
  summarise() %>%
  left_join(data_pred, by="bvotCond")

expt1_class_bysub <-
  fitted(b_logit_exp1, newdata=data_pred_subj) %>%
  as_tibble() %>%
  bind_cols(data_pred_subj)

# get x-intercept (category boundary):
#' Find x intercept with linear interpolation of two nearest points
#'
#' Respects grouping
#'
#' @param d a tibble
#' @param x (unquoted) column with the x coordinates
#' @param y (unquoted) column with the y coordinates
#' @param ytarget (default=0.5) y value to find x intercept at
find_bound <- function(d, x, y, ytarget=0.5) {
  x <- enquo(x)
  x_name <- quo_name(x)
  y <- enquo(y)
  d %>%
    arrange((!!y - ytarget)^2) %>%
    filter(row_number() <= 2) %>%
    summarise(!!x_name := (ytarget - first(!!y)) * diff(!!x)/diff(!!y) + first(!!x))
}


expt1_bounds <-
  expt1_class %>%
  group_by(vot_cond, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt1_bounds_bysub <-
  expt1_class_bysub %>%
  group_by(vot_cond, subject, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt1_bounds_fixef_samples <-
  tidybayes::linpred_draws(b_logit_exp1, data_pred, re_formula=NA) %>%
  group_by(vot_cond, block, trial, .draw) %>%
  find_bound(x=vot, y=.value)

```

```{r exp1-results, fig.width=10, fig.height=3, fig.cap="Results from experiment 1 show that distributional learning is incomplete for large shifts of VOT distributions from a typical talker's distributions"}

perfect_learning <- exposure_stats %>%
  group_by(vot_cond) %>%
  do(stats_to_lhood(.)) %>%
  lhood_to_classification()

no_learning <- prior_lhood %>%
  lhood_to_classification()

vot_limits <- data_exp1 %>% pull(vot) %>% range()

ggplot(expt1_class_bysub, aes(x=vot, color=vot_cond)) +
  geom_line(data=.%>%filter(block==3), aes(y=Estimate, group=subject), alpha=0.2) +
  facet_grid(.~vot_cond) +
  geom_line(data=perfect_learning, aes(y=prob_p), group=1, linetype="33", size=1,
            show.legend=FALSE) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype="99", color='black',
            show.legend=FALSE) +
  geom_point(data=expt1_bounds %>% filter(block==3),
             y=0.5) +
  labs(x="VOT (ms)",
       y="Probability /p/ response",
       title="Experiment 1: Classification functions",
       subtitle="Fitted with mixed effects logistic regression, predictions for final third of trials") +
  lims(x = c(-10, 70))

```

```{r expt1-boundaries, fig.cap="Category boundaries by condition.  Violins show the distribution of subjects' boundaries (random effects), and the white points show the group-level boundary (fixed effects, with 95% Bayesian confidence intervals)"}

no_learning_bound <-
  no_learning %>%
  arrange((prob_p - 0.5)^2) %>%
  head(1) %>%
  pull(vot)

perfect_learning_bound <-
  perfect_learning %>%
  group_by(vot_cond) %>%
  arrange((prob_p - 0.5)^2) %>%
  filter(row_number() == 1)

# estimate uncertainty of fixed effects boundaries:
expt1_bounds_fixef <-
  expt1_bounds_fixef_samples %>%
  group_by(vot_cond, block, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))

expt1_bounds_bysub %>%
  filter(block==3) %>%
  ggplot(aes(x=vot_cond, y=vot, fill=vot_cond, color=vot_cond)) +
  geom_violin(alpha=0.5, color=NA) +
  ## geom_beeswarm(cex=2) +
  geom_pointrange(data = expt1_bounds_fixef %>% filter(block==3),
                  aes(y=mean, ymin=low, ymax=high),
                  color="white", show.legend=FALSE) +
  geom_hline(yintercept=no_learning_bound, color="black", linetype=2) +
  geom_segment(aes(x=as.numeric(vot_cond)-.5, # exposure boundaries_exp1
                   xend=as.numeric(vot_cond)+.5,
                   y=vot,
                   yend=vot,
                   color=vot_cond),
               linetype = 2, size = 1,
               data = perfect_learning_bound) +
  geom_text(data=perfect_learning_bound %>% filter(vot_cond=="20, 60"),
            aes(y=vot, color=vot_cond),
            ## x=3.5, y=41, 
            label="Complete learning\n(Exposure talker)",
            hjust = 0, vjust = 0,
            nudge_x = -0.5, nudge_y = 1) +
  geom_text(data=perfect_learning_bound %>% filter(vot_cond=="20, 60"),
            aes(color=vot_cond, x = as.numeric(vot_cond)-0.5),
            y=no_learning_bound - 1, color='black',
            ## x=3.5, y=41, 
            label="No learning\n(Typical talker)",
            hjust = 1, vjust = 0,
            ) +
  labs(title="Experiment 1: Category boundaries",
       subtitle="Compared with no learning and complete learning",
       y = "boundary VOT (ms)",
       x = "Exposure condition (/b/, /p/ mean VOT)") +
  theme(legend.position="none") +
  coord_flip()

```

```{r bound-diff-p-vals}

# compute Bayesian p-values for differences between boundaries between
# neighboring VOT conditions.

expt1_bounds_fixef_diffs <-
  expt1_bounds_fixef_samples %>%
  group_by(.draw, block) %>%
  arrange(vot_cond) %>%
  mutate(vot_cond_next = lead(vot_cond),
         bound_diff = vot - lead(vot))

expt1_bounds_fixef_diff_stats <-
  expt1_bounds_fixef_diffs %>%
  group_by(block, vot_cond, vot_cond_next) %>%
  filter(!is.na(vot_cond_next)) %>%
  summarise_at(vars(bound_diff),
               funs(mean,
                    low=quantile(., 0.025),
                    high=quantile(., 0.975),
                    p=mean(.>0)))

ex1_pval_str <-
  expt1_bounds_fixef_diff_stats %>%
  filter(block == 3) %>%
  pull(p) %>%
  max() %>%
  daver::p_val_to_less_than()

```

```{r bound-undershoot}

# summaries of boundary undershoot
expt1_boundary_undershoot <-
  perfect_learning_bound %>%
  select(vot_cond, ideal_bound=vot) %>%
  right_join(expt1_bounds_fixef, by="vot_cond") %>%
  filter(block == 3) %>%
  mutate_at(vars(low, high, mean),
            funs(abs=(.-ideal_bound)*sign(no_learning_bound-ideal_bound),
                 perc=round(100 *
                              (.-no_learning_bound) /
                              (ideal_bound-no_learning_bound)))) %>%
  select(-ideal_bound, block, trial)
  

```


The first question is whether listeners learned anything at all from exposure to
these VOT distributions.  One way to answer this is to look at the _category
boundary_---the VOT that is ambiguous between /b/ and /p/---corresponding to the
fixed effects for each exposure condition.  Because trial number was inluded as
a regressor, trial was fixed at 184 (83% or 5/6 of the 222 total trials).  The
large white points in Figure [-@fig:expt1-boundaries] show the estimated
boundary for each condition, and the corresponding confidence intervals are the
95% posterior intervals (e.g., the 95% quantiles of the posterior samples).
Visual inspection shows that the group-level posterior distributions of the
boundaries are almost entirely non-overlapping with their
most-similar/neighboring conditions (all $`r ex1_pval_str`$[^bayes_p]), even though the
_subject_-level distributions do overlap (as shown by the colored violins).

[^bayes_p]: The $p$ values reported here are Bayesian $p$ values, or the
    posterior probability that the relevant statement holds conditional on the
    data and model.  These are determined using _samples_ from the posterior,
    by computing the proportion of those samples where the statement is true.
    In this case, the relevant condition is whether the category boundary for
    one condition is reliably less than the boundary for the condition with a
    next highest mean VOTs.

The second question is how _completely_ listeners learned the distributions they
were exposed to.  Visual inspections of the categorization functions
[@fig:exp1-results] and the category boundaries [@fig:expt1-boundaries] suggests
that listeners category boundaries (at 83% through the exposure) do _not_
perfectly correspond to the ideal category boundary for the VOT distributions
they were exposed to (thick dashed colored lines).  Specifically, listeners seem
to _undershoot_ the ideal category boundary, and their categorizations functions
lie between the ideal boundary for the exposure talker and the boundary
corresponding to a typical talker of American English (thin black colored
lines).

The undershoot is largest in absolute terms for the most shifted distributions,
with 

## Discussion

These results indicate two things.  First, like many other studies [@cite], I
found evidence for distributional learning: listeners _do_ change how they
categorize sounds as voiced or unvoiced due to exposure to different VOT
distributions.  Second, this learning is not _complete_, in the sense that
listeners' voiced-unvoiced boundary does not exactly match the boundary implied
by the bimodal distribution they were exposed to.  Like the findings of
@Sumner2013 and @Idemaru2011, this second result suggests that distributional
learning is _constrained_ somehow.  In the following experiments, I investigate
two possible sources of these constraints.

```{r prop-in-wrong-cat}

data_exp1 %>%
  group_by(vot_cond, trueCat) %>%
  summarise(wrong=mean(ifelse(trueCat == 'b',
                              vot > no_learning_bound,
                              vot < no_learning_bound)))

```


First, it is possible that the primary constraint here is that listeners are
being asked to do _unsupervised_ distributional learning.  That is, they are
free to interpret each VOT they hear as either a /b/ or a /p/, since both
options are available for a response.  For distributions that are highly
shifted, most tokens will fall into one of the two pre-existing distributions
(see @Fig:vot-dists-exp1): in the condition with the highest positive shift
(30ms /b/, 70ms /p/), 73% of all tokens from the lower cluster would be
classified as /p/ according to a typical talker's distributions.  Thus, what I
intended listeners to treat as two separate clusters of VOTs---one for /b/ and
one for /p/---they may instead have interpreted as one large cluster in extreme
conditions, lacking any labels to tell them otherwise.  I address this
possibility in Experiment 2, by providing labels on a portion of trials.

A second possibility is that listeners are constrained by their prior experience
with other talkers.  The fact that listeners' category boundaries tend to lie
between those of a typical talker and the experimental exposure talker suggests
that this is not implausible.  To assess this possibility more rigorously, after
Experiment 2 I use a Bayesian belief-updating model of distributional learning
to investigate whether the pattern of distributional learning across conditions
is consistent with belief updating starting from common prior beliefs.

# Experiment 2

# Model

# Experiment 3
