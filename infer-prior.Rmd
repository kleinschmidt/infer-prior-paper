---
Title: What constrains distributional learning in adults?
Author: Dave Kleinschmidt
bibliography: /home/dave/Zotero/library.bib
output:
    html_document:
        code_folding: hide
        dev: png
        fig_retina: 3
        keep_md: true
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-crossref
    pdf_document:
        md_extensions: +implicit_figures
        keep_tex: true
        pandoc_args:
        - --filter
        - pandoc-crossref
---


```{r knitr-setup, warning=FALSE, message=FALSE, error=FALSE, echo=FALSE, results='hide'}

library(knitr)
opts_chunk$set(warning = FALSE,
               message = FALSE,
               error = FALSE,
               cache=TRUE,
               results = "hide",
               echo=opts_knit$get("rmarkdown.pandoc.to") != 'latex')

options(digits=2)

## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})
## Produce markdown-formatted table captions with anchors for cross-refs.
## Requires pandoc-tablenos to parse the IDs. Refer to tables
## in text with {@tbl:label} or @tbl:label.
## Based partly on http://stackoverflow.com/a/18672268
##
## (see https://github.com/tomduck/pandoc-tablenos)
knit_hooks$set(tbl.cap = function(before, options, envir) {
  if(!before){
    paste0('\n\nTable: ', options$tbl.cap,
           ' {#tbl:', options$label, '}', sep = '')
  }
})

```

```{r preamble, cache=FALSE}

library(tidyverse)
library(beliefupdatr)
library(supunsup)

library(rstan)
library(brms)
library(tidybayes)

library(cowplot)
theme_set(cowplot::theme_cowplot() +
            theme(plot.title=element_text(hjust=0)))

## devtools::install_github('kleinschmidt/daver')
library(daver)
## devtools::install_github('kleinschmidt/phonetic-sup-unsup')
library(supunsup)
## devtools::install_github('kleinschmidt/beliefupdatr')
library(beliefupdatr)

```



# Introduction

((( in contrast to 'standard' distributional learning paradigms which compare
identification or discrimination after exposure to unimodal or bimodal
distributions of acoustic-phonetic cues, .... )))

# Experiment 1: Distributional learning of voicing from VOT

```{r data-exp1}

data_exp1 <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(trueCat = respCategory,
         subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

conditions_exp1 <-
  data_exp1 %>%
  group_by(bvotCond, trueCat) %>%
  summarise(mean_vot = mean(vot)) %>%
  spread(trueCat, mean_vot) %>%
  transmute(vot_cond = paste(b, p, sep=', '),
            ideal_boundary = (b+p)/2) %>%
  ungroup() %>%
  mutate(vot_cond = factor(vot_cond, levels=vot_cond))

data_exp1 <- left_join(data_exp1, conditions_exp1, by="bvotCond")

```

```{r vot-dists-exp1, fig.width=10, fig.height=3, fig.cap="Each subject heard one of these five synthetic accents, which differ only in the distribution of VOTs of the word-initial stops. Black dashed lines show VOT distributions from a hypothetical typical talker [as estimated by @Kronrod2016]. Note that the 0 and 10ms shifted accents are reasonably close to this typical talker, while the -10, 20, and 30ms shifted accents deviate substantially."}

prior_stats_by_talker <-
  votcorpora::vot %>%
  filter(source %in% c('gva13', 'bbg09', 'buckeye'),
         place == 'lab') %>%
  mutate(source = ifelse(source %in% c('gva13', 'bbg09'),
                         'goldricketal',
                         source)) %>%
  group_by(source, prevoiced, subject, phoneme) %>%
  summarise(mu = mean(vot),
            sigma2 = var(vot),
            sigma = sd(vot),
            n = n()) %>%
  rename(category = phoneme)

prior_stats <-
  prior_stats_by_talker %>%
  filter(source == 'goldricketal', category == 'b') %>%
  group_by(source, prevoiced, category) %>%
  summarise_at(vars(mu, sigma2, sigma, n), funs(mean, var, sum)) %>%
  transmute(category,
            mean = mu_mean,
            sd = sqrt(sigma2_mean),
            n = n_sum) %>%
  bind_rows(supunsup::prior_stats %>%
              filter(source=='kronrod2012') %>%
              mutate(n = 1))

prior_lhood <- 
  prior_stats %>%
  filter(source == 'kronrod2012') %>%
  supunsup::stats_to_lhood()

prior_class <- prior_lhood %>% lhood_to_classification()


exposure_stats <- data_exp1 %>%
  group_by(vot_cond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sd_noise = sqrt(82)

exposure_lhood <- exposure_stats %>%
  group_by(vot_cond) %>%
  do(supunsup::stats_to_lhood(., sd_noise))

data_exp1 %>%
  group_by(vot_cond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=vot_cond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  geom_text(data=data.frame(vot_cond='-10, 30'), x = 10, y = 60,
            label = 'Typical Talker',
            color='black', hjust=0, vjust=0.3, size=3) +
  geom_text(data=data.frame(vot_cond='-10, 30'), x = 40, y = 50,
            label = 'Exposure\nTalker',
            hjust=0, vjust=0.8, size=3,
            lineheight=1) + 
  facet_grid(.~vot_cond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  labs(title="Experiment 1: VOT distribution conditions",
       subtitle="Similarity of exposure talker to typical talker controlled by shifting means of /b/ and /p/ distributions")
  ## scale_fill_discrete('/b/, /p/\nmean VOT') ## +
  ## theme(legend.position='none')

```

## Methods

### Participants {#sec:participants}

```{r participants-exp1}

n_subj <- data_exp1 %>% group_by(subject) %>% summarise() %>% tally()

excluded <- supunsup::supunsup_excluded %>%
  filter(supCond == 'unsupervised') %>%
  group_by(subject, bvotCond) %>% 
  summarise() %>% 
  left_join(supunsup::excludes, by="subject") %>%
  select(subject, bvotCond, exclude80PercentAcc, rank)

n_excluded <- nrow(excluded)
n_subj_repeat <- sum(!is.na(excluded$rank))
n_subj_bad <- sum(!is.na(excluded$exclude80PercentAcc))
n_both <- n_subj_repeat + n_subj_bad - n_excluded 

n_total <- n_subj + n_excluded

n_excl_cond <- excluded %>%
  group_by(bvotCond) %>%
  tally() %>%
  arrange(n)

```

`r n_total` participants were recruited via Amazon's Mechanical Turk. Participants were
paid \$2.00 for participation, which took about 20 minutes. We excluded participants
who participated more than once ($n=`r n_subj_repeat`$) or who failed to
classify VOTs reliably ($n=`r n_subj_bad`$; $n=`r n_both`$ for both reasons).
We defined reliable classification as accuracy of at least 80% at 0 and 70ms
VOT. Because some conditions had few stimuli with these VOTs, we extrapolated
participants' responses using a logistic generalized linear model (GLM).  Excluded
participants were roughly equally distributed across conditions (maximum of 
`r last(n_excl_cond$n)` in `r last(n_excl_cond$bvotCond)`ms /b/ VOT condition,
and minimum of `r first(n_excl_cond$n)` in `r first(n_excl_cond$bvotCond)`ms /b/
VOT condition). After these exclusions, data from `r n_subj` participants remained
for analysis.

### Procedure

![Example trial display (beach/peach). Listeners first click on the
    green button to play the word, then click on one picture to indicate what
    they heard.](figure_manual/beach_peach.png){#fig:beach-peach}

The procedure is based on @Clayards2008. Figure -@fig:beach-peach shows an
example trial display.  On each trial, two response option images appeared,
which corresponded to one of three /b/-/p/ minimal pairs (beach/peach,
bees/peas, or beak/peak).  Participants started each trial by clicking on a button
between the two pictures, which played the corresponding minimal pair word audio
stimulus. Participants then clicked on the picture to indicate whether they heard
the /b/ or /p/ member of the minimal pair. Participants performed 222 of these
trials, evenly divided between the three minimal pairs, in random order.

Each trial's word was synthesized with a voice onset time (VOT) that was
randomly drawn from a bimodal distribution, with low and high VOT clusters
implicitly corresponding to /b/ and /p/, respectively. This distribution defined
the *accent* that the subject heard, and each subject was pseudorandomly
assigned to one of five accent conditions ([@Fig:vot-dists-exp1]).

### Materials

The audio and visual stimuli we used were identical to those in
@Clayards2008. Three /b/-/p/ minimal pair audio continua were synthesized using
the 1988 Klatt synthesizer [@Klatt1980], by manipulating VOT in 10ms increments
(either adding voicing before the stop burst to create negative VOTs, or
aspiration after for positive VOTs). Within a /b/-/p/ continuum, the other
parameters were held constant, and modeled on natural tokens of the endpoints
(beach/peach, bees/peas, and beak/peak).

## Results

In order to assess distributional learning, each subject's classification
function is compared to two baselines, corresponding to _no learning_ and
_complete learning_.  The _no learning_ baseline is derived from the category
boundary for a _typical talker_'s VOT distributions [based on @Kronrod2016], and
the category boundary based solely on the exposure talker's distributions.  I
refer to the typical talker's boundary as the "no learning" baseline, and the
exposure talker's boundary as the "complete learning" baseline.

Each subject's classification function was estimated with a Bayesian multilevel
logistic regression, using the `brms` package [@Burkner2017] for `R`
[@RCoreTeam2017].  This approach simultaneously estimates the group-level
effects of the experimental manipulation (VOT shift condition) on the /b/-/p/
category boundary location and sharpness, and each individual participants' boundary
locations and slopes.  A multilevel approach like this has the benefit of
properly accounting for and balancing the joint uncertainty about the group- and
individual-level effects [@Gelman2007].[^shrinkage]

[^shrinkage]: One effect of using a multilevel approach is that the estimate of
    each subject's boundary is "shrunk" towards the group-level estimate.  This
    reflects the assumption that participants are drawn from the same population,
    and hence the data from one subject in a group are informative about other
    participants in that group (and vice versa).  The amount of this shrinkage
    depends on the amount of data available from each individual subject, and
    the consistency of the individual subject estimates.  In this case, each
    subject contributes sufficient data to make shrinkage mild, and an
    alternative analysis that estimates each participants' boundary with a separate
    logistic GLM produces qualitatively similar effects.

```{r exp1-regression, dependson=c("data-exp1")}

data_exp1_mod <-
  data_exp1 %>%
  select(subject, bvotCond, trial, vot, respP) %>%
  mutate(vot_s = (vot - mean(vot)) / sd(vot),
         trial_s = (trial - mean(trial)) / sd(trial))

# b_logit_exp1 <- brm(respP ~ 1 + bvotCond * vot_s * trial_s + (1 + vot_s | subject),
#                     data=data_exp1_mod, family=bernoulli(), chains=4, iter=1000)
# saveRDS(b_logit_exp1, "models/brm_logistic_exp1.rds")
b_logit_exp1 <- readRDS("models/brm_logistic_exp1.rds")

# extract fitted classification functions:
# first get trials from first, second, and third thirds
exp1_blocks <-
  data_exp1_mod %>%
  group_by(block=ntile(trial, 3)) %>%
  summarise_at(vars(trial, trial_s), mean)
# overall fit (fixed effects):
data_pred <-
  cross_df(list(vot_s = seq(min(data_exp1_mod$vot_s),
                            max(data_exp1_mod$vot_s),
                            length.out=100),
                bvotCond = unique(data_exp1_mod$bvotCond),
                block = 1:3)) %>%
  left_join(exp1_blocks, by="block") %>%
  mutate(vot = vot_s * sd(data_exp1_mod$vot) + mean(data_exp1_mod$vot)) %>%
  left_join(conditions_exp1, by="bvotCond")

expt1_class <-
  fitted(b_logit_exp1, newdata=data_pred, re_formula=NA) %>%
  as_tibble() %>%
  bind_cols(data_pred)

# by-subject fit (fixed+random effects):
data_pred_subj <-
  data_exp1_mod %>%
  group_by(subject, bvotCond) %>%
  summarise() %>%
  left_join(data_pred, by="bvotCond")

expt1_class_bysub <-
  fitted(b_logit_exp1, newdata=data_pred_subj) %>%
  as_tibble() %>%
  bind_cols(data_pred_subj)

# get x-intercept (category boundary):
#' Find x intercept with linear interpolation of two nearest points
#'
#' Respects grouping
#'
#' @param d a tibble
#' @param x (unquoted) column with the x coordinates
#' @param y (unquoted) column with the y coordinates
#' @param ytarget (default=0.5) y value to find x intercept at
find_bound <- function(d, x, y, ytarget=0.5) {
  x <- enquo(x)
  x_name <- quo_name(x)
  y <- enquo(y)
  d %>%
    arrange((!!y - ytarget)^2) %>%
    filter(row_number() <= 2) %>%
    summarise(!!x_name := (ytarget - first(!!y)) * diff(!!x)/diff(!!y) + first(!!x))
}


expt1_bounds <-
  expt1_class %>%
  group_by(vot_cond, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt1_bounds_bysub <-
  expt1_class_bysub %>%
  group_by(vot_cond, subject, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt1_bounds_fixef_samples <-
  tidybayes::linpred_draws(b_logit_exp1, data_pred, re_formula=NA) %>%
  group_by(vot_cond, block, trial, .draw) %>%
  find_bound(x=vot, y=.value)

```

```{r exp1-results, fig.width=10, fig.height=3, fig.cap="Results from experiment 1 show that distributional learning is incomplete for large shifts of VOT distributions from a typical talker's distributions"}

perfect_learning <- exposure_stats %>%
  group_by(vot_cond) %>%
  do(stats_to_lhood(.)) %>%
  lhood_to_classification()

no_learning <- prior_lhood %>%
  lhood_to_classification()

vot_limits <- data_exp1 %>% pull(vot) %>% range()

ggplot(expt1_class_bysub, aes(x=vot, color=vot_cond)) +
  geom_line(data=.%>%filter(block==3), aes(y=Estimate, group=subject), alpha=0.2) +
  facet_grid(.~vot_cond) +
  geom_line(data=perfect_learning, aes(y=prob_p), group=1, linetype="33", size=1,
            show.legend=FALSE) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype="99", color='black',
            show.legend=FALSE) +
  geom_point(data=expt1_bounds %>% filter(block==3),
             y=0.5) +
  labs(x="VOT (ms)",
       y="Probability /p/ response",
       title="Experiment 1: Classification functions",
       subtitle="Fitted with mixed effects logistic regression, predictions for final third of trials") +
  lims(x = c(-10, 70))

```

```{r expt1-boundaries, fig.cap="Category boundaries by condition.  Violins show the distribution of participants' boundaries (random effects), and the white points show the group-level boundary (fixed effects, with 95% Bayesian confidence intervals)"}

no_learning_bound <-
  no_learning %>%
  arrange((prob_p - 0.5)^2) %>%
  head(1) %>%
  pull(vot)

perfect_learning_bound <-
  perfect_learning %>%
  group_by(vot_cond) %>%
  arrange((prob_p - 0.5)^2) %>%
  filter(row_number() == 1)

# estimate uncertainty of fixed effects boundaries:
expt1_bounds_fixef <-
  expt1_bounds_fixef_samples %>%
  group_by(vot_cond, block, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))

expt1_bounds_bysub %>%
  filter(block==3) %>%
  ggplot(aes(x=vot_cond, y=vot, fill=vot_cond, color=vot_cond)) +
  geom_violin(alpha=0.5, color=NA) +
  ## geom_beeswarm(cex=2) +
  geom_pointrange(data = expt1_bounds_fixef %>% filter(block==3),
                  aes(y=mean, ymin=low, ymax=high),
                  color="white", show.legend=FALSE) +
  geom_hline(yintercept=no_learning_bound, color="black", linetype=2) +
  geom_segment(aes(x=as.numeric(vot_cond)-.5, # exposure boundaries_exp1
                   xend=as.numeric(vot_cond)+.5,
                   y=vot,
                   yend=vot,
                   color=vot_cond),
               linetype = 2, size = 1,
               data = perfect_learning_bound) +
  geom_text(data=perfect_learning_bound %>% filter(vot_cond=="20, 60"),
            aes(y=vot, color=vot_cond),
            ## x=3.5, y=41, 
            label="Complete learning\n(Exposure talker)",
            hjust = 0, vjust = 0,
            nudge_x = -0.5, nudge_y = 1) +
  geom_text(data=perfect_learning_bound %>% filter(vot_cond=="20, 60"),
            aes(color=vot_cond, x = as.numeric(vot_cond)-0.5),
            y=no_learning_bound - 1, color='black',
            ## x=3.5, y=41, 
            label="No learning\n(Typical talker)",
            hjust = 1, vjust = 0,
            ) +
  labs(title="Experiment 1: Category boundaries",
       subtitle="Compared with no learning and complete learning",
       y = "Boundary VOT (ms)",
       x = "Exposure condition (/b/, /p/ mean VOT)") +
  theme(legend.position="none") +
  coord_flip()

```

```{r bound-diff-p-vals}

# compute Bayesian p-values for differences between boundaries between
# neighboring VOT conditions.

expt1_bounds_fixef_diffs <-
  expt1_bounds_fixef_samples %>%
  group_by(.draw, block) %>%
  arrange(vot_cond) %>%
  mutate(vot_cond_next = lead(vot_cond),
         bound_diff = vot - lead(vot))

expt1_bounds_fixef_diff_stats <-
  expt1_bounds_fixef_diffs %>%
  group_by(block, vot_cond, vot_cond_next) %>%
  filter(!is.na(vot_cond_next)) %>%
  summarise_at(vars(bound_diff),
               funs(mean,
                    low=quantile(., 0.025),
                    high=quantile(., 0.975),
                    p=mean(.>0)))

ex1_pval_str <-
  expt1_bounds_fixef_diff_stats %>%
  filter(block == 3) %>%
  pull(p) %>%
  max() %>%
  daver::p_val_to_less_than()

```

```{r bound-undershoot}

# summaries of boundary undershoot
expt1_boundary_undershoot <-
  perfect_learning_bound %>%
  select(vot_cond, ideal_bound=vot) %>%
  right_join(expt1_bounds_fixef, by="vot_cond") %>%
  filter(block == 3) %>%
  mutate_at(vars(low, high, mean),
            funs(abs=(.-ideal_bound)*sign(no_learning_bound-ideal_bound),
                 perc=round(100 *
                              (.-no_learning_bound) /
                              (ideal_bound-no_learning_bound)))) %>%
  select(-ideal_bound, block, trial)
  

```


The first question is whether listeners learned anything at all from exposure to
these VOT distributions.  One way to answer this is to look at the _category
boundary_---the VOT that is ambiguous between /b/ and /p/---corresponding to the
fixed effects for each exposure condition.  Because trial number was inluded as
a regressor, trial was fixed at 184 (83% or 5/6 of the 222 total trials).  The
large white points in Figure [-@fig:expt1-boundaries] show the estimated
boundary for each condition, and the corresponding confidence intervals are the
95% posterior intervals (e.g., the 95% quantiles of the posterior samples).
Visual inspection shows that the group-level posterior distributions of the
boundaries are almost entirely non-overlapping with their
most-similar/neighboring conditions (all $`r ex1_pval_str`$[^bayes_p]), even though the
_subject_-level distributions do overlap (as shown by the colored violins).

[^bayes_p]: The $p$ values reported here are Bayesian $p$ values, or the
    posterior probability that the relevant statement holds conditional on the
    data and model.  These are determined using _samples_ from the posterior,
    by computing the proportion of those samples where the statement is true.
    In this case, the relevant condition is whether the category boundary for
    one condition is reliably less than the boundary for the condition with a
    next highest mean VOTs.

The second question is how _completely_ listeners learned the distributions they
were exposed to.  Visual inspections of the categorization functions
[@fig:exp1-results] and the category boundaries [@fig:expt1-boundaries] suggests
that listeners category boundaries (at 83% through the exposure) do _not_
perfectly correspond to the ideal category boundary for the VOT distributions
they were exposed to (thick dashed colored lines).  Specifically, listeners seem
to _undershoot_ the ideal category boundary, and their categorizations functions
lie between the ideal boundary for the exposure talker and the boundary
corresponding to a typical talker of American English (thin black colored
lines).

The undershoot is largest in absolute terms for the most shifted distributions,
with 

## Discussion

These results indicate two things.  First, like many other studies [@cite], I
found evidence for distributional learning: listeners _do_ change how they
categorize sounds as voiced or unvoiced due to exposure to different VOT
distributions.  Second, this learning is not _complete_, in the sense that
listeners' voiced-unvoiced boundary does not exactly match the boundary implied
by the bimodal distribution they were exposed to.  Like the findings of
@Sumner2011 and @Idemaru2011, this second result suggests that distributional
learning is _constrained_ somehow.  In the following experiments, I investigate
two possible sources of these constraints.

```{r prop-in-wrong-cat}

data_exp1 %>%
  group_by(vot_cond, trueCat) %>%
  summarise(wrong=mean(ifelse(trueCat == 'b',
                              vot > no_learning_bound,
                              vot < no_learning_bound)))

```


First, it is possible that the primary constraint here is that listeners are
being asked to do _unsupervised_ distributional learning.  That is, they are
free to interpret each VOT they hear as either a /b/ or a /p/, since both
options are available for a response.  For distributions that are highly
shifted, most tokens will fall into one of the two pre-existing distributions
(see @Fig:vot-dists-exp1): in the condition with the highest positive shift
(30ms /b/, 70ms /p/), 73% of all tokens from the lower cluster would be
classified as /p/ according to a typical talker's distributions.  Thus, what I
intended listeners to treat as two separate clusters of VOTs---one for /b/ and
one for /p/---they may instead have interpreted as one large cluster in extreme
conditions, lacking any labels to tell them otherwise.  I address this
possibility in Experiment 2, by providing labels on a portion of trials.

A second possibility is that listeners are constrained by their prior experience
with other talkers.  The fact that listeners' category boundaries tend to lie
between those of a typical talker and the experimental exposure talker suggests
that this is not implausible.  To assess this possibility more rigorously, after
Experiment 2 I use a Bayesian belief-updating model of distributional learning
to investigate whether the pattern of distributional learning across conditions
is consistent with belief updating starting from common prior beliefs.

# Experiment 2: Semi-supervised distributional learning

In this experiment, some trials are presented with _label_ information, which
indicates _which_ member of the /b/-/p/ minimal pair the speaker intended to
produce.

## Methods

```{r exp2-data, dependson=c()}

data_exp2 <-
  supunsup::supunsup_clean %>%
  filter(supCond %in% c("supervised", "mixed")) %>%
  mutate(trueCat = respCategory,
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

conditions_exp2 <-
  data_exp2 %>%
  group_by(bvotCond, supCond, trueCat) %>%
  summarise(mean_vot = mean(vot)) %>%
  spread(trueCat, mean_vot) %>%
  transmute(vot_cond = paste(b, p, sep=', '),
            ideal_boundary = (b+p)/2) %>%
  ungroup() %>%
  mutate(vot_cond = factor(vot_cond, levels=levels(conditions_exp1$vot_cond)),
         supervised = fct_recode(supCond, supervised="mixed"))

n_subj_exp2 <- length(unique(data_exp2$subject))

excluded_exp2 <- supunsup::supunsup_excluded %>%
  inner_join(conditions_exp2) %>%
  group_by(subject, supCond, bvotCond, vot_cond) %>% 
  summarise() %>% 
  left_join(supunsup::excludes, by="subject") %>%
  select(subject, bvotCond, exclude80PercentAcc, rank)

n_excluded_2 <- nrow(excluded_exp2)
n_subj_repeat_2 <- sum(!is.na(excluded_exp2$rank))
n_subj_bad_2 <- sum(!is.na(excluded_exp2$exclude80PercentAcc))
n_both_2 <- n_subj_repeat_2 + n_subj_bad_2 - n_excluded_2 

n_total_2 <- n_subj_exp2 + n_excluded_2

n_excl_cond_2 <- excluded_exp2 %>%
  group_by(bvotCond) %>%
  tally() %>%
  arrange(n)

```

### Procedure

```{r exp2-label-dists, fig.width=9, fig.height=4.5, fig.cap='Distribution of labeled and unlabeled trials in Experiment 2.  These conditions have the same number of labeled and unlabeled trials, but differ in how they are assigned to VOTs.  In the "mixed" condition, most VOTs occurred in a mixture of labeled and un-labeled trials.  In the "separated" condition, each VOT always occurred as either labeled or unlabeled.  Because no difference was found between learning in these conditions, they are analyzed together.'}

data_exp2 %>%
  mutate(supCond = fct_recode(supCond, separated="supervised")) %>%
  group_by(supCond) %>%
  filter(bvotCond == 0) %>%
  filter(subject == first(subject)) %>%
  group_by(supCond, labeled, vot) %>%
  tally() %>% 
  ggplot(aes(x=vot, y=n, fill=labeled)) +
  geom_bar(stat="identity") +
  facet_grid(.~supCond) +
  scale_fill_manual(values=c("black", "gray")) +
  scale_x_continuous(breaks = c(0, 40), labels = c("/b/ mean", "/p/ mean")) +
  labs(x = "",
       y = "Count",
       title = "Experiment 2: semi-supervised learning",
       subtitle = "Distribution of labeled trials in two semi-supervised learning conditions")

```

The procedure was identical to that of Experiment 1, with one change.  Half of
the trials were _unlabeled_ as in Experiment 1: the response options were the
/b/ and /p/ ends of the continuum for that trial's word were present (e.g.,
"beach" and "peach").  The other half the trials were _labeled_: if the word was
drawn from a "beach" to "peach" continuum and the VOT was sampled from the lower
VOT cluster, than the response options could be "beach" and either "peak" or
"peas".  That is, the only response that was compatible with the _rest_ of the
word had an onset consistent with the VOT cluster that that trial was drawn
from.

Two different semi-supervised conditions were run which differed in how labeled
and unlabeled trials were distributed across VOTs (@Fig:exp2-label-dists).  No
differences were found between behavior in these conditions so they are analyzed
analyzed together.[^supconds]

[^supconds]: The two conditions are listed separately in the
    [`supunsup`](https://github.com/kleinschmidt/phonetic-sup-unsup) R data
    package.

<!-- TODO: this should go in a footnote in the introduction -->
The only other difference with Experiment 1 was that the -10ms, 30ms condition
was not included.  This is because these two experiments were originally run
concurrently as a pilot for an imaging study.  The original goal of the
supervised conditions of Experiment 2 was to see if distributional learning
could be accelerated by including some labeled trials.  The -10ms, 30ms
condition of Experiment 1 was run after analyzing data from Experiments 1 and 2
together.

### Participants

I recruited `r n_total_2 ` participants via Amazon's Mechanical Turk.  As in
Experiment 1, participants were paid $2.00 for participation.  Exclusion criteria
were the same as Experiment 1, and data from `r n_excluded_2` participants were
excluded from analysis: `r n_subj_repeat_2` for repeated participation
(including participants who had participated in Experiment 1), `r n_subj_bad_2` for
inaccurate categorization of unambiguous VOTs, with `r n_both_2` excluded for
both criteria.  This left data from `r n_subj_exp2` remaining for analysis.

## Results

```{r exp2-regression-data, dependson=c("exp2-data")}

data_exp2_mod <- supunsup::supunsup_clean %>%
  filter(labeled == "unlabeled", bvotCond != "-10") %>%
  select(subject, supCond, labeled, bvotCond, trial, vot, respP) %>%
  mutate(vot_s = (vot - mean(vot)) / sd(vot),
         trial_s = (trial - mean(trial)) / sd(trial),
         supervised = fct_recode(supCond, supervised="mixed"))

```

```{r exp2-regression, dependson=c("exp2-regression-data")}

f_noint <- respP ~ 0 + bvotCond * supervised * vot_s * trial_s +
  (1 + vot_s | subject)

## b_logit_exp2 <- brm(f_noint,
##                data = d2,
##                family = bernoulli(),
##                chains=4,
##                iter=1000)

## saveRDS(b_logit_exp2, "models/brm_logistic_sup_v_unsup.rds")
b_logit_exp2 <- readRDS("models/brm_logistic_sup_v_unsup.rds")

exp2_blocks <-
  data_exp2_mod %>%
  group_by(block=ntile(trial, 3)) %>%
  summarise_at(vars(trial, trial_s), mean)

# overall fit (fixed effects):
data_pred_exp2 <-
  cross_df(list(vot_s = seq(min(data_exp2_mod$vot_s),
                            max(data_exp2_mod$vot_s),
                            length.out=100),
                bvotCond = unique(data_exp2_mod$bvotCond),
                supervised = unique(data_exp2_mod$supervised),
                block = 1:3)) %>%
  left_join(exp2_blocks, by="block") %>%
  left_join(conditions_exp1, by="bvotCond") %>%
  mutate(vot = vot_s * sd(data_exp2_mod$vot) + mean(data_exp2_mod$vot),
         bvotCond = fct_drop(bvotCond),
         vot_cond = fct_drop(vot_cond))


data_pred_subj_exp2 <-
  data_exp2_mod %>%
  group_by(subject, supervised, bvotCond) %>%
  summarise() %>%
  left_join(data_pred_exp2)

expt2_class_bysub <-
  fitted(b_logit_exp2, newdata=data_pred_subj_exp2) %>%
  as_tibble() %>%
  bind_cols(data_pred_subj_exp2)

expt2_bounds_bysub <-
  expt2_class_bysub %>%
  group_by(vot_cond, supervised, subject, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt2_bounds_fixef_samples <-
  tidybayes::linpred_draws(b_logit_exp2, data_pred_exp2, re_formula=NA) %>%
  group_by(vot_cond, supervised, block, trial, .draw) %>%
  find_bound(x=vot, y=.value)

expt2_bounds_fixef <-
  expt2_bounds_fixef_samples %>%
  group_by(vot_cond, supervised, block, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))

```

```{r expt2-label-consistent}
label_consistent <- data_exp2 %>%
  filter(labeled == "labeled") %>%
  summarise(p = round(100*mean(respCat == trueCat))) %>%
  pull(p)

label_range <- data_exp2 %>%
  filter(labeled == "labeled") %>%
  group_by(subject) %>%
  summarise(p = round(100*mean(respCat == trueCat))) %>%
  pull(p) %>%
  range()

```


```{r expt2-bounds-sup-diff}

expt2_bounds_fixef_samples %>%
  group_by(vot_cond, block, trial) %>%
  spread(supervised, vot) %>%
  mutate(vot_diff = unsupervised - supervised) %>%
  summarise(mean = mean(vot_diff),
            low = quantile(vot_diff, 0.025),
            high = quantile(vot_diff, 0.975))


```

```{r expt2-bounds, fig.cap='Providing 50% labeled trials during distributional learning makes no difference in listener\'s category boundaries (Experiment 2, solid voilins/points) compared to purely unsupervised learning (Experiment 1, outlined violins/points, re-plotted from @fig:expt1-boundaries).  For ease of comparison with results from Experiment 1, the same axis limits are used as @fig:expt1-boundaries; this lead to exclusion from the figure of five listeners with boundaries greater than 50ms (2 in the 20ms /b/ condition, 3 in the 30ms /b/ condition).  All participants are included in the regression model from which the fixed effects boundaries are estimated (points/lines).'}

expt2_bounds_bysub %>%
  filter(block==3) %>%
  ggplot(aes(x=vot_cond, y=vot, fill=vot_cond, color=vot_cond)) +
  geom_violin(data= . %>% filter(supervised=="unsupervised"), alpha=0)+
  geom_violin(data= . %>% filter(supervised=="supervised"), alpha=0.5, color=NA)+
  geom_pointrange(data = expt2_bounds_fixef %>% filter(block==3),
                  aes(y=mean, ymin=low, ymax=high, shape=supervised),
                  position=position_dodge2(width = .2),
                  color="white", show.legend=FALSE) +
  theme(legend.position="none") +
  scale_shape_manual(values = c(16, 1)) +
  lims(y=c(10,50)) +
  labs(title="Experiment 2: Semi-supervised adaptation",
       subtitle="Compared with unsupervised (Experiment 1)",
       y = "Boundary VOT (ms)",
       x = "Exposure condition (/b/, /p/ mean VOT)") +
  coord_flip()

```

First, listeners were highly sensitive to the labels on labeled trials,
responding consistent with the label `r label_consistent`% of the time (range
across participants of `r label_range[1]`--`r label_range[2]`%).  That is, listeners
used labels to guide their responses on that trial.

Second, did listeners use labels to guide or accelerate their _learning_?  As
with Experiment 1 I assessed listeners' distributional learning via category
boundaries estimated with a Bayesian logistic mixed-effects model.  This model
included data from _both_ Experiments 1 and 2 in order to directly assess
effects of supervised learning on the strength and time course of learning.
This model included fixed effects for supervision condition, all fixed effects
from the Experiment 1 model, and all interactions thereof.  The random effects
structure was identical to Experment 1.  Because listeners' responses were so
systematically determined by the label on labeled trials, _only_ unlabeled
trials were included in the model (all trials from Experiment 1, and half of the
trial from Experiment 2).

Figure -@fig:expt2-bounds shows the by-subject and overall category boundaries
for semi-supervised (shaded violins/points) and unsupervised (outlined
violins/points) distributional learning.  As in @Fig:expt1-boundaries, the
boundaries estimated from the model for a trial 5/6 of the way to the end of the
experiment, and the points and lines show the group-level (fixed effects)
estimates and 95% credible intervals.  The intervals for unsupervised and
semi-supervised overlap for all VOT conditions, indicating that the inclusion of
labeled trials makes no reliable difference in distributional learning, at least
at the trial where these boundaries are estimated.

Similarly, the 95% credible intervals for all fixed effects coefficients
involving the supervision condition include 0, suggesting that the presence of
labeled trials makes no difference in the baseline, strength, or time course of
distributional learning.

The condition that comes the closest to showing any effect is the 30ms /b/, 70ms
/p/ VOT, which is the most extreme shift relative to standard VOT distributions.
However, the fixed effect boundaries in this condition may have been biased by
the presence of three outlier participants with unreliable responses which leads to
very shallow classification functions and very high estimated boundaries
(greater than 50ms).  These participants weren't so unreliable as to be excluded by
the pre-set exclusion criteria so they are included in the analysis here for the
sake of completeness.  Even so, there is still no significant (Bayesian
$p<0.05$) effect of supervision in any regression coefficient _or_ the estimated
boundary, and the single-subject boundary distribution for this condition
(violins in @Fig:expt2-bounds) are very similar.

## Discussion

The results of Experiment 2 suggest that labeling individual VOTs as /b/ or /p/
does very little to affect listeners' distributional learning.  Both at the
condition level (fixed effect boundaries) and the subject level (distribution of
participants' boundaries), distributional learning leads to the same shift in
category boundaries regardless of whether all trials are unlabeld (Experiment 1)
or half labeled (Experiment 2).  This in turn suggests that the discrepancy
between listeners learned boundaries and the boundary implied by the exposure
distribution is _not_ due to uncertainty about the whether any particular VOT
was intended to be a /b/ or a /p/.  This is somewhat surprising, given that in
the extremely shifted 30ms /b/, 70ms /p/ condition most of the tokens from the
/b/ distribution would normally be classified as /p/ under a typical talker's
generative model.

One possible interpretation of this result is that the way that label
information was provided made the labels somehow unavailable to update
listeners' beliefs about how phonetic categories are acoustically realized.
This is not particularly plausible, for two reasons.  The first is that a wide
variety of labeling information has been shown to be effective in phonetic
recalibration, including lexical [@Kraljic2005; @Norris2003],
visual/articulatory [@Bertelson2003], and orthographic [@Keetels2016a] labels.
The second is that listeners were highly sensitive to the labeling information
in generating their responses _on labeled trials_, it was only on _unlabeled_
trials where the presence of labels on other trials seems to make no difference.

Another possibility is that even without labels, listeners are able to pick up
on the distributions they are exposed to, and so labeling some of the trials as
a clue to which distribution they come from doesn't provide any additional
information.  If this is the case, how can the discrepancy between the
boundaries implied by those distributions and the boundaries listeners actually
learned?  Experiment 3 addresses this question via computational modeling.

# Experiment 3---Computational modeling

Experiments 1 and 2 together present a puzzle: on the one hand, distributional
learning appears to be "incomplete", with listeners using category boundaries
that are reliably different from the boundaries implied by the VOT distributions
they were exposed to.  On the other hand, providing listeners extra
information---in the form of category labels for half the VOTs they heard---did
not have any effect on distributional learning.  This suggests that listeners
_know_ the distributions that the exposure talker produces, but they just don't
_believe_ them.  Circumstantial evidence to this effect comes from the fact that
the category boundaries that listeners _do_ use appear to be a compromise
between the category boundaries derived from the VOT distributions for the
exposure talker and those produced by a typical talker of American English.

In this experiment I use a computational model to explore the possibility that
listeners start at some common starting point and update their beliefs based on
experience with the current talker.  Specifically, I use a model based on the
ideal adapter framework [@Kleinschmidt2015b], which is based on the idea that
listeners maintain uncertain beliefs about the distribution of cues
corresponding to each phonetic category.  Crucially, according to the ideal
adapter, when listeners encounter an unfamiliar talker, they initially entertain
a set of prior beliefs about the phonetic cue distributions that an unfamiliar
talker is likely to produce, which takes into account the variability across
talkers that they have previously experience, and any available socio-indexical
information that may be informative about this talker's cue distributions
[@Kleinschmidt2019].  After direct experience with that talker, a listener will
_update_ their beliefs about that talker's cue distributions, bringing them into
better alignment with the cue distributions that the talker has actually
produced.

As a result, at any given point a listener's beliefs about that talker's cue
distributions will be a _compromise_ between what they expected a priori, and
the distributions they have actually encountered.  When a listener has limited
experience with that particular talker, or especially strong, specific prior
expectations, this compromise will be especially obvious.  One prediction of the
ideal adapter is that listeners' prior expectations should be calibrated to the
level (and type) of talker variability they have actually encountered.  Because
there's relatively little variability across talkers in VOT distributions for
word-initial stops [@Kleinschmidt2019], this is exactly such a case where
listeners might be expected to bring strong prior expectations and hence show a
clear compromise between those prior expectations and the distributions that
they encounter in these experiments.



## Methods

I use a Bayesian belief updating model, where listeners begin the experiment
with a shared set of prior beliefs about the mean and variance of the VOT
distributions for /b/ and /p/, and update these beliefs according to Bayes rule
based on the VOTs they hear in the experiment.

Specifically, a listener's uncertain beliefs about the mean and variance of each
category (/b/ and /p/) are represented as Normal-Inverse Chi squared
distribution.  At the beginning of the experiment, every listener has the same
prior beliefs, expressed in the model as parameters for the _expected_ mean and
variance ($\mu_{0}$ and $\sigma^2_0$), and for the _strength_ of these beliefs
($\kappa_0$ for the mean and $\nu_0$ for the variance).  These confidence
parameters are "pseudocounts", in that they are equivalent to assuming that the
listener's expectations about the category mean is based on $\kappa_0$ prior
observations from that category, and likewise for the variance and $\nu_0$.
This prior distribution is a _conjugate prior_ because after observing $n$ data
points with sample mean $\bar x$ and variance $s^2$, the posterior distribution
is also a Normal-Inverse Chi squared, with parameters

$$
\begin{align}
\kappa_n &= \kappa_0 + n \\
\nu_n &= \nu_0 + n \\
\mu_n &= \frac{\kappa_0\mu_0 + n\bar x}{\kappa_0 + n} \\
\sigma^2_n &= \frac{1}{\nu_n}\left(\nu_0 \sigma^2_0 + ns^2 +
  \frac{n\kappa_0}{\kappa_0 + n}(\mu_0 - \bar x)^2\right)
\end{align}
$$

These parameter updates have an intuitive interpretation [@Kleinschmidt2015]:
the confidence parameters are increased by the number of observations $n$ (which
is why they are called "pseudocounts"), while the new expected mean and variance
are weigthed averages of the prior expectations and the observed values, with
the weights determined by the pseudocounts and actual count,
respectively.[^var-mean]

[^var-mean]: For the variance update, there is an additional term that captures
    the possibility that a mismatch between the expected and observed mean could
    be due to a higher than expected variance.

Given updated beliefs about the mean and variance of each category the
probability of any particular VOT $x$ being /b/ vs. /p/ can be calculated by the
marginal likelihood of $x$ under the /b/ and /p/ distributions [see
@Kleinschmidt2015 for more details].  The marginal likelihood is essentially a
weighted average of the likelihood under all possible means and variance for
each category, weighted by how probable each mean/variance is based on the VOTs
observed from each category so far and the prior beliefs.  These marginal
likelihoods are then converted to a probability of responding /p/ or /b/ using
Bayes' rule.

### Assumptions

In actually implementing this model, I make a number of simplifying assumptions
for the sake of tractability.  First, and most importantly, I provide the model
with the mean and standard deviation of the two VOT distributions.  This
substantially simplifies the complexity of the learning problem: in order to
model _unsupervised_ learning, a Bayesian model needs to average predicted
behavior over distributional learning given each possible way that all the
tokens could be jointly classified, weighted by the probability of that
classification.  In an experiment with 200 trials, each of which could be either
/b/ or /p/, this ammounts to more than $10^{60}$ possibilities, an impossibly
large number to consider.  There are approximations for unsupervised or
semi-supervised belief updating, but they substantially increase the
computational complexity and exploring them is left for future work.

Second, the use of a conjugate Normal Inverse Chi-squared prior is also an
important simplification, as it means that the belief updating can be computed
analytically.  Combined with the first assumption, this means that the updated
beliefs can be computed directly from the prior parameters and the observed
count, mean, and variance of each category.

### Model fitting procedure

The free parameters of this model were the prior expected mean and variance of
/b/ and /p/, and the mean and variance prior pseudocounts (which were assumed to
be the same for /b/ and /p/ because pilot simulations suggested it was not
possible to reliably identify both).  I also included a guessing rate parameter,
which captured the fact that many listeners never reach floor or ceiling and
thus may be guessing on a proportion of trials.

The model was fit to data from Experiment 1 (unsupervised distributional
learning) using Stan [@Carpenter2017].  The trials were divided up into six
blocks of equal length (37 trials).  Responses in each block in each
distribution condition were predicted based on updated beliefs at the halfway
point of the block, using the sufficient statistics of that distribution
condition.

## Results

```{r infer-prior}

## options(mc.cores = parallel::detectCores())
## fit_inc <- infer_prior_beliefs(data_exp1,
##                                cue = "vot",
##                                category = "trueCat",
##                                response = "respCat",
##                                condition = "vot_cond",
##                                ranefs = "subject",
##                                n_blocks = 6,
##                                chains = 4,
##                                iter = 2000)
## saveRDS(fit_inc, "models/fit_inc.rds")

fit_inc <- readRDS("models/fit_inc.rds")

categories <-
  data_frame(cat_num = 1:2,
             category = c('b', 'p'))

prior_samples_df <-
  fit_inc %>%
  spread_draws(nu_0, kappa_0, mu_0[cat_num], sigma_0[cat_num]) %>%
  left_join(categories)

## create a data_frame with samples for updated parameters
updated_samples_df <-
  fit_inc %>%
  spread_draws(c(kappa_n, nu_n, mu_n, sigma_n)[block_num, cat_num, cond_num],
               lapse_rate[block_num]) %>%
  left_join(categories) %>%
  left_join(mutate(conditions_exp1, cond_num = as.numeric(bvotCond)))

## create a data_frame for lapsing rate samples
lapse_rate_samples <-
  fit_inc %>%
  spread_draws(lapse_rate[block_num])
```

```{r mod-class-funs, dependson=c("infer-prior")}

mod_class_funs <- 
  updated_samples_df %>%
  group_by(.draw) %>%
  nest() %>%
  sample_n(200) %>%
  unnest() %>%
  mutate(mean=mu_n, sd=sigma_n) %>%
  select(.draw, block_num, bvotCond, vot_cond, category, mean, sd) %>%
  group_by(.draw, bvotCond, vot_cond, block_num) %>%
  do(stats_to_lhood(., noise_sd=0)) %>%
  lhood_to_classification() %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = (1-lapse_rate)*prob_p + lapse_rate/2) %>%
  group_by(bvotCond, vot_cond, block_num, vot) %>%
  select(bvotCond, block_num, vot, prob_p) %>%
  summarise(prob_p_low = quantile(prob_p, 0.025),
            prob_p_high = quantile(prob_p, 0.975),
            prob_p = mean(prob_p))

prior_class_funs <-
  prior_samples_df %>%
  group_by(.draw) %>%
  nest() %>%
  sample_n(200) %>%
  unnest() %>%
  mutate(mean=mu_0, sd=sigma_0) %>%
  select(.draw, category, mean, sd) %>%
  group_by(.draw, category) %>%
  do(stats_to_lhood(., noise_sd=0)) %>%
  lhood_to_classification() %>%
  group_by(vot) %>%
  summarise(prob_p_low = quantile(prob_p, 0.025),
            prob_p_high = quantile(prob_p, 0.975),
            prob_p = mean(prob_p))

```

```{r mod-vs-behav-class-funs, fig.width=10, fig.height=3, fig.cap="Belief updating model classification functions (thin ribbons) vs. average probability /p/ response from Experiment 1, both during the final sixth of trials.  Points show the average proportion of /p/ responses, and CIs show 95% bootstrapped CIs over subjects.  Ribbons show 95% Bayesian credible interval for model posterior predictions, and the dashed black lines show classification function from the inferred prior."}

data_exp1 %>%
  filter(ntile(trial, 6) == 6) %>%
  group_by(subject, vot, vot_cond) %>%
  summarise(respP = mean(respP)) %>%
  ggplot() +
  geom_line(data = prior_class_funs,
            aes(x=vot, y=prob_p),
            color="black", linetype=2) +
  geom_ribbon(data=filter(mod_class_funs, block_num==6),
              aes(x=vot, ymin=prob_p_low, ymax=prob_p_high, fill=vot_cond),
              alpha=0.5) +
  geom_pointrange(aes(x=vot, y=respP, color=vot_cond),
                  stat="summary", fun.data="mean_cl_boot") +
  facet_grid(.~vot_cond) +
  labs(title = "Belief updating model",
       subtitle = "Classification function versus behavioral data for final 6th of trials in Experiment 1",
       x = "VOT (ms)",
       y = "Probability /p/ response")

```

```{r mod-vs-behav-class-funs-all-blocks, eval=FALSE}

# all blocks:
data_exp1 %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(subject, vot, vot_cond, block_num) %>%
  summarise(respP = mean(respP)) %>%
  ggplot() +
  geom_pointrange(aes(x=vot, y=respP, group=interaction(vot_cond, block_num),
                      color=vot_cond),
                  stat="summary", fun.data="mean_cl_boot") +
  ## geom_ribbon(data=mod_class_funs,
  ##             aes(x=vot, ymin=prob_p_low, ymax=prob_p_high, fill=vot_cond),
  ##             alpha=0.5) +
  geom_line(data=mod_class_funs,
            aes(x=vot, y=prob_p, group=interaction(vot_cond, block_num), color=vot_cond)) +
  geom_line(data = prior_class_funs,
            aes(x=vot, y=prob_p),
            color="black", linetype=2) +
  facet_grid(.~block_num)

data_exp1 %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(subject, vot, vot_cond, block_num) %>%
  summarise(respP = mean(respP)) %>%
  ggplot() +
  geom_pointrange(aes(x=vot, y=respP, group=interaction(vot_cond, block_num),
                      color=vot_cond),
                  stat="summary", fun.data="mean_cl_boot") +
  ## geom_ribbon(data=mod_class_funs,
  ##             aes(x=vot, ymin=prob_p_low, ymax=prob_p_high, fill=vot_cond),
  ##             alpha=0.5) +
  geom_line(data=mod_class_funs,
            aes(x=vot, y=prob_p, group=interaction(vot_cond, block_num), color=vot_cond)) +
  geom_line(data = prior_class_funs,
            aes(x=vot, y=prob_p),
            color="black", linetype=2) +
  facet_grid(block_num~vot_cond)


```

Figure -@fig:mod-vs-behav-class-funs shows the belief-updating model's predicted
classification functions in the final sixth of trials in the experiment.
Qualitatively, the model provides a good fit to the overall classification
functions for each VOT condition, and the estimated boundaries align well with
the observed boundaries in each case.  It also provides a good fit to the time
course of learning, capturing changes in the category boundaries in each
condition over the experiment.

```{r modeled-category-boundaries}

modeled_boundaries_samples <-
  updated_samples_df %>%
  group_by(.draw) %>%
  nest() %>%
  sample_n(200) %>%
  unnest() %>%
  mutate(mean=mu_n, sd=sigma_n) %>%
  select(.draw, block_num, bvotCond, vot_cond, category, mean, sd) %>%
  group_by(.draw, bvotCond, vot_cond, block_num) %>%
  do(stats_to_lhood(., xlim=c(0, 50), noise_sd=0)) %>%
  lhood_to_classification() %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = (1-lapse_rate)*prob_p + lapse_rate/2) %>%
  find_bound(vot, prob_p)

modeled_boundaries <- modeled_bounaries_samples %>%
  group_by(bvotCond, vot_cond, block_num) %>%
  summarise_at(vars(vot), funs(mean, low=quantile(., 0.025), high=quantile(., 0.975)))

```

```{r}

bounds_6tile_exp1 <-
  data_exp1 %>%
  select(subject, vot_cond, trial, vot, respP) %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(vot_cond, block_num, vot) %>%
  summarise(respP = mean(respP)) %>%
  find_bound(vot, respP)

```

```{r plot-modeled-category-boundaries, fig.width=6.8, fig.height=5.3, fig.cap="Belief updating model's change in category boundary in each VOT condition over trials"}

ggplot(modeled_boundaries, aes(x=block_num, y=mean)) +
  # geom_line(aes(color=vot_cond)) +
  geom_ribbon(aes(ymin=low, ymax=high, fill=vot_cond), alpha=0.5) +
  scale_x_continuous(breaks = 1:6, limits = c(NA, 6.5)) +
  geom_text(data = . %>% filter(block_num == 6), aes(label = vot_cond, color=vot_cond),
            hjust=0, nudge_x=0.05) +
  labs(x = "Block (each is one sixth of trials)",
       y = "Model /b/-/p/ boundary (ms VOT)",
       title = "Model learning curves",
       subtitle = "Change in /b/-/p/ category boundary with learning") +
  theme(legend.position = "hide") +
  geom_point(data = bounds_6tile_exp1,
             aes(x=block_num, y=vot, color=vot_cond)) +
  geom_line(data = bounds_6tile_exp1,
             aes(x=block_num, y=vot, color=vot_cond))
  ## geom_point(data = expt1_bounds_s,
  ##            aes(x=block, y=vot, color=vot_cond)) +
  ## geom_line(data = expt1_bounds_s,
  ##           aes(x=block, y=vot, color=vot_cond))


# alternative figure: same style as boundary plots from experiments
## ggplot(modeled_boundaries_samples,
##        aes(x = vot_cond, y = vot, fill=vot_cond, alpha = block_num,
##            group=interaction(block_num, vot_cond))) +
##   geom_violin(position="identity", color=NA) +
##   geom_violin(data = expt1_bounds_fixef_samples,
##               aes(alpha=2*block, color=vot_cond, group=interaction(block, vot_cond)),
##               fill=NA, position="identity") +
##   coord_flip()

```

# Experiment 4
