---
title: What constrains distributional learning in adults?
author:
  - name: Dave F. Kleinschmidt
    affiliation: Rutgers Unversity (New Brunswick, NJ, USA) and University of Rochester (Rochester, NY, USA)
    corresponding: "dave.kleinschmidt@rutgers.edu; 152 Frelinghuysen Ave, Piscataway NJ, 08854"
bibliography:
  - /home/dave/Documents/papers/zotero.bib
  - packages.bib
abstract: |
  One of the many remarkable features of human language is it's flexibility:
  during acquisition, any normally-developing human infant can acquire any human
  language, and during adulthood, language users quickly and flexibly adapt to a
  wide range of talker variation.  Both language acquisition in infants and
  adaptation in adults have been hypothesized to be forms of distributional
  learning, where flexibility is driven by sensitivity to statistical properties
  of sensory stimuli and the corresponding underlying linguistic strucutres.
  Despite the similarities between these forms of linguistic flexibility, there
  are obvious differences as well, chief among them being that adults have a
  much harder time acquiring the same unfamiliar languages that they would have
  picked up naturally during infancy.  This suggests that there are strong
  constraints on distributonal learnign during adulthood.  This paper provides
  further, direct evidence for these constraints, by showing that American
  English listeners struggle to learn voice-onset time (VOT) distributions that
  are atypical of American English.  Morever, computational modeling shows that
  the pattern of distributional learning (or lack thereof) across different VOT
  distributions is consistent with Bayesian belief-updating, starting from prior
  beliefs that are very similar to the VOT distributions produced by a typical
  talker of American English.  Together, this suggests that distributional
  learning in adults is constrained by prior experience with other talkers, and
  that distributional learning may be a computational principle of human
  language that operates throughout the lifespan.
output:
    html_document:
        code_folding: hide
        dev: png
        fig_retina: 3
        keep_md: true
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-crossref
    pdf_document:
        md_extensions: +implicit_figures
        fig_crop: false
        keep_tex: true
        latex_engine: xelatex
        template: apa6.template.tex
        citation_package: biblatex
        pandoc_args:
        - --filter
        - pandoc-crossref
reference-section-title: "References"
---


```{r knitr-setup, warning=FALSE, message=FALSE, error=FALSE, echo=FALSE, results='hide'}

library(knitr)
opts_chunk$set(warning = FALSE,
               message = FALSE,
               error = FALSE,
               cache=TRUE,
               results = "hide",
               echo=opts_knit$get("rmarkdown.pandoc.to") != 'latex')

options(digits=2)

## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})
## Produce markdown-formatted table captions with anchors for cross-refs.
## Requires pandoc-tablenos to parse the IDs. Refer to tables
## in text with {@tbl:label} or @tbl:label.
## Based partly on http://stackoverflow.com/a/18672268
##
## (see https://github.com/tomduck/pandoc-tablenos)
knit_hooks$set(tbl.cap = function(before, options, envir) {
  if(!before){
    paste0('\n\nTable: ', options$tbl.cap,
           ' {#tbl:', options$label, '}', sep = '')
  }
})

```

```{r preamble, cache=FALSE}

library(tidyverse)
library(dplyr)
library(glue)
library(magrittr)
library(beliefupdatr)
library(supunsup)

library(rstan)
library(brms)
library(loo)
library(tidybayes)

library(cowplot)
theme_set(cowplot::theme_cowplot() +
            theme(plot.title=element_text(hjust=0)))
library(latex2exp)

## devtools::install_github('kleinschmidt/daver')
library(daver)
## devtools::install_github('kleinschmidt/phonetic-sup-unsup')
library(supunsup)
## devtools::install_github('kleinschmidt/beliefupdatr')
library(beliefupdatr)

```



# Introduction

A basic fact of human language is that any typically developing human
infant can learn any human language. Human languages vary dramatically
at every level, including the basic sound systems they use, and the
human language faculty must be flexible enough to deal with this
substantial cross-linguistic variability. The first stages of language
acquisition are characterized by initial flexibility, which declines over
development as the particulars of the native language are acquired. For
instance, as infants become better at discriminating linguistically
important sounds in their native language, they simultaneously *lose*
the ability to discriminate some sounds that are important for other
languages but not their native language
[@Best1995; @Kuhl1992; @Werker1984; but see @Best1988]. Ultimately, people become
sufficiently inflexible over development that they generally struggle to
learn another language in adulthood [@Hartshorne2018].

However, adult listeners still need to deal with substantial variability
*within* their native language, as talkers differ in how they realize
the phonetic categories of the language using acoustic cues [e.g.
@Allen2003; @Newman2001; @Clopper2005]. Accordingly, adult listeners
flexibly adapt to unfamiliar talkers in a wide variety of contexts. At
one extreme, perception of heavily accented non-native talkers becomes
faster and more accurate with just a few minutes of exposure
[@Clarke-Davidson2004; @Bradlow2008; @Baese-berk2013]. At the other
extreme, listeners recalibrate representations of individual phonetic
categories based on subtle changes in single segments in otherwise
unaccented talkers [@Kraljic2006; @Norris2003; @Bertelson2003].

Both acquisition and adaptation have been theorized to be forms of
distributional learning.  First, at a computational level [in the sense of
@Marr1982], acquisition can be treated straightforwardly as a problem of
distributional learning.  The
probabilistic relationships between different linguistic and sensory units
define the structure of the language, and so learning the structure of your
language requires learning---at least approximately and implicitly---the
distributions associated with different structures in the language.  Likewise,
accommodating talker variability is also, at a computational level, a problem of
learning the speaker- or context-specific probabilistic relationships between
observable cues and underlying linguistic structures.  This similarity is
demonstrated by the computational modeling which shows that both acquisition
[@McMurray2009b; @Vallabha2007; @Feldman2013; but see @Hitczenko2018] and rapid
adaptation [@Kleinschmidt2015b; @Theodore2019] can be treated as forms of
distributional learning.

Second, both adults and infants are sensitive to distributional
properties of speech. One set of findings shows that listeners (both
infants and adults) become more sensitive after exposure to a bimodal
distribution of an acoustic cue (like length, voice-onset time, vowel
formant frequencies, etc.) compared with exposure to a unimodal
distribution [e.g. @Escudero2011; @Goudbeek2008;
@Maye2000; @Maye2002; @Feldman2013b], although the evidence that very young
infants can benefit from this kind of exposure in short laboratory sessions is
equivocal [@Cristia2018]. Another set of findings shows that
adult listeners can adapt to changes in the means and/or variances of
the cue distributions for known phonetic categories [e.g.
@Clayards2008; @Theodore2015; @Theodore2019; @Colby2018; @Chladkova2017;
@Schreiber2013].
What both of these sets of findings have in common is that listeners
pick up on the distributions of cues without any explicit instruction
about the intended category label associated with each token. For
example, @Clayards2008 had listeners listen to /b/-/p/ minimal
pair words (e.g., "beach/peach") with different voice-onset times
(VOT), and click on a matching picture to indicate which member of the
minimal pair they heard. On every trial, the VOT was drawn from one of
two bimodal distributions, which had clusters with the same means but
different variances across subjects. Listeners in the high-variance
condition produced shallower categorization functions, reflecting
greater uncertainty associated with the wider range of VOTs they heard
for each cluster.

If both acquisition and adaptation can be treated as forms of
distributional learning, and both infants and adults are sensitive to
distributional information, what distinguishes acquisition from
adaptation? For one, it seems that distributional learning in adults is
*constrained*. Adult listeners struggle to learn new categories that are
not present in their native language. For instance, Japanese listeners
struggle to discriminate the English /r/-/l/ contrast, which corresponds
a single category in their native language [@Goto1971; @Miyawaki1975].
Long-term naturalistic exposure is not sufficient to achieve good
discrimination of this contrast, even after conversational competence
has been achieved [@Takagi1995]. While perception of this contrast can
be improved somewhat by training, it requires extensive training and
these listeners seldom achieve native-like levels of performance
[@Bradlow1997].

There are also apparent constraints on the ability of adult listeners to
adapt to variations in the distributions associated with native language
categories. For instance, @Idemaru2011 tested how well listeners adapt
to distributions of two cues that distinguish voicing (e.g., /b/ vs.
/p/), voice onset time (VOT, the primary cue to voicing) and the pitch
of the following vowel (f0, a secondary cue). These two cues are
typically positively correlated in English, with /p/ corresponding to
high VOT and high f0, and /b/ to low values of both cues [@Kohler1982].
In one condition, listeners were exposed to a talker who produced a
positively correlated distribution of these cues. During a post-test,
these listeners used f0 to categorize stops with ambiguous VOTs. In
another condition, listeners heard a talker who produced an
*un*correlated distribution, where f0 is uninformative. In contrast to
the listeners in the first condition, during the post test these
listeners _ignored_ f0 even for ambiguous VOTs. This effect is
consistent with the idea that listeners are rationally integrating
multiple cues to voicing, weighing them based on how informative they
are [@Ernst2004; @Clayards2008; @Bejjanki2011]. However, listeners in a
third condition who were exposed to a talker who produced an
*anti*correlated distribution did _not_ follow the predictions of
rational cue integration. Despite the fact that f0 was just as
informative for this accent as for the positively correlated accent,
listeners _ignored_ f0 as a cue to voicing. This suggests that these
listeners have ruled out the possibility of a reversed mapping between
f0 and voicing (/b/ vs. /p/), possibly perhaps American English talkers
typically do not typically produce it [e.g., @House1953]. Likewise,
@Sumner2011 found that American English listeners had trouble adapting
to a talker who produced VOT distributions for /b/ and /p/ that had
_substantially_ lower means (approximately -60ms and 0ms, respectively)
than a typical American English talker [approximately 0--10ms and 60ms
VOT; @Lisker1964].

So on the one hand, distributional learning provides a unifying
theoretical perspective on flexibility in language acquisition and
adaptation. On the other hand, it highlights an important difference
between these two basic kinds of plasticity in the language system. From
the lens of distributional learning, one major difference between
acquisition and adaptation is that distributional learning in adulthood
appears to be *constrained*, while it is relatively *unconstrained*
during acquisition.

However, we lack a clear understanding of the nature and source of those
constraints. There are a number of other differences between the
learning problems posed by acquisition and adaptation, even if both are
forms of distributional learning. First, distributional learning in
infancy is, at least initially, almost entirely unsupervised, meaning
that there is very little direct information about whether any two observed
sounds come from the same cluster (category) or different
ones.  Any such information to this effect that *is* available comes from
context that these sounds occur in, like the fact that sounds are embedded in
words, which infants begin to learn early, in parallel with the phonetic
categories of their language [@Bergelson2012].  While this kind of
"self-supervision" can serve as an important additional source of information in
distributional learning [@Feldman2013b; @Feldman2013], it is much weaker than
the richer, less uncertain circumstantial evidence that adults can draw on from
the lexicon, pragmatic context, phonotactics, etc.  These contextual sources
often provide a great deal of information about the intended category for a
particular cue value, which makes the distributional learning problem of
adaptation often (but not always) effectively a supervised learning problem.

Second, when adapting to an unfamiliar talker, adults have a great deal
of prior experience with *other* talkers which they could use to narrow
down the possible distributions they ought to expect
[@Kleinschmidt2015b]. Both of these factors might contribute to
constraints on adult distributional learning. For the first, if adult
adaptation typically operates in a *supervised* setting, the fully
unsupervised setting of a typical distributional learning experiment
might not provide enough information, leading to reduced learning. For
the second, if the distributions encountered in an experiment fall far
enough outside the range of what a listener expects based on their prior
experience, they may struggle to adapt [@Kleinschmidt2015b].

The goal of this paper is to systematically probe the constraints on
distributional learning in American English-speaking adults. Experiment
1 tests the ability of American English listeners to change their
classification of word initial stop voicing based on experience with a
range of distributions of voice-onset time (VOT).  The results of this
experiment show that distributional learning by adults is, in fact, constrained,
and moreover these constraints are qualitatively consistent with prior
experience with typical talkers of American English. Experiment 2 tests an
alternative possible
constraint on distributional learning, which is the absence of *labels*,
which could lead to uncertainty about whether the bimodal distribution
really corresponds to the standard English categories of voiced and
voiceless stops of /b/ and /p/. Surprisingly, telling listeners whether
a particular VOT was intended to be a voiced /b/ or a voiceless /p/ on
half of the trials has no effect on the speed or completeness of
distributional learning. Experiment 3 uses a Bayesian belief-updating
model to test whether the constraints observed in Experiment 1 can be
explained as belief updating starting from a common set of prior beliefs
that is shared by all of the subjects.  Finally, Experiment 4 tests
distributional learning for a wider range of VOT distributions, and the results
suggest that the constraints on distributional learning may be more complex than
revealed by Experiments 1--3.

Together, these results show that distributional learning in adults *is*
constrained, and these constraints are at least consistent with belief
updating starting from a set of prior beliefs about the VOT
distributions that a typical talker of American English will produce.

# Experiment 1: Distributional learning of voicing from VOT

<!-- 
    TODO: point out connection with recalibration (-10, 30 vs. 30, 70)...or
    maybe in Expt 2 intro??
 -->

In order to probe the possible constraints on distributional learning,
Experiment 1[^source-online] measures listener's distributional learning of a range of bimodal
distributions of voice-onset time (VOT).  These distributions vary in how
similar they are to a typical American English talker (Figure
-@fig:vot-dists-exp1).  Each distribution has the same variance and separation
between the VOT clusters (40ms), but the overall VOTs are shifted up or down in
10ms increments.  Previous results have shown that listeners are sensitive to
such shifts in distributions [@Munson2011], but have not systematically explored
the wide range of shifts used here.  While some of these VOT distributions
deviate substantially from American English, they are not outside the range of
variation across other languages [see e.g. @Lisker1964; @Cho2019].  Thus, if
distributional learning by these adult listeners is unconstrained, their
category boundaries should be close to the optimal category boundaries for the
VOT distributions they were exposed to.

[^source-online]: The data from the experiments reported in this paper is
    available downloaded via the [`supunsup` R
    package](https://github.com/kleinschmidt/phonetic-sup-unsup/), and the R
    code to reproduce these analyses (in the form of an RMarkdown file) can be
    found at [osf.io/3wdp2/](https://osf.io/3wdp2/).

```{r data-exp1}

data_exp1 <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(trueCat = respCategory,
         subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

conditions_exp1 <-
  data_exp1 %>%
  group_by(bvotCond, trueCat) %>%
  summarise(mean_vot = mean(vot)) %>%
  spread(trueCat, mean_vot) %>%
  transmute(vot_cond = paste(b, p, sep=', '),
            ideal_boundary = (b+p)/2) %>%
  ungroup() %>%
  mutate(vot_cond = factor(vot_cond, levels=vot_cond))

data_exp1 <- left_join(data_exp1, conditions_exp1, by="bvotCond")

```

```{r vot-dists-exp1, fig.width=10, fig.height=3, fig.cap="Each subject heard one of these five synthetic accents, which differ only in the distribution of VOTs of the word-initial stops. Black dashed lines show VOT distributions from a hypothetical typical talker [as estimated by @Kronrod2016]. Note that the 0 and 10ms shifted accents are reasonably close to this typical talker, while the -10, 20, and 30ms shifted accents deviate substantially."}

prior_stats_by_talker <-
  votcorpora::vot %>%
  filter(source %in% c('gva13', 'bbg09', 'buckeye'),
         place == 'lab') %>%
  mutate(source = ifelse(source %in% c('gva13', 'bbg09'),
                         'goldricketal',
                         source)) %>%
  group_by(source, prevoiced, subject, phoneme) %>%
  summarise(mu = mean(vot),
            sigma2 = var(vot),
            sigma = sd(vot),
            n = n()) %>%
  rename(category = phoneme)

prior_stats <-
  prior_stats_by_talker %>%
  filter(source == 'goldricketal', category == 'b') %>%
  group_by(source, prevoiced, category) %>%
  summarise_at(vars(mu, sigma2, sigma, n), funs(mean, var, sum)) %>%
  transmute(category,
            mean = mu_mean,
            sd = sqrt(sigma2_mean),
            n = n_sum) %>%
  bind_rows(supunsup::prior_stats %>%
              filter(source=='kronrod2012') %>%
              mutate(n = 1))

prior_lhood <- 
  prior_stats %>%
  filter(source == 'kronrod2012') %>%
  supunsup::stats_to_lhood()

prior_class <- prior_lhood %>% lhood_to_classification()


exposure_stats <- data_exp1 %>%
  group_by(vot_cond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sd_noise = sqrt(82)

exposure_lhood <- exposure_stats %>%
  group_by(vot_cond) %>%
  do(supunsup::stats_to_lhood(., sd_noise))

data_exp1 %>%
  group_by(vot_cond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=vot_cond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  geom_text(data=data.frame(vot_cond='-10, 30'), x = 10, y = 60,
            label = 'Typical Talker',
            color='black', hjust=0, vjust=0.3, size=3) +
  geom_text(data=data.frame(vot_cond='-10, 30'), x = 40, y = 50,
            label = 'Exposure\nTalker',
            hjust=0, vjust=0.8, size=3,
            lineheight=1) + 
  facet_grid(.~vot_cond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  labs(title="Experiment 1: VOT distribution conditions",
       subtitle="Similarity of exposure talker to typical talker controlled by shifting means of /b/ and /p/ distributions")
  ## scale_fill_discrete('/b/, /p/\nmean VOT') ## +
  ## theme(legend.position='none')

```

## Methods

### Participants {#sec:participants}

```{r participants-exp1}

n_subj <- data_exp1 %>% group_by(subject) %>% summarise() %>% tally()

excluded <- supunsup::supunsup_excluded %>%
  filter(supCond == 'unsupervised') %>%
  group_by(subject, bvotCond) %>% 
  summarise() %>% 
  left_join(supunsup::excludes, by="subject") %>%
  select(subject, bvotCond, exclude80PercentAcc, rank)

n_excluded <- nrow(excluded)
n_subj_repeat <- sum(!is.na(excluded$rank))
n_subj_bad <- sum(!is.na(excluded$exclude80PercentAcc))
n_both <- n_subj_repeat + n_subj_bad - n_excluded 

n_total <- n_subj + n_excluded

n_excl_cond <- excluded %>%
  group_by(bvotCond) %>%
  tally() %>%
  arrange(n)

```

`r n_total` participants were recruited via Amazon's Mechanical Turk. Participants were
paid \$2.00 for participation, which took about 20 minutes.  Participants were excluded
who participated more than once ($n=`r n_subj_repeat`$) or who failed to
classify VOTs reliably ($n=`r n_subj_bad`$; $n=`r n_both`$ for both reasons).
Reliable classification is defined as accuracy of at least 80% /b/ at 0 and 80%
/p/ at 70ms
VOT. Because some conditions had few stimuli with these VOTs, 
participants' responses were extrapolated using a logistic generalized linear model (GLM).  Excluded
participants were roughly equally distributed across conditions (maximum of 
`r last(n_excl_cond$n)` in `r last(n_excl_cond$bvotCond)`ms /b/ VOT condition,
and minimum of `r first(n_excl_cond$n)` in `r first(n_excl_cond$bvotCond)`ms /b/
VOT condition). After these exclusions, data from `r n_subj` participants remained
for analysis.

### Procedure

![Example trial display (beach/peach). Listeners first click on the
    green button to play the word, then click on one picture to indicate what
    they heard.](figure_manual/beach_peach.png){#fig:beach-peach}

The procedure is based on @Clayards2008. Figure -@fig:beach-peach shows an
example trial display.  On each trial, two response option images appeared,
which corresponded to one of three /b/-/p/ minimal pairs (beach/peach,
bees/peas, or beak/peak).  Participants started each trial by clicking on a button
between the two pictures, which played the corresponding minimal pair word audio
stimulus. Participants then clicked on the picture to indicate whether they heard
the /b/ or /p/ member of the minimal pair. Participants performed 222 of these
trials, evenly divided between the three minimal pairs, in random order.

Each trial's word was synthesized with a voice onset time (VOT) that was
randomly drawn from a bimodal distribution, with low and high VOT clusters
implicitly corresponding to /b/ and /p/, respectively. This distribution defined
the *accent* that the subject heard, and each subject was pseudorandomly
assigned to one of five accent conditions ([@Fig:vot-dists-exp1]).

### Materials

The audio and visual stimuli were identical to those in
@Clayards2008. Three /b/-/p/ minimal pair audio continua were synthesized using
the 1988 Klatt synthesizer [@Klatt1980], by manipulating VOT in 10ms increments
(either adding voicing before the stop burst to create negative VOTs, or
aspiration after for positive VOTs). Within a /b/-/p/ continuum, the other
parameters were held constant, and modeled on natural tokens of the endpoints
(beach/peach, bees/peas, and beak/peak).

## Results

In order to assess distributional learning, each subject's classification
function is compared to two baselines, corresponding to _no learning_ and
_complete learning_.  The _no learning_ baseline is derived from the category
boundary for a _typical talker_'s VOT distributions [based on @Kronrod2016], and
the category boundary based solely on the exposure talker's distributions.  I
refer to the typical talker's boundary as the "no learning" baseline, and the
exposure talker's boundary as the "complete learning" baseline.

Each subject's classification function was estimated with a Bayesian multilevel
(mixed-effect) logistic regression, using the `brms` package [@Burkner2017] for
`R` [@R-base].[^expt1-formula]  This approach simultaneously estimates the group-level
effects of the experimental manipulation (VOT shift condition) on the /b/-/p/
category boundary location and sharpness, and each individual participants'
boundary locations and slopes.  A multilevel approach like this has the benefit
of properly accounting for and balancing the joint uncertainty about the group-
and individual-level effects [@Gelman2007].[^shrinkage]  A specific advantage of a
Bayesian approach, over standard mixed-effects models as fit with
`lme4`, is that the Bayesian approach allows for direct estimates of
subject-level effects (i.e., category boundaries), with associated uncertainty.

[^expt1-formula]: The model formula was `respP ~ 1 + bvotCond * vot_s *
    trial_s + (1 + vot_s | subject)`, where `respP` is a binary variable with 0
    for a /b/ response and 1 for /p/, `bvotCond` is the exposure condition
    (treatment-coded), and `vot_s` and `trial_s` are the centered and scaled VOT
    and number of each trial, respectively.

[^shrinkage]: One effect of using a multilevel approach is that the estimate of
    each subject's boundary is "shrunk" towards the group-level estimate.  This
    reflects the assumption that participants are drawn from the same population,
    and hence the data from one subject in a group are informative about other
    participants in that group (and vice versa).  The amount of this shrinkage
    depends on the amount of data available from each individual subject, and
    the consistency of the individual subject estimates.  In this case, each
    subject contributes sufficient data to make shrinkage mild, and an
    alternative analysis that estimates each participants' boundary with a separate
    logistic GLM produces qualitatively similar effects.

```{r exp1-regression, dependson=c("data-exp1")}

data_exp1_mod <-
  data_exp1 %>%
  select(subject, bvotCond, trial, vot, respP) %>%
  mutate(vot_s = (vot - mean(vot)) / sd(vot),
         trial_s = (trial - mean(trial)) / sd(trial))

# b_logit_exp1 <- brm(respP ~ 1 + bvotCond * vot_s * trial_s + (1 + vot_s | subject),
#                     data=data_exp1_mod, family=bernoulli(), chains=4, iter=1000)
# saveRDS(b_logit_exp1, "models/brm_logistic_exp1.rds")
b_logit_exp1 <- readRDS("models/brm_logistic_exp1.rds")

# extract fitted classification functions:
# first get trials from first, second, and third thirds
exp1_blocks <-
  data_exp1_mod %>%
  group_by(block=ntile(trial, 3)) %>%
  summarise_at(vars(trial, trial_s), mean)

# overall fit (fixed effects):
data_pred <-
  cross_df(list(vot_s = seq(min(data_exp1_mod$vot_s),
                            max(data_exp1_mod$vot_s),
                            length.out=100),
                bvotCond = unique(data_exp1_mod$bvotCond),
                block = 1:3)) %>%
  left_join(exp1_blocks, by="block") %>%
  mutate(vot = vot_s * sd(data_exp1_mod$vot) + mean(data_exp1_mod$vot)) %>%
  left_join(conditions_exp1, by="bvotCond")

expt1_class <-
  fitted(b_logit_exp1, newdata=data_pred, re_formula=NA) %>%
  as_tibble() %>%
  bind_cols(data_pred)

# by-subject fit (fixed+random effects):
data_pred_subj <-
  data_exp1_mod %>%
  group_by(subject, bvotCond) %>%
  summarise() %>%
  left_join(data_pred, by="bvotCond")

expt1_class_bysub <-
  fitted(b_logit_exp1, newdata=data_pred_subj) %>%
  as_tibble() %>%
  bind_cols(data_pred_subj)

# get x-intercept (category boundary):
#' Find x intercept with linear interpolation of two nearest points
#'
#' Respects grouping
#'
#' @param d a tibble
#' @param x (unquoted) column with the x coordinates
#' @param y (unquoted) column with the y coordinates
#' @param ytarget (default=0.5) y value to find x intercept at
find_bound <- function(d, x, y, ytarget=0.5) {
  x <- enquo(x)
  x_name <- quo_name(x)
  y <- enquo(y)
  d %>%
    arrange((!!y - ytarget)^2) %>%
    filter(row_number() <= 2) %>%
    summarise(!!x_name := (ytarget - first(!!y)) * diff(!!x)/diff(!!y) + first(!!x))
}


expt1_bounds <-
  expt1_class %>%
  group_by(vot_cond, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt1_bounds_bysub <-
  expt1_class_bysub %>%
  group_by(vot_cond, subject, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt1_bounds_fixef_samples <-
  tidybayes::linpred_draws(b_logit_exp1, data_pred, re_formula=NA) %>%
  group_by(vot_cond, block, trial, .draw) %>%
  find_bound(x=vot, y=.value)

```

```{r exp1-results, fig.width=10, fig.height=3, fig.cap="Results from experiment 1 show that distributional learning is constrained, with individual subjects' classification functions (fine colored lines) representing an apparent compromise between the ideal boundary given a typical talker's distributions (skinny, black dashed lines), and the ideal boundary for the exposure distributions (heavy colored dashed lines; see @fig:vot-dists-exp1 for both).  Subjects' classification functions were estimated for the final third of trials (i.e., at trial 185 out of 222) based on the posterior distributions of the fixed and random effects.  Dots show the estimated population boundary in each condition, based on the fixed effects alone."}

perfect_learning <- exposure_stats %>%
  group_by(vot_cond) %>%
  do(stats_to_lhood(.)) %>%
  lhood_to_classification()

no_learning <- prior_lhood %>%
  lhood_to_classification()

vot_limits <- data_exp1 %>% pull(vot) %>% range()

ggplot(expt1_class_bysub, aes(x=vot, color=vot_cond)) +
  geom_line(data=.%>%filter(block==3), aes(y=Estimate, group=subject), alpha=0.2) +
  facet_grid(.~vot_cond) +
  geom_line(data=perfect_learning, aes(y=prob_p), group=1, linetype="33", size=1,
            show.legend=FALSE) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype="99", color='black',
            show.legend=FALSE) +
  geom_point(data=expt1_bounds %>% filter(block==3),
             y=0.5) +
  labs(x="VOT (ms)",
       y="Probability /p/ response",
       title="Experiment 1: Classification functions",
       subtitle="Estimated with Bayesian mixed effects logistic regression, predictions for final third of trials") +
  lims(x = c(-10, 70))

```

```{r expt1-boundaries, fig.cap="Category boundaries by condition.  Violins show the distribution of participants' boundaries (random effects), and the white points show the group-level boundary (fixed effects, with 95% Bayesian credible intervals).  The black dashed line shows the ideal category boundary given a typical talker's distributions, and the colored heavy dashed lines show the ideal boundaries based on each condition's exposure distributions (see @fig:vot-dists-exp1)."}

no_learning_bound <-
  no_learning %>%
  arrange((prob_p - 0.5)^2) %>%
  head(1) %>%
  pull(vot)

perfect_learning_bound <-
  perfect_learning %>%
  group_by(vot_cond) %>%
  arrange((prob_p - 0.5)^2) %>%
  filter(row_number() == 1)

# estimate uncertainty of fixed effects boundaries:
expt1_bounds_fixef <-
  expt1_bounds_fixef_samples %>%
  group_by(vot_cond, block, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))

expt1_bounds_bysub %>%
  filter(block==3) %>%
  ggplot(aes(x=vot_cond, y=vot, fill=vot_cond, color=vot_cond)) +
  geom_violin(alpha=0.5, color=NA) +
  ## geom_beeswarm(cex=2) +
  geom_pointrange(data = expt1_bounds_fixef %>% filter(block==3),
                  aes(y=mean, ymin=low, ymax=high),
                  color="white", show.legend=FALSE) +
  geom_hline(yintercept=no_learning_bound, color="black", linetype=2) +
  geom_segment(aes(x=as.numeric(vot_cond)-.5, # exposure boundaries_exp1
                   xend=as.numeric(vot_cond)+.5,
                   y=vot,
                   yend=vot,
                   color=vot_cond),
               linetype = 2, size = 1,
               data = perfect_learning_bound) +
  geom_text(data=perfect_learning_bound %>% filter(vot_cond=="20, 60"),
            aes(y=vot, color=vot_cond),
            ## x=3.5, y=41, 
            label="Complete learning\n(Exposure talker)",
            hjust = 0, vjust = 0,
            nudge_x = -0.5, nudge_y = 1) +
  geom_text(data=perfect_learning_bound %>% filter(vot_cond=="20, 60"),
            aes(color=vot_cond, x = as.numeric(vot_cond)-0.5),
            y=no_learning_bound - 1, color='black',
            ## x=3.5, y=41, 
            label="No learning\n(Typical talker)",
            hjust = 1, vjust = 0,
            ) +
  labs(title="Experiment 1: Category boundaries",
       subtitle="Compared with no learning and complete learning",
       y = "Boundary VOT (ms)",
       x = "Exposure condition (/b/, /p/ mean VOT)") +
  theme(legend.position="none") +
  coord_flip()

```

```{r bound-diff-p-vals}

# compute Bayesian p-values for differences between boundaries between
# neighboring VOT conditions.

expt1_bounds_fixef_diffs <-
  expt1_bounds_fixef_samples %>%
  group_by(.draw, block) %>%
  arrange(vot_cond) %>%
  mutate(vot_cond_next = lead(vot_cond),
         bound_diff = vot - lead(vot))

expt1_bounds_fixef_diff_stats <-
  expt1_bounds_fixef_diffs %>%
  group_by(block, vot_cond, vot_cond_next) %>%
  filter(!is.na(vot_cond_next)) %>%
  summarise_at(vars(bound_diff),
               funs(mean,
                    low=quantile(., 0.025),
                    high=quantile(., 0.975),
                    p=mean(.>0)))

ex1_pval_str <-
  expt1_bounds_fixef_diff_stats %>%
  filter(block == 3) %>%
  pull(p) %>%
  max() %>%
  daver::p_val_to_less_than()

```

```{r bound-undershoot}

# summaries of boundary undershoot
expt1_boundary_undershoot <-
  perfect_learning_bound %>%
  select(vot_cond, ideal_bound=vot) %>%
  right_join(expt1_bounds_fixef, by="vot_cond") %>%
  filter(block == 3) %>%
  mutate_at(vars(low, high, mean),
            funs(abs=(.-ideal_bound)*sign(no_learning_bound-ideal_bound),
                 perc=round(100 *
                              (.-no_learning_bound) /
                              (ideal_bound-no_learning_bound)))) %>%
  select(-ideal_bound, block, trial) %>%
  mutate(abs_ci = glue("{round(mean_abs,0)}ms (95% CI $[{round(low_abs,0)}, {round(high_abs,0)}]$)"))
  

```

```{r exp1-respP-logodds}
exp1_p_logodds_summary <-
  b_logit_exp1 %>%
  gather_draws(b_bvotCond0, b_bvotCond10, b_bvotCond20, b_bvotCond30) %>%
  group_by(.draw) %>%
  arrange(.draw, .variable) %>%
  mutate(.value_d = lag(.value, default=0) - .value) %>%
  group_by(.variable) %>%
  summarise(p = mean(.value_d > 0),
            mean = mean(.value_d),
            low = quantile(.value_d, 0.025),
            high = quantile(.value_d, 0.975))

min_p_increase <- format(min(exp1_p_logodds_summary$p), digits=3)

```


The first question is whether listeners learned anything at all from exposure to
these VOT distributions.  Figure -@fig:exp1-results shows that, qualitatively,
listeners generated different categorization functions after exposure to
different distributions.  The results of the Bayesian mixed-effects regression
verify this qualitative impression, with the overall probability of responding /p/
increasing from each condition to the next with $p > `r min_p_increase`$.[^bayes_p]

A more informative way of answering this question is to look at the _category
boundary_---the VOT that is ambiguous between /b/ and /p/---corresponding to the
fixed effects for each exposure condition.  Because trial number was included as
a regressor, trial was fixed at 184 (83% or 5/6 of the 222 total trials).  The
large white points in Figure [-@fig:expt1-boundaries] show the estimated
boundary for each condition, and the corresponding confidence intervals are the
95% posterior intervals (e.g., the 95% quantiles of the posterior samples).
Visual inspection shows that the group-level posterior distributions of the
boundaries are almost entirely non-overlapping with their
most-similar/neighboring conditions (all $`r ex1_pval_str`$), even though the
distributions of individual subjects' boundaries _do_ overlap (as shown by the
colored violins).

[^bayes_p]: The $p$ values reported here are Bayesian $p$ values, or the
    posterior probability that the relevant statement holds conditional on the
    data and model.  These are determined using _samples_ from the posterior,
    by computing the proportion of those samples where the statement is true.
    In this case, the relevant condition is whether the category boundary for
    one condition is reliably less than the boundary for the condition with a
    next highest mean VOTs.

The second question is how _completely_ listeners learned the distributions they
were exposed to.  Visual inspections of the categorization functions
[@fig:exp1-results] and the category boundaries [@fig:expt1-boundaries] suggests
that listeners category boundaries (at 83% through the exposure) do _not_
perfectly correspond to the ideal category boundary for the VOT distributions
they were exposed to (thick dashed colored lines).  Specifically, listeners seem
to _undershoot_ the ideal category boundary, and their categorizations functions
lie between the ideal boundary for the exposure talker and the boundary
corresponding to a typical talker of American English (thin black lines).

The undershoot is largest in absolute terms for the most shifted distributions,
with the boundary for the 30, 70ms VOT condition undershooting by 
`r expt1_boundary_undershoot %>% filter(vot_cond=="30, 70") %>% pull(abs_ci)` and the -10,
30ms condition by 
`r expt1_boundary_undershoot %>% filter(vot_cond=="-10, 30") %>% pull(abs_ci)`.

## Discussion

These results indicate two things.  First, like many other studies
[e.g. @Chladkova2017; @Clayards2008; @Theodore2015; @Theodore2019; @Colby2018],
I found evidence for distributional learning by adults: listeners _do_ change how they
categorize sounds as voiced or unvoiced due to exposure to different VOT
distributions.  Second, this learning is not _complete_, in the sense that
listeners' voiced-unvoiced boundary does not exactly match the boundary implied
by the bimodal distribution they were exposed to.  Like the findings of
@Sumner2011 and @Idemaru2011, this second result suggests that distributional
learning is _constrained_ somehow.  In the following experiments, I investigate
two possible sources of these constraints.

```{r prop-in-wrong-cat}

data_exp1 %>%
  group_by(vot_cond, trueCat) %>%
  summarise(wrong=mean(ifelse(trueCat == 'b',
                              vot > no_learning_bound,
                              vot < no_learning_bound)))

```


First, it is possible that the primary constraint here is that listeners are
being asked to do _unsupervised_ distributional learning.  That is, they are
free to interpret each VOT they hear as either a /b/ or a /p/, since both
options are available for a response.  For distributions that are highly
shifted, most tokens will fall into one of the two pre-existing distributions
(see @Fig:vot-dists-exp1): in the condition with the highest positive shift
(30ms /b/, 70ms /p/), 73% of all tokens from the lower cluster would be
classified as /p/ according to a typical talker's distributions.  Thus, what
was intended to be two separate clusters of VOTs---one for /b/ and
one for /p/---may instead have been interpreted by listeners as one large
cluster in the extreme
conditions, lacking any labels to tell them otherwise.  I address this
possibility in Experiment 2, by providing labels on a portion of trials.

A second possibility is that listeners are constrained by their prior experience
with other talkers.  The fact that listeners' category boundaries tend to lie
between those of a typical talker and the experimental exposure talker suggests
that this is not implausible.  To assess this possibility more rigorously, in
Experiment 3 I use a Bayesian belief-updating model of distributional learning
to investigate whether the pattern of distributional learning across conditions
is consistent with belief updating starting from common prior beliefs.

# Experiment 2: Semi-supervised distributional learning

In this experiment, some trials are presented with _label_ information, which
indicates _which_ member of the /b/-/p/ minimal pair the speaker intended to
produce.  The goal of this experiment is to investigate whether the constraints
on distributional learning observed in Experiment 1 are due to the unsupervised
nature of the distributional learning task.  Adding labels to some of the trials
might improve the degree of learning, the speed of learning, or both.

## Methods

```{r exp2-data, dependson=c()}

data_exp2 <-
  supunsup::supunsup_clean %>%
  filter(supCond %in% c("supervised", "mixed")) %>%
  mutate(trueCat = respCategory,
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

conditions_exp2 <-
  data_exp2 %>%
  group_by(bvotCond, supCond, trueCat) %>%
  summarise(mean_vot = mean(vot)) %>%
  spread(trueCat, mean_vot) %>%
  transmute(vot_cond = paste(b, p, sep=', '),
            ideal_boundary = (b+p)/2) %>%
  ungroup() %>%
  mutate(vot_cond = factor(vot_cond, levels=levels(conditions_exp1$vot_cond)),
         supervised = fct_recode(supCond, supervised="mixed"))

n_subj_exp2 <- length(unique(data_exp2$subject))

excluded_exp2 <- supunsup::supunsup_excluded %>%
  inner_join(conditions_exp2) %>%
  group_by(subject, supCond, bvotCond, vot_cond) %>% 
  summarise() %>% 
  left_join(supunsup::excludes, by="subject") %>%
  select(subject, bvotCond, exclude80PercentAcc, rank)

n_excluded_2 <- nrow(excluded_exp2)
n_subj_repeat_2 <- sum(!is.na(excluded_exp2$rank))
n_subj_bad_2 <- sum(!is.na(excluded_exp2$exclude80PercentAcc))
n_both_2 <- n_subj_repeat_2 + n_subj_bad_2 - n_excluded_2 

n_total_2 <- n_subj_exp2 + n_excluded_2

n_excl_cond_2 <- excluded_exp2 %>%
  group_by(bvotCond) %>%
  tally() %>%
  arrange(n)

```

### Procedure

```{r exp2-label-dists, fig.width=9, fig.height=4.5, fig.cap="Distribution of labeled and unlabeled trials in Experiment 2.  These conditions have the same number of labeled and unlabeled trials, but differ in how they are assigned to VOTs.  In the \"mixed\" condition, most VOTs occurred in a mixture of labeled and un-labeled trials.  In the \"separated\" condition, each VOT always occurred as either labeled or unlabeled.  Because no difference was found between learning in these conditions, they are analyzed together."}

data_exp2 %>%
  mutate(supCond = fct_recode(supCond, separated="supervised")) %>%
  group_by(supCond) %>%
  filter(bvotCond == 0) %>%
  filter(subject == first(subject)) %>%
  group_by(supCond, labeled, vot) %>%
  tally() %>% 
  ggplot(aes(x=vot, y=n, fill=labeled)) +
  geom_bar(stat="identity") +
  facet_grid(.~supCond) +
  scale_fill_manual(values=c("black", "gray")) +
  scale_x_continuous(breaks = c(0, 40), labels = c("/b/ mean", "/p/ mean")) +
  labs(x = "",
       y = "Count",
       title = "Experiment 2: semi-supervised learning",
       subtitle = "Distribution of labeled trials in two semi-supervised learning conditions")

```

The procedure was identical to that of Experiment 1, with one change.  Half of
the trials were _unlabeled_ as in Experiment 1: pictures for both the 
/b/ and /p/ members of the minimal pair for that trial's word were available (e.g.,
"beach" and "peach").  The other half of the trials were _labeled_: if the word was
drawn from a "beach" to "peach" continuum and the VOT was sampled from the lower
VOT cluster, than one response option would be "beach", and the other either "peak" or
"peas".  That is, the only response that was compatible with the _rest_ of the
word had an onset consistent with the VOT cluster that that trial was drawn
from.

Two different semi-supervised conditions were run which differed in how labeled
and unlabeled trials were distributed across VOTs (@Fig:exp2-label-dists).  No
differences were found between behavior in these conditions so they are analyzed
together.[^supconds]

[^supconds]: For the curious---or skeptical---reader, the two conditions are
    listed separately in the
    [`supunsup`](https://github.com/kleinschmidt/phonetic-sup-unsup) R data
    package that accompanies this paper.

<!-- TODO: this should go in a footnote in the introduction -->
The only other difference with Experiment 1 was that the -10ms, 30ms condition
was not included.  This is because these two experiments were originally run
concurrently as a pilot for an imaging study, during which the -10ms, 30ms
condition was not considered.  The original goal of the supervised conditions of
Experiment 2 was to see if distributional learning could be accelerated by
including some labeled trials.  The -10ms, 30ms condition of Experiment 1 was
run later, after analyzing data from the other conditions Experiments 1 and 2
together.

### Participants

A total of `r n_total_2 ` participants were recruited via Amazon's Mechanical Turk.  As in
Experiment 1, participants were paid $2.00 for participation.  Exclusion criteria
were the same as Experiment 1, and data from `r n_excluded_2` participants were
excluded from analysis: `r n_subj_repeat_2` for repeated participation
(including participants who had participated in Experiment 1), `r n_subj_bad_2` for
inaccurate categorization of unambiguous VOTs, with `r n_both_2` excluded for
both criteria.  This left data from `r n_subj_exp2` remaining for
analysis.[^excluded-data]  The larger sample size reflects the fact that there
were originally two different variants of the supervised learning task, which
yielded similar results and are thus analyzed together here.

[^excluded-data]: The full datasets for all experiments reported here, including
    subjects excluded from analysis, is available in the
    [`supunsup`](https://github.com/kleinschmidt/phonetic-sup-unsup) R package.

## Results

```{r exp2-regression-data, dependson=c("exp2-data")}

data_exp2_mod <- supunsup::supunsup_clean %>%
  filter(labeled == "unlabeled", bvotCond != "-10") %>%
  select(subject, supCond, labeled, bvotCond, trial, vot, respP) %>%
  mutate(vot_s = (vot - mean(vot)) / sd(vot),
         trial_s = (trial - mean(trial)) / sd(trial),
         supervised = fct_recode(supCond, supervised="mixed"))

```

```{r exp2-regression, dependson=c("exp2-regression-data")}

f2_int <- respP ~ bvotCond * supervised * vot_s * trial_s +
  (1 + vot_s | subject)

beta_prior <- set_prior("student_t(3, 0, 1)", class="b")

## b_logit_sup_v_unsup_w_prior <- brm(f2_int,
##                                    data = d2,
##                                    family = bernoulli(),
##                                    chains = 4,
##                                    prior = beta_prior,
##                                    iter = 1000,
##                                    sample_prior = "yes")

## saveRDS(b_logit_sup_v_unsup_w_prior, "models/b_logit_sup_v_unsup_w_prior.rds")

b_logit_exp2 <- readRDS("models/b_logit_sup_v_unsup_w_prior.rds")

exp2_blocks <-
  data_exp2_mod %>%
  group_by(block=ntile(trial, 3)) %>%
  summarise_at(vars(trial, trial_s), mean)

# overall fit (fixed effects):
data_pred_exp2 <-
  cross_df(list(vot_s = seq(min(data_exp2_mod$vot_s),
                            max(data_exp2_mod$vot_s),
                            length.out=100),
                bvotCond = unique(data_exp2_mod$bvotCond),
                supervised = unique(data_exp2_mod$supervised),
                block = 1:3)) %>%
  left_join(exp2_blocks, by="block") %>%
  left_join(conditions_exp1, by="bvotCond") %>%
  mutate(vot = vot_s * sd(data_exp2_mod$vot) + mean(data_exp2_mod$vot),
         bvotCond = fct_drop(bvotCond),
         vot_cond = fct_drop(vot_cond))


data_pred_subj_exp2 <-
  data_exp2_mod %>%
  group_by(subject, supervised, bvotCond) %>%
  summarise() %>%
  left_join(data_pred_exp2)

expt2_class_bysub <-
  fitted(b_logit_exp2, newdata=data_pred_subj_exp2) %>%
  as_tibble() %>%
  bind_cols(data_pred_subj_exp2)

expt2_bounds_bysub <-
  expt2_class_bysub %>%
  group_by(vot_cond, supervised, subject, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt2_bounds_fixef_samples <-
  tidybayes::linpred_draws(b_logit_exp2, data_pred_exp2, re_formula=NA) %>%
  group_by(vot_cond, supervised, block, trial, .draw) %>%
  find_bound(x=vot, y=.value)

expt2_bounds_fixef <-
  expt2_bounds_fixef_samples %>%
  group_by(vot_cond, supervised, block, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))

```

```{r expt2-label-consistent}
label_consistent <- data_exp2 %>%
  filter(labeled == "labeled") %>%
  summarise(p = round(100*mean(respCat == trueCat))) %>%
  pull(p)

label_range <- data_exp2 %>%
  filter(labeled == "labeled") %>%
  group_by(subject) %>%
  summarise(p = round(100*mean(respCat == trueCat))) %>%
  pull(p) %>%
  range()

```


```{r expt2-bounds-sup-diff}

expt2_bounds_fixef_samples %>%
  group_by(vot_cond, block, trial) %>%
  spread(supervised, vot) %>%
  mutate(vot_diff = unsupervised - supervised) %>%
  summarise(mean = mean(vot_diff),
            low = quantile(vot_diff, 0.025),
            high = quantile(vot_diff, 0.975))


```

```{r expt2-class-funs, fig.cap="Classification functions from semi-supervised distributional learning.  As for Experiment 1 (Figure -@fig:exp1-results), individual subjects categorization functions (fine colored lines) are intermediate between the ideal boundary given a typical talker's distributions (skinny, black dashed lines), and the ideal boundary for the exposure distributions (heavy colored dashed lines).", fig.width=8.5, fig.height=3}

ggplot(expt2_class_bysub, aes(x=vot, color=vot_cond)) +
  geom_line(data=.%>%filter(block==3), aes(y=Estimate, group=subject), alpha=0.2) +
  facet_grid(.~vot_cond) +
  geom_line(data=filter(perfect_learning, vot_cond != '-10, 30'),
            aes(y=prob_p), group=1, linetype="33", size=1,
            show.legend=FALSE) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype="99", color='black',
            show.legend=FALSE) +
  geom_point(data=expt2_bounds_fixef %>% filter(block==3, supervised=='supervised'),
             aes(x=mean),
             y=0.5) +
  labs(x="VOT (ms)",
       y="Probability /p/ response",
       title="Experiment 2: Classification functions (semi-supervised learning)",
       subtitle="Fitted with mixed effects logistic regression, predictions for final third of trials") +
  lims(x = c(-10, 70))

```

```{r expt2-bounds, fig.cap="Providing 50% labeled trials during distributional learning makes no difference in listener's category boundaries (Experiment 2, solid voilins/points) compared to purely unsupervised learning (Experiment 1, outlined violins/points, re-plotted from @fig:expt1-boundaries).  For ease of comparison with results from Experiment 1, the same axis limits are used as @fig:expt1-boundaries; this lead to exclusion from the figure of five listeners with boundaries greater than 50ms (2 in the 20ms /b/ condition, 3 in the 30ms /b/ condition).  All participants are included in the regression model from which the fixed effects boundaries are estimated (points/lines)."}

expt2_bounds_bysub %>%
  filter(block==3) %>%
  ggplot(aes(x=vot_cond, y=vot, fill=vot_cond, color=vot_cond)) +
  geom_violin(data= . %>% filter(supervised=="unsupervised"), alpha=0)+
  geom_violin(data= . %>% filter(supervised=="supervised"), alpha=0.5, color=NA)+
  geom_pointrange(data = expt2_bounds_fixef %>% filter(block==3),
                  aes(y=mean, ymin=low, ymax=high, shape=supervised),
                  position=position_dodge2(width = .2),
                  color="white", show.legend=FALSE) +
  theme(legend.position="none") +
  scale_shape_manual(values = c(16, 1)) +
  lims(y=c(10,50)) +
  labs(title="Experiment 2: Semi-supervised adaptation",
       subtitle="Compared with unsupervised (Experiment 1)",
       y = "Boundary VOT (ms)",
       x = "Exposure condition (/b/, /p/ mean VOT)") +
  coord_flip()

```

```{r exp2-sup-effects}

hyps_sup <- b_logit_exp2 %>%
  fixef() %>%
  rownames() %>%
  purrr::keep(~ str_detect(., "supervised")) %>%
  glue("{x} = 0", x=.)

hyps_sup_test <- hypothesis(b_logit_exp2, hypothesis=hyps_sup)

n_weak <- sum(hyps_sup_test$hypothesis$Evid.Ratio < 3.2)
n_strong <- sum(hyps_sup_test$hypothesis$Evid.Ratio >= 3.2)
n_total <- nrow(hyps_sup_test$hypothesis)

hyps_rest <- b_logit_exp2 %>%
  fixef() %>%
  rownames() %>%
  purrr::keep(~ !str_detect(., "supervised")) %>%
  glue("{x} = 0", x=.)

hypothesis(b_logit_exp2, hypothesis=hyps_rest)

```

First, listeners were highly sensitive to the labels on labeled trials,
responding consistent with the label `r label_consistent`% of the time (range
across participants of `r label_range[1]`--`r label_range[2]`%).

Second, did listeners use labels to guide or accelerate their _learning_?  As
with Experiment 1 I assessed listeners' distributional learning via category
boundaries estimated with a Bayesian logistic mixed-effects model.  This model
included data from _both_ Experiments 1 and 2 in order to directly assess
effects of supervised learning on the strength and time course of learning.
This model included fixed effects for supervision condition, all fixed effects
from the Experiment 1 model, and all interactions thereof.  The random effects
structure was identical to Experiment 1.[^expt2-formula]  Because listeners' responses were so
systematically determined by the label on labeled trials, _only_ unlabeled
trials were included in the model (all trials from Experiment 1, and half of the
trials from Experiment 2).

[^expt2-formula]: The model formula was `respP ~ 1 + bvotCond * supervised *
  vot_s * trial_s + (1 + vot_s | subject)`.  As in Experiment 1, VOT and trial
  number were centered and scaled to have zero mean and unit variance.  Default
  treatment contrasts were used for `supervised` and `bvotCond`, with the base
  levels being `supervised` and /b/ mean VOT of 0ms, respectively.  Default
  priors were used except for the addition of a Student's $t$ prior for the
  regression coefficients ($\beta$s) with 3 degrees of freedom, mean 0, and
  scale of 1 (`student_t(3, 0, 1)` in the `brms` syntax); for discussion of this
  see the supplementary material.

Figure -@fig:expt2-bounds shows the by-subject and overall category boundaries
for semi-supervised (shaded violins/points) and unsupervised (outlined
violins/points, re-plotted from Figure -@fig:expt1-boundaries) distributional
learning.  As in @Fig:expt1-boundaries, the
boundaries estimated from the model for a trial 5/6 of the way to the end of the
experiment, and the points and lines show the group-level (fixed effects)
estimates and 95% credible intervals.  The intervals for unsupervised and
semi-supervised overlap for all VOT conditions, indicating that the inclusion of
labeled trials makes no reliable difference in distributional learning, at least
at the trial where these boundaries are estimated.

Similarly, by a variety of measures, the estimates of the regression
coefficients themselves suggest that the addition of labeling information did
not change listeners distributional learning.  First, the 95% credible intervals
for all fixed effects coefficients involving the supervision condition
include 0.  Second, the Bayes Factors (BFs) for the null hypotheses that each of
the regression coefficients involving the supervision condition is exactly zero
suggest that the data provide either weak
($1 < BF < 3.2$, `r n_weak` out of `r n_total` predictors) 
or "substantial" 
($BF \geq 3.2$, `r n_strong` of `r n_total`) 
evidence in favor of the null [using the somewhat arbitrary cutoff of
@Kass1995].  By both standards, the addition of labeling information has little
to no effect on the overall amount of learning in each condition individually,
does not affect category boundary steepness (interactions with VOT), and does
not change the speed of learning speed (interactions with trial number).

Based on the category boundaries themselves, the condition that comes the
closest to showing any effect is the 30ms /b/, 70ms
/p/ VOT, which is the most extreme shift relative to standard VOT distributions.
However, the boundaries in this condition are highly variable across subjects,
and moreover the group-level boundaries in this condition may have been biased by
the presence of three outlier participants with unreliable responses which leads to
very shallow classification functions and very high estimated boundaries
(greater than 50ms; see Figure -@fig:expt2-class-funs).  These participants
weren't so unreliable as to be excluded by
the pre-set exclusion criteria so they are included in the analysis here for the
sake of completeness.  Even so, there is still no significant (Bayesian
$p<0.05$) effect of supervision in any regression coefficient _or_ the estimated
boundary, and the single-subject boundary distribution for this condition
(violins in @Fig:expt2-bounds) are very similar.

## Discussion

The results of Experiment 2 suggest that labeling individual VOTs as /b/ or /p/
does very little to affect listeners' distributional learning.  Both at the
condition level (fixed effect boundaries) and the subject level (distribution of
participants' boundaries), distributional learning leads to the same shift in
category boundaries regardless of whether all trials are unlabled (Experiment 1)
or half labeled (Experiment 2).  This in turn suggests that the discrepancy
between listeners' learned boundaries and the boundary implied by the exposure
distribution is _not_ due to uncertainty about the whether any particular VOT
was intended to be a /b/ or a /p/.  This is somewhat surprising, given that in
the extremely shifted 30ms /b/, 70ms /p/ condition most of the tokens from the
/b/ distribution would normally be classified as /p/ under a typical talker's
generative model.

One possible interpretation of this result is that the way that label
information was provided made the labels somehow unavailable to update
listeners' beliefs about how phonetic categories are acoustically realized.
This is not particularly plausible, for two reasons.  The first is that a wide
variety of labeling information has been shown to be effective in phonetic
recalibration, including lexical [@Kraljic2005; @Norris2003],
visual/articulatory [@Bertelson2003], and orthographic [@Keetels2016a] labels.
The second is that listeners were highly sensitive to the labeling information
in generating their responses _on labeled trials_, it was only on _unlabeled_
trials where the presence of labels on other trials seems to make no difference.

Another possibility is that even without labels, listeners are able to pick up
on the distributions they are exposed to, and so labeling some of the trials as
a clue to which distribution they come from doesn't provide any additional
information.  If this is the case, how can we explain the discrepancy between the
boundaries implied by those distributions and the boundaries listeners actually
learned?  Experiment 3 addresses this question via computational modeling.

# Experiment 3---Computational modeling

Experiments 1 and 2 together present a puzzle: on the one hand, distributional
learning appears to be "incomplete", with listeners using category boundaries
that are reliably different from the boundaries implied by the VOT distributions
they were exposed to.  On the other hand, providing listeners extra
information---in the form of category labels for half the VOTs they heard---did
not have any effect on distributional learning.  This suggests that listeners
_know_ the distributions that the exposure talker produces, but they just don't
_believe_ them, and hedge their bets by erring in the direction of a typical
talker's boundary.  Circumstantial evidence to this effect comes from the fact that
the category boundaries that listeners _do_ use appear to be a compromise
between the category boundaries derived from the VOT distributions for the
exposure talker and those produced by a typical talker of American English.

In this experiment I use a computational model to explore this possibility in
more detail.  Specifically, I use a model inspired by the
ideal adapter framework [@Kleinschmidt2015b], which is based on the idea that
listeners maintain uncertain beliefs about the distribution of cues
corresponding to each phonetic category, and update this beliefs incrementally
based on their recent experience.  Crucially, according to the ideal
adapter, when listeners encounter an unfamiliar talker, they initially entertain
a set of prior beliefs about the range of different phonetic cue distributions that an unfamiliar
talker is likely to produce, which takes into account the variability across
talkers that they have previously experience, and any available socio-indexical
information that may be informative about this talker's cue distributions
[@Kleinschmidt2015b; @Kleinschmidt2019].  After direct experience with a talker, a listener will
_update_ their beliefs about that talker's cue distributions, bringing them into
better alignment with the cue distributions that the talker has actually
produced.

Why is this type of model a plausible explanation of the results of Experiments
1 and 2?  First, this same type of model captures the changes in phonetic
classification that happen during recalibration and selective adaptation
[@Kleinschmidt2015b] as well as during distributional learning due to different
VOT distribution variances [@Theodore2019].

Second, the process of belief updating means that at any given point a
listener's beliefs about that talker's cue distributions will be a _compromise_
between what they expected a priori, and the distributions they have actually
encountered.  When a listener has limited experience with a particular
talker, or especially strong, specific prior expectations, this compromise will
be especially obvious because their prior beliefs will be more informative
relative to their recent, direct experience with the talker.

Finally, one prediction of the ideal adapter is that listeners' prior
expectations should be calibrated to the level (and type) of talker variability
they have actually encountered [@Kleinschmidt2015b].  In this framework, the
prior beliefs that listeners start from are based on listeners' expectations, on
the one hand, about what a _typical talker_ will produce, and on the other hand,
how _consistent_ talkers are in their realization of the given phonetic cue
distributions.  In the case of VOT distributions for word-initial stop voicing,
there is relatively little variability across talkers in VOT distributions
[@Kleinschmidt2019], the distributional learning experiments here are exactly
the kind of situations where listeners might be
expected to bring strong prior expectations and hence show a clear compromise
between those prior expectations and the distributions that they encounter in
the experiments.



## Methods

I use a Bayesian belief updating model, where listeners begin the experiment
with a shared set of prior beliefs about the mean and variance of the VOT
distributions for /b/ and /p/, and update these beliefs according to Bayes rule
based on the VOTs they hear in the experiment.[^model-theory]

[^model-theory]: Note that, as in @Kleinschmidt2015b, this _model_ is an
    instantiation of only a _component_ of the ideal adapter _theory_---the
    process of evidence accumulation within a single context---and does not
    stand in for the entire theory.

Specifically, a listener's uncertain beliefs about the mean and variance of each
category (/b/ and /p/) are represented as Normal-Inverse Chi squared
distribution.  At the beginning of the experiment, every listener has the same
prior beliefs, expressed in the model as parameters for the _expected_ mean and
variance ($\mu_{0}$ and $\sigma^2_0$), and for their _confidence_ in these beliefs
($\kappa_0$ for the mean and $\nu_0$ for the variance).  These confidence
parameters are "pseudocounts", in that they are equivalent to assuming that the
listener's expectations about the category mean is based on $\kappa_0$ prior
observations from that category, and likewise for the variance and $\nu_0$.
This prior distribution is a _conjugate prior_ because after observing $n$ data
points with sample mean $\bar x$ and variance $s^2$, the posterior distribution
is also a Normal-Inverse Chi squared, with updated values of the four parameters:

\begin{align}
\kappa_n &= \kappa_0 + n \\
\nu_n &= \nu_0 + n \\
\mu_n &= \frac{\kappa_0\mu_0 + n\bar x}{\kappa_0 + n} \\
\sigma^2_n &= \frac{1}{\nu_n}\left(\nu_0 \sigma^2_0 + ns^2 +
  \frac{n\kappa_0}{\kappa_0 + n}(\mu_0 - \bar x)^2\right)
\end{align}

These parameter updates have an intuitive interpretation [@Kleinschmidt2015b]:
the confidence parameters are increased by the number of observations $n$ (which
is why they are called "pseudocounts"), while the new expected mean and variance
are weighted averages of the prior expectations and the observed values, with
the weights determined by the pseudocounts and actual count,
respectively.[^var-mean]

[^var-mean]: For the variance update, there is an additional term that captures
    the possibility that a mismatch between the expected and observed mean could
    be due to a higher than expected variance.

Given updated beliefs about the mean and variance of each category the
probability of any particular VOT $x$ being /b/ vs. /p/ can be calculated by the
marginal likelihood of $x$ under the /b/ and /p/ distributions [see
@Kleinschmidt2015b for more details].  The marginal likelihood is essentially a
weighted average of the likelihood under all possible means and variance for
each category, weighted by how probable each mean/variance is based on the VOTs
observed from each category so far and the prior beliefs.  These marginal
likelihoods are then converted to a probability of responding /p/ or /b/ using
Bayes' rule.

### Assumptions

In actually implementing this model, I make a number of simplifying assumptions
for the sake of tractability.  First, and most importantly, I provide the model
with the observed mean and standard deviations of the two exposure VOT
distributions ($\bar x$ and $s$ above).  This
substantially simplifies the complexity of the learning problem: in order to
model _unsupervised_ learning, a Bayesian model needs to average predicted
behavior over distributional learning given each possible way that all the
tokens could be jointly classified, weighted by the probability of that
classification.  In an experiment with 200 trials, each of which could be either
/b/ or /p/, this amounts to more than $10^{60}$ possibilities, an impossibly
large number to consider.  There are approximations for unsupervised or
semi-supervised belief updating, but they substantially increase the
computational complexity and exploring them is left for future work.

Second, the use of a conjugate Normal Inverse Chi-squared prior is also an
important simplification, as it means that the belief updating can be computed
analytically.  Combined with the first assumption, this means that the updated
beliefs can be computed directly from the prior parameters and the observed
count, mean, and variance of each category, rather than requiring an additional
level of MCMC sampling to approximate.

### Model fitting procedure

The free parameters of this model were the prior expected mean and variance of
/b/ and /p/, and the mean and variance prior pseudocounts (which were assumed to
be the same for /b/ and /p/ because pilot simulations suggested it was not
possible to reliably identify both).  I also included a guessing rate parameter,
which captured the fact that many listeners never reach floor or ceiling even
for unambiguous high/low VOTs, and thus may be guessing on a proportion of
trials.

The model was fit to data from Experiment 1 (unsupervised distributional
learning) with the [`beliefupdatr`
package](https://github.com/kleinschmidt/beliefupdatr), which includes a Stan
[@Carpenter2017] implementation of this belief updating model and a convenient
`R` [@R-base] interface.
The trials were divided up into six blocks of equal length
(37 trials).  Responses in each block in each distribution condition were
predicted based on updated beliefs at the halfway point of the block, using the
sufficient statistics of that distribution condition.

No "random effects" (hierarchical effects by subject) were included.  This is a
difficult choice to make.  On the one hand, there likely _are_ differences in
listeners' prior expectations and learning rates, and ignoring these ignores
important variability in the data.  On the other hand, including such random
effects by subjects introduces many additional degrees of freedom into the
model, and increases the number of data patterns it could fit.  This is
especially dangerous in this particular case since each listener only
encountered a single set of distributions, and so random effects for _subjects_
effectively introduces unconstrained flexibility in how the model captures
differences between _distributions_.  So, I opted not to include random effects
in the belief updating model, so as to make as rigorous as possible a test of
its ability to capture differences in distributional learning found above.
However, the data and analysis scripts are available online for the interested
reader to explore the consequences of alternative choices.
<!-- 
TODO: link?
-->

### Model evaluation

The goodness-of-fit is evaluated using the PSIS-LOO method of @Vehtari2017.
This method approximates leave-one-out cross-validation to provide an estimate
of the out-of-sample prediction error from the posterior samples of the model.
This provides a goodness-of-fit measurement that penalizes overly complex models
in a way that is directly (approximately) related to their inability to
generalize beyond the data they were fit to, and as such is a more robust
measure for comparing between different models than indirect metrics like AIC or
BIC which rely on counting numbers of parameters or WAIC/DIC which are hard to
estimate reliably from posterior samples [see @Vehtari2017 for more discussion].
Because data was nested within subject, I used subjects as the cross-validation
units (instead of individual trial responses).  I report the expected
log-posterior density in the form of LOOIC, which is on the deviance scale ($-2$
times the log-likelihood) like AIC.  On this scale, lower is better,
corresponding to higher predictive log likelihood (better-fitting or more
parsimonious models).  Finally, because these are estimated from a finite
dataset, there is uncertainty associated with them (and the differences between
models), which is indicated by the standard error of the estimate (or
difference).

The belief updating model was compared with two other models via LOOIC [both fit
via `brms`, @Burkner2017].  The first was a "null model" with a constant
probability of responding /p/.  The second was a baseline model similar to the
logistic regression model used to analyze the data above, with two differences
to make it a suitable baseline.  First, because the belief-updating model fit
all subjects with a single set of prior beliefs, the baseline model included no
random effect, while maintaining the fixed effects structure (condition, trial,
VOT, and their interactions).  Second, because the belief-updating model
included a "guessing rate" parameter that varied over the experiment, the
baseline model also included a variable guessing rate (via a `brms` `mixture`
family with varying weight).  As in the belief updating model, the guessing rate
was allowed to vary freely over the six blocks of 37 trials.

```{r infer-prior}

## options(mc.cores = parallel::detectCores())
## fit_inc <- infer_prior_beliefs(data_exp1,
##                                cue = "vot",
##                                category = "trueCat",
##                                response = "respCat",
##                                condition = "vot_cond",
##                                ranefs = "subject",
##                                n_blocks = 6,
##                                chains = 4,
##                                iter = 2000)
## saveRDS(fit_inc, "models/fit_inc.rds")

fit_inc <- readRDS("models/fit_inc.rds")

categories <-
  data_frame(cat_num = 1:2,
             category = c('b', 'p'))

prior_samples_df <-
  fit_inc %>%
  spread_draws(nu_0, kappa_0, mu_0[cat_num], sigma_0[cat_num]) %>%
  left_join(categories)

## create a data_frame with samples for updated parameters
updated_samples_df <-
  fit_inc %>%
  spread_draws(c(kappa_n, nu_n, mu_n, sigma_n)[block_num, cat_num, cond_num],
               lapse_rate[block_num]) %>%
  left_join(categories) %>%
  left_join(mutate(conditions_exp1, cond_num = as.numeric(bvotCond)))

## create a data_frame for lapsing rate samples
lapse_rate_samples <-
  fit_inc %>%
  spread_draws(lapse_rate[block_num])
```

```{r loo-belief-mod}

class_fun_samples <- 
  updated_samples_df %>%
  ungroup() %>%
  transmute(.draw, vot_cond, block_num, mu=mu_n, sigma2=sigma_n^2, kappa=kappa_n, nu=nu_n, category) %>%
  crossing(data_exp1 %>% group_by(vot) %>% summarise()) %>%
  mutate(lhood = d_nix2_predict(vot, list(mu=mu, sigma2=sigma2, kappa=kappa, nu=nu))) %>%
  select(-mu, -sigma2, -kappa, -nu) %>%
  spread(key = category, value = lhood) %>%
  left_join(select(lapse_rate_samples, lapse_rate, .draw, block_num)) %>%
  mutate(prob_p = (1-lapse_rate) * p / (p+b) + lapse_rate/2) %>%
  select(-lapse_rate)

ll_by_sub_draw <- data_exp1 %>%
  group_by(subject, vot, vot_cond, block_num=ntile(trial, 6)) %>%
  summarise(n_p = sum(respP), n_resp = n()) %>%
  left_join(class_fun_samples) %>%
  group_by(subject, .draw) %>%
  summarise(loglik_binom = sum(dbinom(n_p, size=n_resp, prob=prob_p, log=TRUE)),
            loglik_bernoulli = sum(log(prob_p)*n_p + log(1-prob_p)*(n_resp-n_p)))

ll_bernoulli_mat <- ll_by_sub_draw %>%
  select(.draw, loglik = loglik_bernoulli) %>%
  spread(.draw, loglik) %>%
  ungroup() %>%
  select(-subject) %>%
  as.matrix() %>%
  t()

loo_bernoulli <- loo(ll_bernoulli_mat)

```

```{r loo-glm-baselines}

# convert a sample-by-trial matrix of log likelihoods to a sample-by-subject
# matrix with linear algebra
trials_by_subjects <-
  map(unique(data_exp1_mod$subject), ~ data_exp1_mod$subject == .x) %>%
  lift(cbind)(.)

# first baseline: lapsing logistic regression with same variable lapse rate (by block)
## glm_logit_lapsing_variable <-
##   brm(bf(respP ~ 1,
##          mu1 ~ 1 + bvotCond * vot_s * trial_s,
##          mu2 ~ 1,
##          theta2 ~ 1 + block),
##       family = mixture(bernoulli(), bernoulli()),
##       data = data_exp1_mod,
##       chains=4, iter=1000)
## saveRDS(glm_logit_lapsing_variable, "models/expt1_glm_logit_lapsing_variable.rds")

glm_logit_lapsing_variable <- readRDS("models/expt1_glm_logit_lapsing_variable.rds")

ll_glm_lapsing_var <- log_lik(glm_logit_lapsing_variable)
ll_glm_lapsing_var_bysub <- ll_glm_lapsing_var %*% trials_by_subjects

loo_glm_lapsing_var_bysub <- loo(ll_glm_lapsing_var_bysub)

# second baseline: null model with intercept only
## glm_intercept_only <-
##   brm(respP ~ 1, family=bernoulli(), data = data_exp1_mod, chains=4, iter=1000)
## saveRDS(glm_intercept_only, "models/expt1_glm_intercept_only.rds")
glm_intercept_only <- readRDS("models/expt1_glm_intercept_only.rds")

ll_glm_intercept_only <- log_lik(glm_intercept_only, )
ll_glm_intercept_only_mat <- ll_glm_intercept_only %*% trials_by_subjects

loo_glm_intercept_only <- loo(ll_glm_intercept_only_mat)

```

## Results

The belief updating model can be evaluated in a number of ways.  At the highest
level, it quantitatively fits the data well, as shown by Table -@tbl:loo-tab.
The LOOIC for the model is better (lower) than the LOOIC of two baseline models:
conclusively so for the null, intercept-only model, and numerically better
for the logistic regression baseline, although the SE of that difference is
large so it is not possible to say conclusively.[^aic-uncertainty] Whether or
not the difference is conclusive, the
belief updating model fits the data as well as or better than the logistic
regression baseline despite having many fewer free parameters (excepting the
common lapse rate parameters, 6: prior mean and variance for two categories, and
prior confidence in mean and variance; versus 20: a separate bias and VOT slope
for each of five conditions, plus linear change in these with trial).

[^aic-uncertainty]: Although note that any difference---especially one on the
    order observed here---using analogous model-comparison statistics like AIC,
    BIC, etc. is traditionally treated as evidence in favor of the lower scoring
    model, despite the fact that all of these scores are uncertain estimates
    based on a finite set of data.  LOOIC is in this way a more honest measure
    in that it makes it straightforward to get an approximate measure of how
    stable this estimate is.

Next I will show three ways of evaluating the model's fit _qualitatively_.
First, I discuss the _outcome_ of distributional learning predicted by the
model: does the model capture the classification functions that listeners use at
the end of the experiment?  Second, I look at the _time course_ of the learning
that the model takes: does it capture the how classification functions _change_
with increasing exposure to the experimental distributions?  Third, and finally,
I will examine whether the shared prior beliefs---the starting point for belief
updating in the model---match the distributions produced by a typical talker of
American English.

```{r loo-tab, tbl.cap="LOOIC values for the belief updating model and two baselines (lapsing logistic regression and a null, intercept-only model).  The individual LOOIC values are shown as well as the difference between the belief updating model and the others, along with the standard errors of the estimates associated with each.", results='asis'}

format_loo <- function(myloo) {
  looic <- myloo$estimates['looic', 'Estimate']
  looic_se <- myloo$estimates['looic', 'SE']
  glue("{round(looic)} ($\\pm$ {round(looic_se)})")
}

loo_comp <- loo::loo_compare(
  list(
    "Belief updating"=loo_bernoulli,
    "Baseline (logistic)"=loo_glm_lapsing_var_bysub,
    "Null (intercept-only)"=loo_glm_intercept_only
  )
)

loo_comp_tab <- abs(-2 * loo_comp[, c('elpd_loo', 'se_elpd_loo', 'elpd_diff', 'se_diff')])
loo_comp_tab[loo_comp_tab == 0] <- NA

options("knitr.kable.NA"="")
knitr::kable(
  loo_comp_tab,
  col.names=c("LOOIC", "(SE)", "LOOIC diff.", "(SE)"),
  format.args = list(big.mark = ",", scientific = FALSE),

)

```

```{r mod-class-funs, dependson=c("infer-prior")}

mod_class_funs <- 
  updated_samples_df %>%
  group_by(.draw) %>%
  nest() %>%
  sample_n(200) %>%
  unnest() %>%
  mutate(mean=mu_n, sd=sigma_n) %>%
  select(.draw, block_num, bvotCond, vot_cond, category, mean, sd) %>%
  group_by(.draw, bvotCond, vot_cond, block_num) %>%
  do(stats_to_lhood(., noise_sd=0)) %>%
  lhood_to_classification() %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = (1-lapse_rate)*prob_p + lapse_rate/2) %>%
  group_by(bvotCond, vot_cond, block_num, vot) %>%
  select(bvotCond, block_num, vot, prob_p) %>%
  summarise(prob_p_low = quantile(prob_p, 0.025),
            prob_p_high = quantile(prob_p, 0.975),
            prob_p = mean(prob_p))

prior_class_funs <-
  prior_samples_df %>%
  group_by(.draw) %>%
  nest() %>%
  sample_n(200) %>%
  unnest() %>%
  mutate(mean=mu_0, sd=sigma_0) %>%
  select(.draw, category, mean, sd) %>%
  group_by(.draw, category) %>%
  do(stats_to_lhood(., noise_sd=0)) %>%
  lhood_to_classification() %>%
  group_by(vot) %>%
  summarise(prob_p_low = quantile(prob_p, 0.025),
            prob_p_high = quantile(prob_p, 0.975),
            prob_p = mean(prob_p))

```

```{r mod-vs-behav-class-funs, fig.width=10, fig.height=3, fig.cap="Belief updating model classification functions (thin ribbons) vs. average probability /p/ response from Experiment 1, both during the final sixth of trials.  Points show the average proportion of /p/ responses, and CIs show 95% bootstrapped CIs over subjects.  Ribbons show 95% Bayesian credible interval for model posterior predictions, and the dashed black lines show classification function from the inferred prior."}

data_exp1 %>%
  filter(ntile(trial, 6) == 6) %>%
  group_by(subject, vot, vot_cond) %>%
  summarise(respP = mean(respP)) %>%
  ggplot() +
  geom_line(data = prior_class_funs,
            aes(x=vot, y=prob_p),
            color="black", linetype=2) +
  geom_ribbon(data=filter(mod_class_funs, block_num==6),
              aes(x=vot, ymin=prob_p_low, ymax=prob_p_high, fill=vot_cond),
              alpha=0.5) +
  geom_pointrange(aes(x=vot, y=respP, color=vot_cond),
                  stat="summary", fun.data="mean_cl_boot") +
  facet_grid(.~vot_cond) +
  labs(title = "Belief updating model",
       subtitle = "Compared with behavioral data for final 6th of trials in Experiment 1",
       x = "VOT (ms)",
       y = "Probability /p/ response")

```

```{r mod-vs-behav-class-funs-all-blocks, eval=FALSE}

# all blocks:
data_exp1 %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(subject, vot, vot_cond, block_num) %>%
  summarise(respP = mean(respP)) %>%
  ggplot() +
  geom_pointrange(aes(x=vot, y=respP, group=interaction(vot_cond, block_num),
                      color=vot_cond),
                  stat="summary", fun.data="mean_cl_boot") +
  ## geom_ribbon(data=mod_class_funs,
  ##             aes(x=vot, ymin=prob_p_low, ymax=prob_p_high, fill=vot_cond),
  ##             alpha=0.5) +
  geom_line(data=mod_class_funs,
            aes(x=vot, y=prob_p, group=interaction(vot_cond, block_num), color=vot_cond)) +
  geom_line(data = prior_class_funs,
            aes(x=vot, y=prob_p),
            color="black", linetype=2) +
  facet_grid(.~block_num)

data_exp1 %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(subject, vot, vot_cond, block_num) %>%
  summarise(respP = mean(respP)) %>%
  ggplot() +
  geom_pointrange(aes(x=vot, y=respP, group=interaction(vot_cond, block_num),
                      color=vot_cond),
                  stat="summary", fun.data="mean_cl_boot") +
  ## geom_ribbon(data=mod_class_funs,
  ##             aes(x=vot, ymin=prob_p_low, ymax=prob_p_high, fill=vot_cond),
  ##             alpha=0.5) +
  geom_line(data=mod_class_funs,
            aes(x=vot, y=prob_p, group=interaction(vot_cond, block_num), color=vot_cond)) +
  geom_line(data = prior_class_funs,
            aes(x=vot, y=prob_p),
            color="black", linetype=2) +
  facet_grid(block_num~vot_cond)


```

### Outcome of learning

First, does the belief updating model successfully capture the overall pattern
of incomplete distributional learning by the end of the experiment?  This serves
as a check whether the overall pattern of distributional learning is compatible
with an "ideal adapter" belief updating process, where every listener starts
with the same, shared prior expectations about an unfamiliar talker's VOT
distributions for /b/ and /p/.

Figure -@fig:mod-vs-behav-class-funs shows the belief-updating model's predicted
classification functions in the final sixth of trials (186--222) in the
experiment, along with the average probability of a /p/ response for each VOT (plus
bootstrapped 95% CI over listeners).  Qualitatively, the model provides a good
fit to the overall classification functions for each VOT condition, and the
estimated boundaries align well with the observed boundaries in each case.
<!-- TOO: maybe move this to discussion?? -->
This suggests that the pattern of incomplete distributional learning found in
Experiment 1 is _compatible_ with a belief updating model.  This is not terribly
surprising, since the category boundaries listeners were using by the end of the
experiment appeared to reflect a compromise between a typical talker and the
experimental talker's distributions, and a Bayesian belief-updating model
formalizes exactly this kind of compromise in a statistical framework.  However,
the ability of the model to _quantitatively_ capture the pattern of
distributional learning with a single set of starting beliefs serves as an
existence proof, putting this explanation of the constrained distributional
learning observed above in Experiment 1 on firmer footing.


```{r modeled-category-boundaries}

modeled_boundaries_samples <-
  updated_samples_df %>%
  group_by(.draw) %>%
  nest() %>%
  sample_n(200) %>%
  unnest() %>%
  mutate(mean=mu_n, sd=sigma_n) %>%
  select(.draw, block_num, bvotCond, vot_cond, category, mean, sd) %>%
  group_by(.draw, bvotCond, vot_cond, block_num) %>%
  do(stats_to_lhood(., xlim=c(0, 50), noise_sd=0)) %>%
  lhood_to_classification() %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = (1-lapse_rate)*prob_p + lapse_rate/2) %>%
  find_bound(vot, prob_p)

exp1_blocks6 <-
  data_exp1_mod %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(block_num) %>%
  summarise(trial = mean(trial), trial_s = mean(trial_s))


modeled_boundaries <- modeled_boundaries_samples %>%
  group_by(bvotCond, vot_cond, block_num) %>%
  summarise_at(vars(vot), funs(mean, low=quantile(., 0.025), high=quantile(., 0.975))) %>%
  left_join(exp1_blocks6, by="block_num")

```

```{r}

## re-compute expt1 bounds course with 6-tiles instead of 3-tiles

data_pred6 <-
  cross_df(list(vot_s = (seq(0, 50, by=3) - mean(data_exp1_mod$vot)) / sd(data_exp1_mod$vot),
                bvotCond = unique(data_exp1_mod$bvotCond),
                block_num = 1:6)) %>%
  left_join(exp1_blocks6, by="block_num") %>%
  mutate(vot = vot_s * sd(data_exp1_mod$vot) + mean(data_exp1_mod$vot)) %>%
  left_join(conditions_exp1, by="bvotCond")

expt1_bounds_fixef_samples6 <-
  tidybayes::linpred_draws(b_logit_exp1, data_pred6, re_formula=NA) %>%
  group_by(vot_cond, block_num, trial, .draw) %>%
  find_bound(x=vot, y=.value)

expt1_bounds_fixef6 <-
  expt1_bounds_fixef_samples6 %>%
  group_by(vot_cond, block_num, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))


```

```{r}

bounds_6tile_exp1 <-
  data_exp1 %>%
  select(subject, vot_cond, trial, vot, respP) %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(vot_cond, block_num, vot) %>%
  summarise(respP = mean(respP)) %>%
  find_bound(vot, respP) %>%
  left_join(exp1_blocks6, by="block_num")
  

```

```{r plot-modeled-category-boundaries, fig.width=6.8, fig.height=5.3, fig.cap="Belief updating model's learning curves (ribbons, 95% credible intervals) compared with empirical category boundary, defined as the VOT where subjects' responses (linearly interpolated) are 50% /p/, 50% /b/.  Both model and empirical boundaries are estimated for each sixth of trials (blocks of 37 out of 222)."}

ggplot(modeled_boundaries, aes(x=trial, y=mean)) +
  # geom_line(aes(color=vot_cond)) +
  geom_ribbon(aes(ymin=low, ymax=high, fill=vot_cond), alpha=0.5) +
  geom_text(data = . %>% filter(block_num == 6), aes(label = vot_cond, color=vot_cond),
            hjust=0, nudge_x=6) +
  labs(x = "Trial",
       y = "/b/-/p/ boundary (ms VOT)",
       title = "Model predicted and actual learning curves",
       subtitle = "Change in /b/-/p/ category boundary with learning") +
  lims(x = c(NA, 222* 1.01)) +
  theme(legend.position = "hide") +
  geom_point(data = bounds_6tile_exp1,
             aes(y=vot, color=vot_cond)) +
  geom_line(data = bounds_6tile_exp1,
            aes(y=vot, color=vot_cond))
  ## geom_pointrange(data = expt1_bounds_fixef6,
  ##                 aes(y=mean, ymin=low, ymax=high, color=vot_cond)) +
  ## geom_line(data = expt1_bounds_fixef6,
  ##           aes(y=mean, color=vot_cond))
  ## geom_point(data = expt1_bounds_s,
  ##            aes(y=vot, color=vot_cond)) +
  ## geom_line(data = expt1_bounds_s,
  ##           aes(y=vot, color=vot_cond))


# alternative figure: same style as boundary plots from experiments
## ggplot(modeled_boundaries_samples,
##        aes(x = vot_cond, y = vot, fill=vot_cond, alpha = block_num,
##            group=interaction(block_num, vot_cond))) +
##   geom_violin(position="identity", color=NA) +
##   geom_violin(data = expt1_bounds_fixef_samples,
##               aes(alpha=2*block, color=vot_cond, group=interaction(block, vot_cond)),
##               fill=NA, position="identity") +
##   coord_flip()

```

### Learning curves

Next, how well does the belief-updating model capture the learning curve, or
change in category boundaries with further experience?  Figure
-@fig:plot-modeled-category-boundaries shows the belief-updating predicted
learning curves, compared with the empirical boundaries (points/lines) estimated
for each condition in each sixth of the experiment (blocks of 37 trials).  The
belief updating model qualitatively captures the change in category boundaries,
both within each condition and across conditions.  Boundaries start off quite
similar at the beginning of the experiment, and gradually diverge as listeners
gain more exposure to the particular VOT distributions of their condition.

Comparing these modeled learning curves with the behavioral data directly is not
straightforward.  Figure -@fig:plot-modeled-category-boundaries shows one
estimate of the changing category boundaries, estimated directly from the data
in each sixth of the experiment (e.g., the VOT where the average proportion-/p/
response curve crosses 50% in each condition).  Other reasonable possibilities
include a boundary calculated from the regression model reported in Experiment 1
(which includes a linear change in intercept, or /p/ bias, over trials, as well
as a linear change in the slope of the VOT boundary), or a more complex
regression model using a spline term in each condition to capture non-linear
changes in the offset (but not the slope).

All of these methods agree qualitatively on where the boundaries are _later_ in
the experiment, when listeners make very few errors or random responses.  Where
they disagree (with each other, and with the belief updating model) is in the
estimated boundaries for the early trials, where listeners make many random or
guessing responses, even on unambiguous VOTs.  In the belief updating model,
this is captured by the "lapse rate" or the proportion of trials on which the
simulated subjects simply guess randomly, which is allowed to vary over trials
(Figure -@fig:lapse-rate-by-block).  The inferred lapse rate is nearly 12% in
the first sixth of trials (1-37), while for the remainder of the experiment it
is closer to 5%.  Models that don't take into such guessing into account are
subject to mis-estimate the boundary [location and slope @Clayards2008;
@Wichmann2001], and so for this reason I chose the more interpretable---if
somewhat noisier---direct estimate of the boundary over the estimates from
either of the regression models for comparison with the belief updating model.

```{r lapse-rate-by-block, fig.width=5.8, fig.height=3.9, fig.cap="Lapse (guess) rate inferred in the belief updating model for each 1/6th of trials in Experiment 1.  The inferred lapse rate starts rather high (nearly 12%) but decreases to around 5% after the first 1/6 of trials (1--37)"}

lapse_rate_samples %>%
  summarise_at(vars(lapse_rate),
               funs(mean, low=quantile(., 0.025), high=quantile(., 0.975))) %>%
  left_join(exp1_blocks6, by="block_num") %>%
  ggplot(aes(x=trial, y=mean, ymin=low, ymax=high)) +
  geom_pointrange() +
  labs(y="Guessing rate",
       title="Inferred guessing rate for belief updating model",
       subtitle="Means and 95% credible intervals") +
  lims(y=c(0, NA),
       x=c(0, NA))

```

### Inferred prior beliefs

Finally, what are the prior beliefs that the model infers listeners are bringing
to this task?  There are two relevant aspects of the inferred prior beliefs: the
_content_ of those beliefs, and their _strength_.  The strength corresponds to how
confident the modeled listener is in their prior beliefs, which is represented
in the model in the prior confidence pseudocounts for the means ($\kappa_0$) and
variance ($\nu_0$).  These parameters correspond to the number of observations
required to get to a point where the listener's updated beliefs are in equal parts
informed by their prior expectations and recent experience.

```{r confidence-params, fig.width=5.6, fig.height=4.4, fig.cap="Sampled values of prior confidence parameters from belief updating model fit to Experiment 1.  Upper left shows values where $\\kappa_0 < \\nu_0$ (\"shifting\"), and lower right shows values where $\\nu_0 < \\kappa_0$ (\"expanding\").  Red stars show approximate MAP values for each kind of solution (individual sample with the highest posterior)."}

confidence_summaries <-
  prior_samples_df %>%
  filter(category=="b") %>%
  ungroup() %>%
  summarise_at(vars(kappa_0, nu_0), funs(mean, low=quantile(., 0.025), high=quantile(., 0.975))) %>%
  mutate_all(funs(formatC(signif(., digits=2), digits=2, format="fg", preserve.width="none")))

p_shift <- round(with(prior_samples_df, mean(nu_0 > kappa_0)), 2)

confidence_maps <- tidybayes::spread_draws(fit_inc, kappa_0, nu_0, lp__) %>%
  group_by(kappa_0 > nu_0) %>%
  arrange(desc(lp__)) %>%
  filter(row_number() == 1)

prior_samples_df %>%
  ggplot(aes(x=kappa_0, y=nu_0)) +
  #geom_density2d() +
  geom_point(alpha=0.05) +
  scale_x_log10(TeX("Prior confidence in mean (pseudo-observations $\\kappa_0$)")) +
  scale_y_log10(expression(atop("Prior confidence in variance", paste("(pseudo-observations ", nu[0], ")")))) +
  coord_equal() +
  geom_abline() +
  geom_point(data = confidence_maps, size=10, shape="*", color="red")

```

Based on the data from Experiment 1, listeners are highly confident in their
prior beliefs.  The expected prior pseudocount for the means was
$\kappa_0 = `r confidence_summaries$kappa_0_mean`$ (95% credible interval of
$[`r confidence_summaries['kappa_0_low']`,
`r confidence_summaries['kappa_0_high']`]$).  This is nearly twice the number of
trials in the experiment from each cluster (111 each, for a total of 222),
meaning that by the end of the experiment the model's beliefs about the mean
VOTs for /b/ and /p/ are still primarily determined by the prior beliefs.
Similarly, the expected prior pseudocount for the variances 
$\nu_0 = `r confidence_summaries$nu_0_mean`$ (95% credible interval of 
$[`r confidence_summaries["nu_0_low"]`,
`r confidence_summaries["nu_0_high"]`]$), which is much larger than the number
of trials from each cluster.

Together, these parameters suggest that listeners have strong prior beliefs
about the VOT distributions associated with /b/ and /p/, and moreover that their
beliefs in the _variance_ of VOTs is stronger than their beliefs in the _means_.
In turn, this suggests that the distributional learning observed in Experiment 1
is driven mostly by changes in listeners' beliefs about the underlying means,
with only small contributions from changes in the variance (e.g., a relaxation
in their standards for what counts as a good /b/ or /p/).

However, as in previous work with similar belief updating models
[@Kleinschmidt2015b], the posterior distribution of these parameters was
bimodal.  One mode that puts more confidence in the means ($\kappa_0 > \nu_0$)
and adapts mostly by updating beliefs in the variance, while the other puts more
in the variances ($\nu_0 > \kappa_0$) and adapts by shifting.  Nevertheless, the
"shifting" solution is preferred, with $\nu_0 > \kappa_0$ in the majority of
MCMC samples (Bayesian $p$ value of `r p_shift`).

```{r inferred-beliefs, fig.width = 8, fig.height=5, fig.cap="Inferred prior beliefs from belief updating model, compared with VOT distributions estimated by @Kronrod2016.  Inferred priors are derived using the maximum a posteriori (MAP) estimate of the expected prior mean and variance parameters $\\mu_0$, $\\sigma^2_0$ (heavy lines).  To give a sense of the range of uncertainty that the *model* has about listener's prior beliefs based on their behavior in the experiment, also plotted are the prior beliefs from 100 values sampled from the posterior (light lines)."}

map_iter <- spread_draws(fit_inc, lp__) %>%
  arrange(desc(lp__)) %>%
  head(1)

## prior_samples_df %>%
##   gather(param, value, mu_0, sigma_0) %>%
##   ggplot(aes(x=value)) +
##   geom_density() +
##   facet_grid(param ~ category, scales="free")

## prior_samples_df %>%
##   ggplot(aes(y=log(kappa_0), x=mu_0, color=category)) +
##   geom_point(alpha=0.1) +
##   geom_point(data = . %>% inner_join(map_iter, by=".draw"),
##              size=6, shape="*", color="black")

map_prior_stats <-
  prior_samples_df %>%
  ungroup() %>%
  inner_join(map_iter, by=".draw") %>%
  select(category, mean=mu_0, sd=sigma_0)

map_prior_lhood <- 
  map_prior_stats %>%
  stats_to_lhood(noise_sd = 0)  

prior_stats_by_talker %>% 
  group_by(source, category, prevoiced) %>% 
  summarise_at(vars(mu, sigma2), mean) %>% 
  mutate(sigma = sqrt(sigma2)) %>% 
  rename(mean=mu, sd=sigma) %>% 
  group_by(source, category, prevoiced) %>%
  stats_to_lhood()

# extract 100 draws and draw lhood functions
n_prior_samps <- 100
sampled_prior_lhood <- prior_samples_df %>%
  group_by(.draw) %>%
  select(.draw, category, mean=mu_0, sd=sigma_0) %>%
  nest() %>%
  sample_n(n_prior_samps) %>%
  unnest(lhood=map(data, ~ stats_to_lhood(., noise_sd=0))) %>%
  left_join(spread_draws(fit_inc, lp__))

ggplot(mapping=aes(x=vot, y=lhood, linetype=category, color=source)) +
  geom_line(data = map_prior_lhood,
            aes(color="Belief updating model"),
            size=1) +
  geom_line(data = prior_lhood,
            aes(color="Kronrod et al. (2016)"),
            size=1) +
  geom_line(data = sampled_prior_lhood,
            aes(group=interaction(.draw, category), color="Belief updating model"),
            show.legend = FALSE, alpha=0.1) +
  scale_x_continuous(breaks = seq(-40, 120, by=20)) +
  theme(axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.line.y = element_blank(),
        legend.position=c(1,1),
        legend.justification=c(1,1)
        ) +
  labs(title = "Inferred prior beliefs",
       subtitle = "Maximum a posteriori beliefs and 100 samples from belief updating model")+
  scale_color_discrete("") +
  scale_linetype_discrete("")

```

Figure -@fig:inferred-beliefs shows the prior expected VOT distributions for /b/
and /p/ that are most likely given the distributional learning behavior (maximum
a posteriori/MAP), along with .  The belief updating model recovers distributions that are
similar but not identical to those estimated by @Kronrod2016 (based on
classification and discrimination data, assuming that /b/ has a mean VOT of 0
ms).  The inferred distribution for /p/ has a slightly lower mean but very
similar variance, while /b/ has a similar mean but higher variance.  Notably,
the variance of the two distributions is quite similar, and the difference
between the prior means for /b/ and /p/ is very close to 40ms, the difference
between the cluster means of the _exposure_ distributions.

Also shown in Figure -@fig:inferred-beliefs are 100 samples from the
distribution of prior beliefs given listeners' behavior in Experiment 1.  This
shows that the behavior does not uniquely constrain these prior beliefs, and is
instead compatible with belief updating starting from prior beliefs with a range
of means and variances.

## Discussion

The results of the belief updating modeling show that the pattern of constrained
distributional learning from Experiment 1 can be explained as belief updating
starting from a shared set of prior beliefs.  Both the outcome of this learning
and the learning curves are captured by a Bayesian belief updating model modeled
on previous work on phonetic recalibration [@Kleinschmidt2015b].  This suggests
that despite their superficial differences, both recalibration and
distributional learning may be reflections of the same underlying process of
belief updating [see also @Theodore2019].

### Content of prior beliefs

The starting point for belief updating that is most consistent with the
distributional learning behavior from Experiment 1 is reasonably similar to
VOT distributions produced by a typical talker of American English.  The notable
exception to this is that the belief updating model infers that the /b/ and /p/
distributions have similar variance, whereas in actual speech the /b/
distribution has much lower variance compared with /p/.  This likely reflects
the fact that such variance asymmetry is hard to detect from purely
_classification_ data.  @Kronrod2016 used discrimination data in addition to
classification data in order to estimate listeners' beliefs about the underlying
variances of voiced and voiceless stops (in addition to the sensory uncertainty
associated with VOT itself).

### Strength of prior beliefs

The belief updating model also suggests that these prior beliefs are relatively
strong, with listeners acting as if their prior beliefs were based on
$2-5\times$ more VOT samples prior than they actually encounter during the
experiment itself.  This is in stark contrast to the results of the same belief
updating model applied to /b/-/d/ recalibration [@Kleinschmidt2015b], which
found that listeners have relatively _weak_ prior beliefs.  Despite the fact
that the number of trials in the experiments were similar (about 200),
@Kleinschmidt2015b found that recalibration of /b/-/d/ was best explained by
prior beliefs that were 10 times weaker than here (prior pseudocounts on the
order of 10s of observations, rather than 100s like the current results).  Why
this discrepancy?  One possibility is the different learning tasks: in a
recalibration tasks like that being modeled by @Kleinschmidt2015b, listeners
repeatedly hear stimuli chosen to be acoustically ambiguous (between /aba/ and
/ada/ in this case), paired with a label of some kind of label [like a visual
cue to the articulatory gesture; @Bertelson2003; @Vroomen2007].  While my
Experiment 2 found that providing labeling information did not appreciably
change the speed or completeness of distributional learning, it's possible that
by targeting ambiguous stimuli specifically---rather than the wider range of
stimuli used here---could lead to more reliance on labeling information
[although the fact that exposure to not-fully-ambiguous or even prototypical
stimuli leads to equally rapid belief updating suggests this is unlikely;
@Kleinschmidt2015b].

Another possibility, proposed by @Kleinschmidt2015b, is that the strength of
listeners' prior beliefs about a particular cue distribution is related to how
much that particular distribution varies _across talkers_: when talker
variability is low, an _ideal adapter_ will use strong prior beliefs and thus be
relatively inflexible.  While there is _some_ talker variability in VOT
distributions [@Chodroff2017; @Allen2003], there is much less variability than
in vowel formant frequencies [@Kleinschmidt2019].  Steady-state vowel formant
frequencies are not exactly the same as the formant _trajectories_ which are the
primary cue for distinguishing voiced intervocalic stops, but there is still
likely to be similar levels of talker variability in such spectral cues.




# Experiment 4

The evidence presented thus far suggests that adult distributional learning is
constrained in substantial ways (Experiment 1), that these constraints cannot be
attributed to uncertainty about the intended category due to the minimal pair
words used (Experiment 2), and that the observed constraints are at least
consistent with belief updating starting from a shared set of prior beliefs
(Experiment 3).  One interpretation of the modeling results from Experiment 3 is
that the prior beliefs that the model infers as the starting point for
distributional learning actually correspond to the real contents of listener's
prior beliefs.  However, despite the fact that some of the VOT distributions
tested thus far deviate substantially from a typical talker of American English,
in reality they cover only a rather narrow range of possible VOT distributions.

In this final experiment, I test listeners' distributional learning from
distributions that vary more dramatically, in two ways.  First, I include
distributions with large negative VOTs.  On the one hand, negative VOTs
(prevoicing) are common cross-linguistically, and are also produced by many
speakers of American English in word-initial voiced stops [@Goldrick2013;
@Lisker1964].  On the other hand, word-initial voiced stops are canonically
analyzed as un-aspirated un-voiced stops with VOTs around zero, and American
English listeners have difficulty in recalibrating their classification
boundaries to negatively-shifted VOT distributions [@Sumner2011], so there may
be less flexibility in how they are interpreted relative to positive VOTs.

Second, in Experiments 1 and 2 the means of the two VOT distributions vary in
lock-step.  There is some evidence from production data [@Chodroff2017] that the
means of different VOT distributions may be positively correlated across talkers
in this way.  If listeners are sensitive to this structured variation across
talkers, it may result in additional constraints on distributional learning not
detectable with the range of distributions tested in Experiments 1 and 2.

In both cases, the belief-updating model from Experiment 3 does not include
these possible constraints.  In the first case, it assumes that there are
exactly and only two VOT distributions, and cannot account for the fact that in
reality there are three clusters of VOT of word-initial stops in American
English, with means that are negative (prevoiced), approximately zero
(un-aspirated), and positive (aspirated).  In the second case, the model assumes
that the priors on the VOT distributions for /b/ and /p/ are _independent_, and
thus by design cannot capture additional constraints on distributional learning
that might arise from experience with correlated variability across different
phonetic categories.

Thus, this final experiment provides a challenging test of the ability of the
prior beliefs inferred from the pattern of distributional learning for various
distributions in Experiment 1 to _generalize_ to quite different situations.

```{r sepmeans-data}

sepmeans <- supunsup::separatemeans_clean

sepmeans_conds <-
  sepmeans %>%
  group_by(bvotCond, pvotCond) %>%
  summarise() %>%
  ungroup() %>%
  arrange(bvotCond, pvotCond) %>%
  mutate(vot_cond = paste(bvotCond, pvotCond, sep=', '),
         vot_cond = factor(vot_cond, levels=vot_cond),
         ideal_boundary = (pvotCond + bvotCond)/2)

sepmeans %<>% inner_join(sepmeans_conds)

sepmeans_test <- sepmeans %>%
  filter(is_test) %>%
  mutate(vot_s = (vot - mean(vot)) / sd(vot),
         trial_s = (trial - mean(trial)) / sd(trial))

post_test_len <-
  sepmeans_test %>%
  filter(subject==first(subject)) %>%
  nrow()

post_test_len_each <-
  sepmeans_test %>%
  filter(subject==first(subject)) %>%
  group_by(vot) %>%
  tally() %>%
  pull(n) %>%
  unique()

exp4_blocks <-
  sepmeans_test %>%
  group_by(block=ntile(trial, 6)) %>%
  summarise_at(vars(trial, trial_s), mean)

```

## Methods

```{r exposure-dists-exp4, fig.width=10, fig.height=3, fig.cap="In Experiment 4, each subject heard a talker that produced one of these five VOT distributions. The variance of each category was constant across conditions and the same as in Experiments 1 and 2, but the means varied semi-independently.  The first four conditions had a mean /p/ VOT of 50ms, with /b/ mean from -80 to 10ms VOT.  The 10ms /b/, 50ms /p/ condition was the same exposure distributions as in Experiment 1.  The final condition had means of 10ms for /b/ and 80ms for /p/.  Distributions for a typical talker are shown in dashed lines."}

sepmeans %>%
  left_join(sepmeans_conds) %>%
  filter(!is_test) %>%
  group_by(vot_cond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=vot_cond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  facet_grid(.~vot_cond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  labs(title="Experiment 4: VOT distribution conditions",
       subtitle="Mean VOTs for /b/ and /p/ clusters")


```

As in Experiment 1, listeners performed a word identification task without
labeling information, selecting from a pair of images corresponding to
/b/-/p/ minimal pair words _beak-peak_, _beach-peach_, and _bees-peas_.  Each
listeners heard one of five different synthetic "accents," corresponding to a
different bimodal distributions of VOT.  Across these distributions, the mean
VOT for /b/ and /p/ varied semi-independently (Figure
-@fig:exposure-dists-exp4). These distributions cover a wider range of implied
category boundaries (from -15 to 45ms VOT) than Experiment 1 (10 to 50ms),
especially on the lower end.  They also varied in the distance between cluster
means (from 40 to 130ms VOT), unlike Experiment 1 which maintained a constant
separation between /b/ and /p/ mean VOT of 40ms.  Note that the condition with
/b/ mean of 10ms and /p/ mean of 50ms VOT is the same as in Experiment 1.  This
condition was included to provide a way of comparing with Experiment 1, since
the design of Experiment 4 required a change in procedure (discussed next).

### Procedure: assessing category boundaries

The procedure was identical to Experiment 1, with one exception. After
completing the 222 trial exposure phase as in Experiment 1, listeners completed
a test phase in order to assess their category boundaries. This phase consisted
of `r post_test_len` additional trials with VOTs evenly distributed from -10 to
50ms in 10ms steps (`r post_test_len_each` each). This additional phase was
necessary because of the large
separation between /b/ and /p/ clusters in the exposure distributions meant that
in most conditions there were no trials with VOTs anywhere near the predicted
(or typical) boundaries.

Critically, listeners were not told about the change from exposure to test
phase: the procedures were identical, and there was no break in between.
Besides the change in VOT distributions, there was no way for listeners to tell
that they had entered the test phase. Of course, if listeners are actually (as
hypothesized) _learning_ these distributions, their behavior may well change
as they proceed through the test phase, gradually erasing any effect of the
differences in the exposure distributions. Thus, when analyzing data from the
test phase, whenever possible analyses are focused on the early parts of the test
phase, when listeners behavior should be minimally affected by the change in
distributions.

As in Experiment 1, listeners' adaptation was evaluated via their classification
functions, estimated with a hierarchical Bayesian logistic regression model via
`brms` [@Burkner2017].  In
addition to VOT, a predictor for trial was also included, in order to account
for any (un-)learning effects that might happen during post-test, as well as all
interactions between these and condition.[^expt4-formula]  In order to
visualize the fitted classification functions and to estimate listeners'
category boundaries, I used the classification function estimates from the trial
halfway through the first third of the post-test.[^why-not-beginning]

[^expt4-formula]: The complete formula for the model was `respP ~ 1 + vot_cond *
    vot_s * trial_s + (1 + vot_s | subject)`.  As before, VOT and trial were
    centered and scaled to have zero mean and unit variance.  Because the
    regression coefficients were not directly interpreted (the fitted category
    boundaries used instead), the default (treatment) coding was used for
    condition.  The default priors for parameters were used.

[^why-not-beginning]: The predictions from a regression model are more uncertain
    and more driven by noise at the extreme ranges of continuous predictors,
    like trial number. Thus I use estimated value at the trial that is one-sixth
    into the post-test to estimate
    classification functions, rather than the very beginning, because it is
    _close_ to the beginning of the post test, but not so close that the
    predictions are substantially affected by the extra instability of the
    predictions that come at the edge of a continuous predictors range.


### Participants

```{r exp2-subjects}

n_excl4 <- supunsup::separatemeans_excluded %>%
  left_join(sepmeans_conds) %>%
  group_by(vot_cond, subject) %>%
  summarise() %>%
  tally()

n_clean4 <- sepmeans %>%
  left_join(sepmeans_conds) %>%
  group_by(vot_cond, subject) %>%
  summarise() %>%
  tally()

n_total4 <- bind_rows(n_excl4, n_clean4) %$% sum(n)

accept_to_submit_time <-
  supunsup::separatemeans_assignments %>%
  do(daver::boot_ci(., function(d,i) with(d[i, ], mean(submittime-accepttime))))

active_time <- supunsup::separatemeans %>%
  group_by(subject) %>%
  summarise(time = max(tend) - min(tstart)) %>%
  mutate(time_min = time / 60000) %>%
  do(daver::boot_ci(.$time_min, function(d,i) mean(d[i])))

```

A total of `r n_total4` participants were recruited via Amazon's Mechanical
Turk.  Subjects were paid \$2.50 for participation.  The task took subjects
about 25 minutes to complete (because of the additional post-test, discussed
above).  As in Experiment 1, subjects were excluded
who failed to classify words with unambiguous VOTs reliably during exposure
($n=`r n_excl4 %$% sum(n)`$).  After exclusion, `r n_clean4 %$% sum(n)` subjects
remained for analysis (no subjects had previously participated in previous
experiments).  Subjects were evenly distributed across conditions (range of
`r n_clean4 %$% min(n)` to `r n_clean4 %$% max(n)` per condition).

```{r sepmeans-regression}

f4 <- respP ~ 1 + vot_cond * vot_s * trial_s + (1 + vot_s | subject)

## b_logit_exp4 <- brm(f4, data=sepmeans_test, family=bernoulli(), chains=4, iter=1000)
## saveRDS(b_logit_exp4, "models/brm_logistic_expt4.rds")
b_logit_exp4 <- readRDS("models/brm_logistic_expt4.rds")

sepmeans_pred <-
  cross_df(list(vot_s = seq(min(sepmeans_test$vot_s),
                            max(sepmeans_test$vot_s),
                            length.out=100),
                vot_cond = unique(sepmeans_test$vot_cond),
                block = 1:6)) %>%
  left_join(exp4_blocks, by="block") %>%
  mutate(vot = vot_s * sd(sepmeans_test$vot) + mean(sepmeans_test$vot)) %>%
  left_join(sepmeans_conds, by="vot_cond")

sepmeans_class <-
  fitted(b_logit_exp4, newdata = sepmeans_pred, re_formula = NA) %>%
  as_tibble() %>%
  bind_cols(sepmeans_pred)

sepmeans_pred_subj <-
  sepmeans_test %>%
  group_by(subject, vot_cond) %>%
  summarise() %>%
  inner_join(sepmeans_pred, by="vot_cond")

sepmeans_class_bysub <-
  fitted(b_logit_exp4, newdata = sepmeans_pred_subj) %>%
  as_tibble() %>%
  bind_cols(sepmeans_pred_subj)

```

## Results



```{r sepmeans-results, fig.width=10, fig.height=3, fig.cap="Classification functions from Experiment 4.  Fine lines show individual subjects' classification functions (estimated based on random effects/subject-level predictors in the Bayesian mixed effects model) and thick solid lines show the group-level estimates (based on the fixed effects).  Thick colored dashed lines show the ideal categorization function given the exposure distributions, and the thin black dashed lines show the categorization function for a typical talker." }

sepmeans_exposure_stats <- sepmeans %>%
  filter(!is_test) %>%
  group_by(vot_cond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sepmeans_perfect_learning <- sepmeans_exposure_stats %>%
  group_by(vot_cond) %>%
  do(stats_to_lhood(.)) %>%
  lhood_to_classification()

ggplot(sepmeans_class_bysub, aes(x=vot, color=vot_cond)) +
  geom_line(data=.%>%filter(block==1), aes(y=Estimate, group=subject), alpha=0.2) +
  facet_grid(.~vot_cond) +
  geom_line(data=sepmeans_perfect_learning, aes(y=prob_p), group=1, linetype="33", size=1,
            show.legend=FALSE) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype="99", color='black',
            show.legend=FALSE) +
  geom_line(data = filter(sepmeans_class, block==1), aes(y=Estimate), size=1) +
  ## geom_point(data=expt1_bounds %>% filter(block==3),
  ##            y=0.5) +
  labs(x="VOT (ms)",
       y="Probability /p/ response",
       title="Experiment 4: Classification functions",
       subtitle="Fit with mixed effects logistic regression, predictions for first sixth of test trials") +
  lims(x = c(-10, 70))

```

```{r sepmeans-bounds }

sepmeans_bounds <-
  sepmeans_class %>%
  group_by(vot_cond, trial, block) %>%
  find_bound(x=vot, y=Estimate)

sepmeans_bounds_bysub <-
  sepmeans_class_bysub %>%
  group_by(vot_cond, subject, trial, block) %>%
  find_bound(x=vot, y=Estimate)

sepmeans_bounds_fixef_samples <-
  tidybayes::linpred_draws(b_logit_exp4, sepmeans_pred, re_formula=NA) %>%
  group_by(vot_cond, block, trial, .draw) %>%
  find_bound(x=vot, y=.value)

sepmeans_bounds_fixef <-
  sepmeans_bounds_fixef_samples %>%
  group_by(vot_cond, block, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))

```

Figure -@fig:sepmeans-results shows the classification functions produced by
listeners during the first sixth of the test phase of Experiment 4 (estimated by
Bayesian hierarchical logistic regression), and Figure
-@fig:plot-sepmeans-bounds shows the corresponding estimates of the /b/-/p/
boundaries.  First, consider the 10, 50ms condition.  This condition serves as a
replication of the condition with the same exposure distributions from
Experiment 1, and a check of the post-test procedure used in the other
conditions of Experiment 4 to measure listeners' category boundaries.  As Figure
-@fig:plot-sepmeans-bounds shows, listeners in the 10, 50ms condition from
Experiment 4 produced very similar---if somewhat more variable---category
boundaries in the post-test (filled violin) compared with the boundaries
measured in the 10, 50ms condition Experiment 1 (empty
violin), despite the fact that the Experiment 4 boundaries were measured during
post-test and the Experiment 1 boundaries during
exposure.  There is also more uncertainty associated with the estimates of the
group-level (fixed-effects) boundaries as well (the filled vs. open white
circles and associated credible intervals), even though the estimates themselves
are largely in agreement (27 vs. 28ms VOT).  On the one hand, this validates the
use of the post-test procedure in Experiment 4 as a way of measuring the outcome
of distributional learning.  On the other hand, the greater variability in these
measurements suggests that the use of a post-test might reduce the power to
detect _differences_ in learning between conditions of Experiment 4.

```{r boundary-summaries-exp4}

boundary_diff_p_vals <-
  sepmeans_bounds_fixef_samples %>%
  filter(block == 1) %>%
  spread(vot_cond, vot) %>%
  transmute(b20_50 = `-20, 50` - `-50, 50`,
            b50_80 = `-50, 50` - `-80, 50`,
            b20_80 = `-20, 50` - `-80, 50`,
            b80_50 = `10, 80` - `10, 50`) %>%
  gather("contrast", "diff", -block, -trial) %>%
  group_by(contrast) %>%
  summarise_at("diff", list(mean=mean,
                            low=~quantile(., 0.025),
                            high=~quantile(., 0.975),
                            p=~mean(.>0))) %>%
  mutate(mean_ci = glue("{signif(mean, 2)}ms (95% credible interval $[{signif(low, 2)}, {signif(high, 2)}]$)"))

expt4_bounds_vs_no_learning <-
  sepmeans_bounds_fixef_samples %>%
  filter(block==1) %>%
  group_by(vot_cond) %>%
  summarise(p = mean(vot < no_learning_bound))

```

Next, exposure to large negative VOTs produces comparably small shifts in the
/b/-/p/ category boundary.  In fact, in the conditions with a lower mean of -80,
-50, and -20ms, listeners' category boundaries seem to cluster around the
boundary for a typical talker, rather than lying in between the typical talker's
boundary and the exposure talker's ideal boundary as in Experiment 1.  Moreover,
while the differences in boundaries between these three conditions are trending
in the expected direction, with more negative VOTs expected to lead to lower
category boundaries, these differences are small (on the order of 0-5ms,
compared with exposure distributions with 15ms differences between implied
boundaries) and not particularly reliable (going in the expected direction with
only $p=`r boundary_diff_p_vals[['p']][1]`$[^bayes_p2] for the -20ms vs. -50ms
/b/ conditions and $p=`r boundary_diff_p_vals[['p']][3]`$ for -50ms vs. -80ms
/b/).  Even comparing the -80ms /b/ and -20ms /b/ conditions, the difference
between the boundaries is more reliable but still small in absolute terms and
with a credible interval which contains 0, only
`r boundary_diff_p_vals[['mean_ci']][2]`.

[^bayes_p2]: As above, these are Bayesian posterior $p$ values, or the
    proportion of posterior samples in which the inferred boundary was higher
    for the condition with the higher exposure mean.

However, this lack of robust distributional learning in this experiment is not
unique to large
negative VOTs.  The 10ms /b/, 80ms /p/ mean condition also produces comparably
small shifts in category boundaries compared with the 10ms /b/, 50ms /p/
condition, with a posterior mean difference between the population-level
boundaries of only `r boundary_diff_p_vals[['mean_ci']][4]`.  Compare this with 
the 15ms difference between the boundaries implied by the exposure distributions
(30ms vs. 45ms VOT).

```{r plot-sepmeans-bounds, fig.cap="/b/-/p/ category boundaries from Experiment 4.  Violins show distributions of subject's boundaries (estimated based on Bayesian mixed effect logistic regression) and points/CIs show the posterior mean and 95% CI for the group-level (fixed effect) estimate from the same regression.  Subjects' boundaries from the Experiment 1 condition with the same exposure statistics are shown in outline, and are quite close to the results from Experiment 4 (if less variable).  Note that the position of the 'complete learning' boundaries (thick dashed lines; based on the exposure statistics) are much more extreme than in Experiment 1, reflecting the more extreme shifts in the distributions that listeners experience in Experiment 4."}

sepmeans_bounds_bysub %>%
  filter(block==1) %>%
  ggplot(aes(x=vot_cond, y=vot)) +
  geom_violin(alpha=0.5, aes(fill=vot_cond, color=vot_cond)) + #, color=NA)+
  geom_violin(data = expt1_bounds_bysub %>%
                filter(vot_cond == "10, 50"),
              aes(fill=vot_cond, color=vot_cond),
              alpha=0) +
  geom_pointrange(data = sepmeans_bounds_fixef %>% filter(block==1),
                  aes(y=mean, ymin=low, ymax=high),
                  color="white", show.legend=FALSE) +
  geom_pointrange(data = expt1_bounds_fixef %>% filter(block==3, vot_cond=="10, 50"),
                  aes(y=mean, ymin=low, ymax=high), shape = 1,
                  position=position_nudge(x=0.1),
                  color="white", show.legend=FALSE) +
  theme(legend.position="none") +
  scale_shape_manual(values = c(16, 1)) +
  lims(y=c(-16,50)) +
  labs(title="Experiment 4: Category boundaries",
       subtitle="Estimated values at first sixth of test block",
       y = "Boundary VOT (ms)",
       x = "Exposure condition (/b/, /p/ mean VOT)") +
  geom_hline(yintercept=no_learning_bound, color="black", linetype=2) +
  geom_segment(aes(x=as.numeric(vot_cond)-.5, # exposure boundaries_exp1
                   xend=as.numeric(vot_cond)+.5,
                   y=ideal_boundary,
                   yend=ideal_boundary,
                   color=vot_cond),
               linetype = 2, size = 1,
               data = sepmeans_conds) +
  geom_text(data=sepmeans_conds %>% filter(vot_cond=="10, 50"),
            aes(y=ideal_boundary, color=vot_cond),
            ## x=3.5, y=41, 
            label="Complete learning\n(Exposure talker)",
            hjust = 0, vjust = 0,
            nudge_x = -0.7, nudge_y = 1) +
  geom_text(data=perfect_learning_bound %>% filter(vot_cond=="10, 50"),
            aes(color=vot_cond, x = as.numeric(vot_cond)+1.3),
            y=no_learning_bound-1, color='black',
            ## x=3.5, y=41, 
            label="No learning\n(Typical talker)",
            hjust = 1, vjust = 0,
            ) +
  coord_flip()

```

```{r sepmeans-posterior-check}

#' Convert from stan parametrization to beliefupdatr::nix2
stan_conj_to_nix2 <- function(stan_p) {
  with(stan_p, list(nu = nu_0,
                    kappa = kappa_0,
                    mu = mu_0,
                    sigma2 = sigma_0 ^ 2))
}

# convert samples of prior params in array form to list of samples in nix2
# parameter list form.
#
# e.g., mod_nix2_samples[[1]][[1]] is the first 
mod_nix2_samples <- 
  rstan::extract(fit_inc, pars=c('nu_0', 'kappa_0', 'mu_0', 'sigma_0')) %>%
  ## repeate these since there's just one for both categories
  map_at(c('nu_0', 'kappa_0'), ~ cbind(.x, .x)) %>%
  ## turn arrays into nested lists
  map(array_tree) %>%
  ## zip list of variables into list of samples
  transpose() %>%
  ## zip each sample's list of variables into list of categories
  map(transpose) %>%
  ## ...and zip list of samples into a list of categories
  transpose() %>%
  set_names(c('b', 'p')) %>%
  ## rename and convert expected sd to var
  at_depth(2, stan_conj_to_nix2)

## confirm that we have nix2 params at depth 2
## invisible(mod_nix2_samples %>% at_depth(2, ~ assert_that(is_nix2_params(.))))

## get summary statistics for each condition
updated_nix2_samples <- 
  sepmeans %>%
  left_join(sepmeans_conds) %>%
  group_by(bvotCond, pvotCond, trueCat) %>%
  filter(subject == first(subject),
         is_test == FALSE) %>%
  nest() %>%
  mutate(prior_samples = map(trueCat, ~ mod_nix2_samples[[.x]]),
         updated_samples = map2(data, prior_samples,
                                function(d, s) map(s, nix2_update, x=d$vot)))

sample_to_lhood <- function(p)
  data_frame(vot = seq(-10,50),
             lhood = d_nix2_predict(vot, p))

predicted_lhood <- updated_nix2_samples %>%
  mutate(test_lhood = map(updated_samples,
                          . %>%
                            map(sample_to_lhood) %>%
                            do.call(what=rbind))) %>%
  unnest(test_lhood)

lapse_rate_samples_block6 <-
  rstan::extract(fit_inc, pars='lapse_rate')[[1]][, 6] %>%
  as.numeric() %>%
  tibble(lapse_rate=.) %>%
  mutate(sample = row_number())

predicted_prob_p_samples <-
  predicted_lhood %>%
  group_by(bvotCond, pvotCond, trueCat, vot) %>%
  mutate(sample = row_number()) %>%
  spread(trueCat, lhood) %>%
  left_join(lapse_rate_samples_block6) %>%
  mutate(prob_p = p / (b+p),
         prob_p_lapse = prob_p * (1-lapse_rate) + lapse_rate*0.5)

predicted_prob_p <-
  predicted_prob_p_samples %>%
  group_by(bvotCond, pvotCond, vot) %>%
  summarise_at(vars(prob_p, prob_p_lapse),
               funs(mean=mean, lo=quantile(., 0.025), hi=quantile(., 0.975)))

```

```{r plot-exp4-predictions-first-third, fig.width=8, fig.height=2, fig.cap="The actual categorization functions observed in this experiment (dots and lines) diverge from the predictions of the simple belief updating model from Experiment 3 (shaded regions), which are based on prior beliefs inferred based on distributional learning in Experiment 1.  Shaded regions show the 95% posterior predictive intervals for the belief updating model, based on prior beliefs inferred from Experiment 1 and exposure distributions from Experiment 2.  Dots and errorbars show mean and 95% bootstrapped CIs for the mean probability of /p/ response over subjects. Note the additional uncertainty in predictions relative to the posterior predictive distribution given Experiment 1's conditions (Figure -@fig:mod-vs-behav-class-funs)."}

predicted_prob_p %>%
  left_join(sepmeans_conds) %>%
  ggplot(aes(x=vot, y=prob_p_lapse_mean, color=vot_cond, fill=vot_cond)) +
  # geom_line() +
  geom_ribbon(aes(ymin=prob_p_lapse_lo, ymax=prob_p_lapse_hi),
              alpha=0.2, color=NA) +
  geom_pointrange(data = sepmeans_test %>%
                    filter(ntile(trial,3) == 1) %>%
                    group_by(vot_cond, vot, subject) %>%
                    summarise(resp_p = mean(respP)),
                  aes(y=resp_p),
                  stat='summary', fun.data=mean_cl_boot) +
  facet_grid(.~vot_cond) +
  labs(x = 'VOT (ms)',
       y = 'Probability /p/ response')

```

Finally, can the simple belief updating model from Experiment 3---using the
prior beliefs inferred from Experiment 1---explain the distributional learning
or lack thereof observed in this experiment?  Figure
-@fig:plot-exp4-predictions-first-third shows the posterior predictive intervals
for the model compared with the average probability of /p/ responses in the first
third of the test phase of Experiment 4.  The model's predictions are quite
accurate for the 10ms /b/, 50ms /p/ condition which is a near replication of the
same exposure condition from Experiment 1.  But for the other conditions, the
model tends to predict *more* distributional learning (larger positive or
negative boundary shifts) than listeners actually show, or in the case of the
two most extreme negative VOT conditions, makes very vague predictions.


## Discussion

On the one hand, the results from Experiment 4 are qualitatively consistent with
the earlier experiments: distributional learning is constrained, and the limited
learning (change in category boundaries) observed here is consistent with the
large differences between the exposure distributions and a typical talker's VOT
distributions.  On the other hand, in Experiments 1 and 2 there was _some_
distributional learning even for the most shifted distributions, whereas in this
experiment exposure to increasingly negative VOTs led to minimal (and
statistically unreliable) changes in listeners' category boundaries.  Likewise,
there was also little difference in category boundaries after exposure to a /p/
mean of 50ms vs. 80ms VOT (with /b/ mean of 10ms VOT).  Moreover, the
quantitative predictions of a Bayesian belief updating model---based on the
pattern of learning observed in Experiment 1---do not match the limited learning
observed here.

There are a number of possible reasons for these discrepancies.  Here, I discuss
two possible explanations: first, differences in the VOT distributions listeners
were exposed to, and second, methodological differences in how boundaries were
assessed.

### Differences in exposure distributions

In this experiment, listeners were exposed to VOT distributions that differed in
important ways from Experiments 1 and 2.  First, they deviated even more
dramatically from the distributions produced by a typical talker of American
English.  In particular, some conditions included large negative VOTs, for which
little distributional learning was observed.  The lack of reliable
distributional learning from negative VOTs is consistent with @Sumner2011, who
also found that negative VOTs failed to produce large shifts in category
boundaries except in a few particular presentation orders.  Moreover, as I
discuss below, the belief updating model makes a basic assumption that there are
exactly two clusters of VOT for word-initial stops, which may prevent it from
effectively predicting the (lack of) distributional learning of negative VOTs.

```{r expt4-error-rates}

max_error_perc <- 
  sepmeans %>%
  filter(!is_test) %>%
  group_by(vot_cond) %>%
  summarise(err = round(100*mean(xor(trueCat == 'p', vot > no_learning_bound)), digits=1)) %>%
  pull(err) %>%
  max() %>%
  glue(., "%")

```

Second, in Experiments 1 and 2 the means of the high and low VOT clusters moved
together in lock-step, while in this experiment they were manipulated
independently.  Varying the means of each cluster together maximizes the
necessity of a boundary shift, since failure to shift the boundary would result
in a large increase in the rate of errors given the implied category assignments
from the clusters.  Exposure distributions where only *one* cluster deviates
from listeners expectations do not *require* as large of a shift in the boundary
between /b/ and /p/ in order to achieve a low recognition error.  In fact, for
all the exposure distributions tested in Experiment 4, continuing to use the
ideal boundary for a "typical talker" would yield low error rates (less than 
`r max_error_perc` 
in all conditions; see Figure -@fig:exposure-dists-exp4).  Thus
the potential benefit of distributional learning was lower in this experiment
than in Experiments 1 and 2.  If listeners are doing boundedly rational belief
updating under some kind of resource constraints, distributional learning might
only lead to changes in behavior when the cost of changing behavior is
outweighed by the benefit of any such change [@Tavoni2019]

### Methodological differences

There are also important differences in the procedure used to assess
distributional learning which could explain the overall weaker distributional
learning in this experiment.  Experiments 1 and 2 did not distinguish between an
exposure and test phase, and measured distributional learning via responses to
trials during the bimodally distributed exposure.  In Experiment 4, the VOT
clusters in the exposure distributions were so widely separated that in many
conditions no trials had VOTs near the expected boundary, and so a separate
post-test phase was necessary to measure listeners' categorization functions.
In this phase, listeners experienced a very different _distribution_ of VOTs
which was flat between -10ms and 50ms.  Thus, it's possible that distributional
learning continued throughout the test phase, undoing some of what listeners had
learned from the earlier bimodal distributions (and indeed there is some
evidence that the small differences between conditions present at the beginning
of the test phase gradually disappear, see Supplementary Materials).

More generally, the bimodal exposure distributions in Experiment 4 provide no
direct evidence about how the range of VOTs during test (-10ms to 50ms VOT)
should be categorized.  The one exception to this is the 10ms /b/, 50ms /p/
condition, which is the only condition that showed a comparable degree of
distributional learning as the earlier experiments.  The evidence provided by
the exposure distributions in any of these experiments may not seem to be
*direct* in the sense that there was no explicit labeling of any of the VOTs as
/b/ or /p/.  Nevertheless, tokens that were encountered during test actually
*occurred* during exposure in Experiments 1 and 2, whereas in this experiment
listeners are hearing many of the VOTs of the test phase for the first time
during the test phase itself.  Thus, they have to extrapolate from their
experience with other, quite different VOTs, and this extrapolation is much more
sensitive to particular underlying assumptions of the speech perception system
(or my model thereof) than the interpolation of Experiments 1 and 2.

# General Discussion

This paper has explored the nature and possible explanations of constraints on
distributional learning in adults.  Experiment 1 shows that distributional
learning of voice onset time (VOT) as a cue to word-initial stop voicing is
constrained: listeners learn some distributions ("accents") better than others,
as measured by the agreement between their actual /b/-/p/ category boundary and
the boundary that is implied by the distributions they are exposed to.
Moreover, listeners show more complete learning for distributions that are
closer to what a typical talker of American English produces, which suggests
that listeners are drawing on their previous experience with other talkers to
guide their learning.  Experiment 2 tests an alternative hypothesis: that these
constraints are really due to the fact that the typical distributional learning
task is _unsupervised_, and that the apparent constraints found in Experiment 1
would go away when some VOTs are _labeled_ according to the intended category.
Surprisingly, adding labeling information to half the trials had no effect on
distributional learning.  Experiment 3 shows that a belief updating
model---where all listeners start from the same prior beliefs---can capture the
pattern of distributional learning (or lack thereof) found in Experiment 1.
Moreover, the model's inferred prior beliefs are similar to the VOT
distributions that a typical talker of English produces.  This suggests that the
constraints on distributional learning observed in Experiment 1 are at least
_consistent_ with belief updating guided by prior experience with other talkers.
In the remainder of this paper, I will discuss the implications of these results
for future work, and the caveats given a final Experiment 4, which
suggests that the constraints on distributional learning of VOT/voicing may be
more complex than revealed in the first three experiments.

## Is distributional learning (Bayesian) belief updating?

The evidence from Experiments 1-3 provides clear support for the idea that
constraints on distributional learning arise from
prior experience with talker variability.  Nearly all listeners use category
boundaries that are a compromise between the optimal boundaries for the VOT
distributions of a typical talker and those of the exposure talker, and a
Bayesian belief-updating model can explain the particular pattern of
distributional learning or lack thereof given prior beliefs that are similar to
the VOT distributions produced by a typical talker of American English.

However, this same model, given prior beliefs that explain distributional
learning in Experiment 1, fails to accurately predict distributional learning to
different VOT distributions in Experiment 4.  Does this mean that the
belief-updating hypothesis is falsified?  In evaluating any computational model,
it is important to keep in mind that successful prediction or generalization
depends not only on suitability of the _theory_ that the model instantiates, but
also on the suitability or approximate correctness of every one of a long chain
of linking hypotheses, computational simplifications, and technical assumptions
that connect the theory to behavior.  If only a single link in this chain is
broken, then the model may not generalize.  In the discussion of Experiment 4
above I discussed some methodological differences between the two experiments
that might explain the observed failure to generalize.  Here, I will discuss
some relevant specific assumptions of the model itself.

### Potential points of failure in the belief updating model

One of the basic assumptions of the belief updating model tested here is that
there are exactly two clusters of VOTs for word initial stops: one corresponding
to the voiced stop /b/ and a second for voiceless /p/.
However,
even though American English has two phonemic voicing _categories_, many talkers
produce three distinct _clusters_ of VOTs, with a single long-lag VOT cluster
for voiceless (aspirated) stops, and _two_ clusters for phonemically voiced stops:
one with a mean around 0ms VOT and low variance (short-lag), and another with a
large negative mean VOT and high variance (pre-voiced) [e.g. @Lisker1964;
@Goldrick2013].  In an ideal listener model with these three VOT clusters, the short-lag
cluster would largely determine the voiced/voiceless category boundary, since
the pre-voiced and short-lag clusters are both phonemically voiced.
When presented with un-labeled VOTs, a fully ideal adapter model would
simultaneously classify VOTs and update beliefs about the underlying
distributions, weighted by how likely each distribution is to explain that
observation [see e.g. @Kleinschmidt2015b eq. 7, p. 155].  Starting from prior
beliefs based on a typical talker of American English, such a model
would likely assign large negative VOTs to the pre-voiced cluster, leading to
minimal change in beliefs about the un-aspirated cluster and hence minimal
changes in categorization.

While this is a conceptually straightforward prediction of the theory that
distributional learning is a form of Bayesian belief-updating, testing this in
an implemented model presents a number of technical challenges.
Not least among them is the fact that the model presented in Experiment 3 is not
a fully ideal adapter which classifies and adapts tokens at the same time, but
rather assumes that the cluster labels of each token are known.  This assumption
massively simplifies the inference in the model, and is reasonable given not
only that the bimodal structure of the exposure distributions is extremely
clear, but also that Experiment 2 found that providing cluster labels to many
tokens led to no change in distributional learning.  But it's unclear what the
cluster labels should be when there are three clusters (two for phonemically
voiced stops), so I leave exploring this possibility for future work.  One
promising direction would be to use sequential Monte Carlo algorithms to
approximate the online inference required to simultaneously classify and adapt
[see e.g. @Kleinschmidt2018a; @Sanborn2010].

The other major simplifying assumption of the belief updating model which may
be suspect is the *form* of the prior beliefs themselves.  Specifically, the model
assumes that the prior beliefs about /b/ and /p/ are _independent_, and that
each distribution has a _conjugate prior_.  @Chodroff2017 found strong
correlations between the mean VOTs for voiceless stops with different places of
articulation across talkers, and weaker but significant correlations between the
means for voiced and voiceless stops (at least for spontaneous speech).
Independent priors for different voiced stops cannot capture the actual
structure of variation across talkers, and thus may not be a good approximation
of listeners' prior beliefs.  Furthermore, the use of a _conjugate_ prior
(again, for computational reasons) and results in a restricted set of possible
relationships between expectations about within- and between-talker variance in
VOT.

### Long-term trajectory of distributional learning

Finally, a more serious problem for the belief updating model used here is the
long-term distributional learning behavior.  Specifically, a straightforward
Bayesian belief-updating model predicts that no matter how strong the prior
beliefs, they can eventually be overcome by observed data.  This is a
consequence of treating every observation as equally informative about the
current distributions.  The empirical evidence on this question is mixed, with
some studies finding that listeners continue to integrate evidence even over
rather dramatic changes in context [e.g. @Theodore2019; @Kraljic2006;
@Kraljic2007], while others find that listeners seem to weight more recent
evidence more strongly, even within the context of a single talker
[@Saltzman2017; @Olasagasti2018].

These findings may not be compatible with the simple _model_ presented here, but
they are compatible with the _theory_ of Bayesian belief-updating.  They simply
correspond to belief updating models which make different assumptions about the
generative process.  If the generative process is such that the distribution
parameters (mean, variance, etc.) vary _across_ contexts (e.g., talkers) but are
stable _within_ talkers, then each token that a talker produces should be
treated as equally informative.  However, if the generative process is instead
one where there is some amount of drift, variability, or change over time even _within_ a
context (talker), then a Bayesian belief updating model would weight more recent
observations more heavily, since according to the assumed generative process
they come from distributions with parameters that are more similar to the
current ones.

The empirical data from these studies is not clear enough about the learning
curves over time to be able to resolve this question unambiguously.  However,
visually inspecting the learning curves predicted by the belief updating model
compared with the (admittedly crude) measurements of the actual learning curves
(Figure -@fig:plot-modeled-category-boundaries) suggests that learning may be
initially more rapid and asymptote more quickly than this particular belief
updating model captures.  If this is in fact the case, then in a belief-updating
framework it would suggest that listeners assume there can be some local
variation within a talker, in addition to global variation across talkers.  More
work that examines the time course of adaptation more carefully is needed to
better understand these dynamics and further constrain models of adaptation.  A
final but important caveat for this work is that the time course and assumed
generative model may be different for different parts of the linguistic
generative model.  The "ideal adapter" framework of @Kleinschmidt2015b [as well
as @Qian2012] points out that the kind of adaptation a system does is itself
something that should be adapted to the amount and kind of variability that's
present in the environment, and if different aspects of the linguistic
generative model have different amounts and kinds of variability, we should
expect adaptation to have different characteristics as well.

## Implications for future work

The first, most general implication of these results is that they suggest a view
where distributional learning operates continuously throughout language
acquisition and adult language use and adaptation.  The idea that statistical
learning continues throughout the lifespan is not a new idea [e.g.,
@Botvinick2004; @Chang2006; @Elman1990a; @Johnson1997; @Goldinger1998;
@Nielsen2008], although it has received more attention of late in the study of
language processing [@Kleinschmidt2015b; @Fine2013a; @Schuster2020;
@Yildirim2016].  Nor is it specific to language [e.g., @Qian2012]: at a basic
level, the sensitivities of perceptual systems are tuned to both the long-run
statistical properties of the environment [e.g., @Ganguli2016; @Simoncelli2001]
and short-run changes in those properties [e.g., @Schweinhart2017;
@Stocker2006].

In this view, acquisition and adaptation are both forms of
distributional learning, with the primary difference between them being the
different amounts and types of prior experience that the system can draw on.
The role of prior experience in constraining distributional learning hints at
another sense in which acquisition and adaptation may be connected by
distributional learning.  In order for prior experience with talker variability
to constrain adaptation in a belief updating model, the listener must have
_learned_ something about the patterns of variability across talkers.  At a
computational level [in the sense of @Marr1982], this may also be a form of
distributional learning.  In order to effectively and efficiently adapt to an unfamiliar
talker's accent, a listener needs to have some reasonably good estimate of the
amount and kind of talker variability they should expect, which is directly
related to the distribution of talkers' accents[^accent] that exist in their environment.
In *this* view---articulated in @Kleinschmidt2015b---not only are adaptation and
acquisition both forms of distributional learning, but the difference between
them is due to continuous distributional learning at another level.  The
experiments reported here are at least consistent with this view: distributions
that diverge more from a typical talker's distributions are harder to learn.
The other existing
evidence for this claim was reviewed in @Kleinschmidt2015b, and I will not
revisit it here in depth.  However, one additional piece of evidence for this possibility
is that learning at the second level (of talker variability) appears to operate
continuously during language acquisition, and in parallel with acquisition of
the linguistic structure [for a recent review, see @Johnson2020].

[^accent]: Using "accent" here as a shorthand for the probabilistic generative
    model that characterizes an individuals use of their language.

This possibility raises two related sets of questions for future research: (1)
what distributions of talkers' accents are *available* for learning, and (2) do
listeners' distributional learning biases and constraints reflect such
meta-distributions (distributions of accents across talkers) that they have
experienced?

### What distributions are available to learn?

The first question can be answered by looking at the amount and structure of the
talker variability that actually exists in the world.  Current attempts at this
have already revealed interesting findings.  For instance, vowel formant
distributions vary little *within* a talker (over the course of a day), but vary
considerably *across* talkers [@Heald2015].  Vowels are more variable across
talkers than voicing of stop consonants, even once variability in vocal tract
length is taken into account [@Kleinschmidt2019].  Moreover, this variability is
in many cases highly structured *within* a talker, across categories
[@Chodroff2017].

When combined with a Bayesian belief updating model like the one employed here,
these findings suggest clear predictions for constraints on distributional
learning during adaptation.  If talkers are consistent in their realization
of vowels, learning should be relatively stable and accumulate evidence over
time.  If vowels are more variable than stop voicing, listeners should
show less constrained, more strongly experience-driven distributional learning
of vowels than stop voicing.  If certain phonemes' distributions are
correlated within talkers, listeners should have a harder time learning
distributions that violate these correlations (e.g., a higher-than average mean
VOT for /p/ but lower-than-average mean VOT for /k/).

Thus far, the studies of variability in production are only scratching the surface
of the possible structures that might exist in talker variability.
Investigating differences across languages and dialects is particularly
pressing.  To what extent are these patterns *universal* across languages,
rather than language- or dialect-specific [e.g., @Schertz2016]?
Language-specific patterns in talker variability must, by definition, be learned
during language acquisition, greatly adding to the burden on the learner.

### What do people know about talker variability?

One of the major challenges in this research program is to probe the implicit,
subjective expectations that listeners bring when adapting to an unfamiliar
talker.  These beliefs are difficult to probe because we can only ever observe
with any amount of directness a listener's beliefs about the *current* context.
These beliefs combine their implicit expectations with their recent experience,
and so any of the measurements we can make in an experiment provide only indirect
evidence about those prior expectations.  Some kind of linking hypothesis is
needed in order to be able to assess these implicit prior expectations.

One possible such linking hypothesis is the kind of belief
updating model presented in Experiment 3.  This sort of model, along with data on the degree and
specific type of distributional learning in different exposure conditions, could
in principle
serve as a kind of "mind reading" device, allowing us to probe listeners'
subjective prior beliefs.  This is a straightforward application
of the principles of Bayesian cognitive modeling: there is some unobservable
cognitive variables (subjective prior beliefs), and Bayesian inference allows us
to infer them based on behavior.  This procedure did yield some meaningful
information (beyond the simple "existence proof" of the suitability of
belief updating as a model for distributional learning in this experiment).  First, the model tells
us something about how *confident* listeners are in their prior beliefs.
Second, the model tells us something about the *content* of those beliefs, the
specific expectations about the mean and variance of the VOT distributions for
/b/ and /p/ that are most consistent with the overall pattern of distributional
learning.

<!-- I'm trying to add soemthing about the input data to the model... -->
<!-- 
However, as the results from Experiment 4 show, the generalizability of the
inference such a model makes are limited by a number of factors.  These include
both fundamental theoretical assumptions
 -->

However, in practice, these inferences are rather more tenuous than we might
like, since as discussed above they depend on all of the various assumptions that the _model_ makes
in linking behavior to the cognitive variables of interest.  Some assumptions
are fundamental to the logical structure of the model, but some are
simplifications made for reasons of mathematical or computational tractability.
For instance, one important assumption was that no information about the actual
underlying distributions of VOTs for /b/ and /p/ (or their variability across
talkers) was built in.  Doing so would be straightforward (as a "hyperprior" in
the belief updating model), but would also mean that the inferred prior beliefs
would be colored by our own assumptions as scientists about what VOT
distributions should look like, negating somewhat the usefulness of this
technique for "mind reading".

The tenuousness of the inferences about listeners prior beliefs is demonstrated
by the model's poor performance when predicting behavior in Experiment 4.  This
was a stringent, but critically important test.  If the model really had, in
fact, recovered listener's prior expectations about what kind of VOT
distributions an unfamiliar talker is likely to produce, then it should be able
to predict distributional learning even to distributions it was not trained on.
This was clearly not the case in Experiment 4, which means that one or more of
the many linking hypotheses connecting this model to the experimental data is
incorrect.  While this calls for some caution in using such a Bayesian
belief-updating model as a mind-reading tool or even as an explanation of
distributional learning, it also provides a number of potential directions for
future work, both computationally (e.g., investigating the consequences of the
various simplifications made here) and empirically [e.g., investigating learning
from even other distributions, with multiple cues, and under varying training
regimes; see @Harmon2019 for work in this direction].

### What is the role of specific language experience?

Finally, the evidence I have presented here is *circumstantial*: I observe
constraints on distributional learning of VOT among American English listeners,
and I have shown that these constraints are largely *consistent* with
belief-updating starting from prior beliefs that look very similar to a typical
talker of American English.  Moreover, the VOT distributions that the American
English listeners struggled to learn---while atypical of American English---are
not outside the range of VOT distributions found across languages [@Cho2019;
@Lisker1964].  However, more work is necessary to show *directly* that it is
previous experience with other talkers that constrains distributional learning.
One step in this direction would be comparing learning of the same distributions
by listeners with differing language experiences, provided that that experience
differs enough in the relevant cue distributions.  Another important step would
be showing that specific experience or specific prior expectations can further
constrain distributional learning, although this may be difficult since the
strong constraints exhibited in these experiments likely reflect language-level
priors which cannot be easily or quickly changed [@Kleinschmidt2015b]

# Conclusion

One of the major discoveries of the last two decades of research in speech
processing is that adults maintain a surprising amount of flexibility, and
quickly adapt to unusual variants of sounds in their language.  Moreover, this
rapid _adaptation_ may correspond to the continued operation of the same kind of
distributional learning mechanisms which have been implicated in language
_acquisition_ early in life [@Kleinschmidt2015b].  In this paper, I have
explored some constraints on the ability of distributional learning in adults,
finding that adult distributional learning _is_ constrained, these constraints
do not appear to arise from the unsupervised nature of distributional learning,
but rather are consistent with belief updating guided by a listener's prior
experience with talker variability.

# Acknowledgments

This work was begun during my PhD studies at the University of Rochester and an
early version formed one chapter of my dissertation.  In addition, some of the
data reported here was presented at the Annual Meeting of the Cognitive Science
Society (2015 and 2016).  I gratefully acknowledge my PhD advisor (Florian
Jaeger), my committee (Dick Aslin, Jeff Runner, and Lori Holt), and Rajeev
Raizada for their suggestions and careful critiques in the early phases of
developing this work.

```{r}
packages = c(
  'base',
  'knitr',
  'rmarkdown',
  'tidyverse',
  'dplyr',
  'glue',
  'magrittr',
  'forcats',
  'rstan',
  # 'brms',   # JSS cited already
  'loo',
  'tidybayes',
  'ggplot2',
  'cowplot',
  'latex2exp'
)

knitr::write_bib(packages, file="packages.bib")
citation_strings <- glue("`{pkg}` [@R-{pkg}]", pkg=packages[-1])
citation_string <- str_c(lift(str_c)(head(citation_strings, -1), sep=", "), ", and ", tail(citation_strings, 1))

```

I am also indebted to the developers and maintainers of the R language
[@R-base], the Stan language [@Carpenter2017], and the following R
packages: `brms` [@Burkner2017], `r citation_string`.

This work was supported by an NSF GRFP and NIH NICHD F31 (HD082893) to DFK and
NIH NICHD R01 (HD075797) to TFJ.

