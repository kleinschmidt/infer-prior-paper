---
Title: What constrains distributional learning in adults?
Author: Dave Kleinschmidt
bibliography: /home/dave/Documents/papers/zotero.bib
output:
    html_document:
        code_folding: hide
        dev: png
        fig_retina: 3
        keep_md: true
        md_extensions: +implicit_figures
        pandoc_args:
        - --filter
        - pandoc-crossref
    pdf_document:
        md_extensions: +implicit_figures
        keep_tex: true
        pandoc_args:
        - --filter
        - pandoc-crossref
---


```{r knitr-setup, warning=FALSE, message=FALSE, error=FALSE, echo=FALSE, results='hide'}

library(knitr)
opts_chunk$set(warning = FALSE,
               message = FALSE,
               error = FALSE,
               cache=TRUE,
               results = "hide",
               echo=opts_knit$get("rmarkdown.pandoc.to") != 'latex')

options(digits=2)

## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})
## Produce markdown-formatted table captions with anchors for cross-refs.
## Requires pandoc-tablenos to parse the IDs. Refer to tables
## in text with {@tbl:label} or @tbl:label.
## Based partly on http://stackoverflow.com/a/18672268
##
## (see https://github.com/tomduck/pandoc-tablenos)
knit_hooks$set(tbl.cap = function(before, options, envir) {
  if(!before){
    paste0('\n\nTable: ', options$tbl.cap,
           ' {#tbl:', options$label, '}', sep = '')
  }
})

```

```{r preamble, cache=FALSE}

library(tidyverse)
library(beliefupdatr)
library(supunsup)

library(rstan)
library(brms)
library(tidybayes)

library(cowplot)
theme_set(cowplot::theme_cowplot() +
            theme(plot.title=element_text(hjust=0)))
library(latex2exp)

## devtools::install_github('kleinschmidt/daver')
library(daver)
## devtools::install_github('kleinschmidt/phonetic-sup-unsup')
library(supunsup)
## devtools::install_github('kleinschmidt/beliefupdatr')
library(beliefupdatr)

```



# Introduction

A basic fact of human language is that any typically developing human
infant can learn any human language. Human languages vary dramatically
at every level, including the basic sound systems they use, and the
human language faculty must be flexible enough to deal with this
substantial cross-linguistic variability. The first stages of language
acquistion are characterized by initial flexibility, which declines over
development as the particulars of the native language are acquired. For
instance, as infants become better at discriminating linguistically
important sounds in their native language, they simultaneously *lose*
the ability to discriminate sounds that are important for other
languages but not their native language
[@Best1995; @Kuhl1992; @Werker1984]. Ultimately, people become
sufficiently inflexible over development that they generally struggle to
learn another language in adulthood [@Hartshorne2018].

However, adult listeners still need to deal with substantial variability
*within* their native language, as talkers differ in how they realize
the phonetic categories of the language using acoustic cues [e.g.
@Allen2003; @Newman2001; @Clopper2005]. Accordingly, adult listeners
flexibly adapt to unfamiliar talkers in a wide variety of contexts. At
one extreme, perception of heavily accented non-native talkers becomes
faster and more accurate with just a few minutes of exposure
[@Clarke-Davidson2004; @Bradlow2008; @Baese-berk2013]. At the other
extreme, listeners recalibrate representations of individual phonetic
categories based on subtle changes in single segments in otherwise
unaccented talkers [@Kraljic2006; @Norris2003; @Bertelson2003].

Both acquisition and adaptation have been theorized to be forms of
distributional learning. First, computational modeling shows that both
acquisition [@McMurray2009; @Vallabha2007; @Feldman2013; but see
@Hitczenko2018] and rapid adaptation [@Kleinschmidt2015b] can be treated
as forms of distributional learning. At some level, acquisition simply
*is* a problem of distributional learning, in the sense that
<!-- ((( -->
<!-- computational level analysis?? it IS a problem of distributional -->
<!-- learning ))) -->

Second, both adults and infants are sensitive to distributional
properties of speech. One set of findings shows that listeners (both
infants and adults) become more sensitive after exposure to a bimodal
distribution of an acoustic cue (like length, voice-onset time, vowel
formant frequencies, etc.) compared with exposure to a unimodal
distribution [e.g. @Escudero2011; @Goudbeek2008;
@Maye2000; @Maye2002; @Feldman2013b]. Another set of findings shows that
adult listeners can adapt to changes in the means and/or variances of
the cue distributions for known phonetic categories [e.g.,
@Clayards2008; @Theodore2015; @Theodore2019; @Colby2018; @Chladkova2017].
What both of these sets of findings have in common is that listeners
pick up on the distributions of cues without any explicit instruction
about the itended category label associated with each token. For
example, @Clayards2008 had listeners listen to /b/-/p/ minimal
pair words (e.g., "beach/peach") with different voice-onset times
(VOT), and click on a matching picture to indicate which member of the
minimal pair they heard. On every trial, the VOT was drawn from one of
two bimodal distributions, which had clusters with the same means but
different variances across subjects. Listeners in the high-variance
condition produced shallower categorization functions, reflecting
greater uncertainty associated with the wider range of VOTs they heard
for each cluster.

If both acquisition and adaptation can be treated as forms of
distributional learning, and both infants and adults are sensitive to
distributional information, what distinguishes acquisition from
adaptation? For one, it seems that distributional learning in adults is
*constrained*. Adult listeners struggle to learn new categories that are
not present in their native language. For instance, Japanese listeners
struggle to discriminate the English /r/-/l/ contrast, which corresponds
a single category in their native language [@Goto1971; @Miyawaki1975].
Long-term naturalistic exposure is not sufficient to achieve good
discrimination of this contrast, even after convserational competence
has been achieved [@Takagi1995]. While perception of this contrast can
be improved somewhat by training, it requries extensive training and
these listeners seldom achieve native-like levels of performance
[@Bradlow1997].

There are also apparent constraints on the ability of adult listeners to
adapt to variations in the distributions associated with native language
categories. For instance, @Idemaru2011 tested how well listeners adapt
to distributions of two cues that distinguish voicing (e.g., /b/ vs.
/p/), voice onset time (VOT, the primary cue to voicing) and the pitch
of the following vowel (f0, a secondary cue). These two cues are
typically positively correlated in English, with /p/ corresponding to
high VOT and high f0, and /b/ to low values of both cues [@Kohler1982].
In one condition, listeners were exposed to a talker who produced a
positively correlated distribution of these cues. During a post-test,
these listeners used f0 to categorize stops with ambiguous VOTs. In
another condition, listeners heard a talker who produced an
*un*correlated distribution, where f0 is uninformative. In contrast to
the listeners in the first condition, during the post test these
listeners _ignored_ f0 even for ambiguous VOTs. This effect
is consistent with the idea that listeners are rationally integrating
multiple cues to voicing, weighing them based on how informative they
are [@Ernst2004; @Clayards2008; @Bejjanki2011]. However, listeners in a
third condition who were exposed to a talker who produced an
*anti*correlated distribution did _not_ follow the
predictions of rational cue integration. Despite the fact that f0 was
just as informative for this accent as for the positively correlated
accent, listeners _ignored_ f0 as a cue to voicing. This
suggests that these listeners have ruled out the possibility of a
reversed mapping between f0 and voicing (/b/ vs. /p/), possibly perhaps
American English talkers typically do not typically produce it [e.g.,
@House1953]. Likewise, @Sumner2011 found that listeners had trouble
adapting to a talker who produced VOT distributions for /b/ and /p/ that
had _substantially_ lower means (approximately -60ms and
0ms, respectively) than a typical talker [approximately 0--10ms and 60ms
VOT; @Lisker1964].

So on the one hand, distributional learning provides a unifying
theoretical perspective on flexibility in language acquisition and
adaptation. On the other hand, it highlights an important difference
between these two basic kinds of plasticity in the language system. From
the lens of distributional learning, one major difference between
acquisition and adaptation is that distributional learning in adulthood
appears to be *constrained*, while it is relatively *unconstrained*
during acquisition.

However, we lack a clear understanding of the nature and source of those
constraints. There are a number of other differences between the
learning problems posed by acquisition and adaptation, even if both are
forms of distributional learning. First, distributional learning in
infancy is, at least initially, almost entirely unsupervised, meaning
that there is very little information about whether any two observed
acoustic cue values come from the same cluster (category) or different
ones. Adults have a great deal of circumstantial evidence from the
lexicon, pragmatic context, phonotactics, etc. which provides *some*
information about the intended category for a particular cue value. This
makes the distributional problem of adaptation at least semi-supervised.

Second, when adapting to an unfamiliar talker, adults have a great deal
of prior experience with *other* talkers which they could use to narrow
down the possible distributions they ought to expect
[@Kleinschmidt2015b]. Both of these factors might contribute to
constraints on adult distributional learning. For the first, if adult
adaptation typically operates in a *supervised* setting, the fully
unsupervised setting of a typical distributional learning experiment
might not provide enough information, leading to reduced learning. For
the second, if the distributions encountered in an experiment fall far
enough outside the range of what a listener expects based on their prior
experience, they may struggle to adapt [@Kleinschmidt2015b].

The goal of this paper is to systematically probe the constraints on
distributional learning in American English-speaking adults. Experiment
1 tests the ability of American English listeners to change their
classification of word initial stop voicing based on experience with a
range of distributions of voice-onset time (VOT). I find that
distributional learning is more complete when the experimental
distributions are more similar to those of a typical American English
talker, suggesting that prior experience with other talkers may
constrain distributional learning. Experiment 2 tests another possible
constraint on distributional learning, which is the absence of *labels*,
which could lead to uncertainty about whether the bimodal distribution
really corresponds to the standard English categories of voiced and
voiceless stops of /b/ and /p/. Surprisingly, telling listeners whether
a particular VOT was intended to be a voiced /b/ or a voiceless /p/ on
half of the trials has no effect on the speed or completeness of
distributional learning. Experiment 3 uses a Bayesian belief-updating
model to test whether the constraints observed in Experiment 1 can be
explained as belief updating starting from a common set of prior beliefs
that is shared by all of the subjects.

Together, these results show that distributional learning in adults *is*
constrained, and these constraints are at least consistent with belief
updating starting from a set of prior beliefs about the VOT
distributions that a typical talker of American English will produce.

# Experiment 1: Distributional learning of voicing from VOT

In order to probe the possible constraints on distributional learning,
Experiment 1 measures listener's distributional learning of a range of bimodal
distributions of voice-onset time (VOT).  These distributions vary in how
similar they are to a typical American English talker.

```{r data-exp1}

data_exp1 <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(trueCat = respCategory,
         subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

conditions_exp1 <-
  data_exp1 %>%
  group_by(bvotCond, trueCat) %>%
  summarise(mean_vot = mean(vot)) %>%
  spread(trueCat, mean_vot) %>%
  transmute(vot_cond = paste(b, p, sep=', '),
            ideal_boundary = (b+p)/2) %>%
  ungroup() %>%
  mutate(vot_cond = factor(vot_cond, levels=vot_cond))

data_exp1 <- left_join(data_exp1, conditions_exp1, by="bvotCond")

```

```{r vot-dists-exp1, fig.width=10, fig.height=3, fig.cap="Each subject heard one of these five synthetic accents, which differ only in the distribution of VOTs of the word-initial stops. Black dashed lines show VOT distributions from a hypothetical typical talker [as estimated by @Kronrod2016]. Note that the 0 and 10ms shifted accents are reasonably close to this typical talker, while the -10, 20, and 30ms shifted accents deviate substantially."}

prior_stats_by_talker <-
  votcorpora::vot %>%
  filter(source %in% c('gva13', 'bbg09', 'buckeye'),
         place == 'lab') %>%
  mutate(source = ifelse(source %in% c('gva13', 'bbg09'),
                         'goldricketal',
                         source)) %>%
  group_by(source, prevoiced, subject, phoneme) %>%
  summarise(mu = mean(vot),
            sigma2 = var(vot),
            sigma = sd(vot),
            n = n()) %>%
  rename(category = phoneme)

prior_stats <-
  prior_stats_by_talker %>%
  filter(source == 'goldricketal', category == 'b') %>%
  group_by(source, prevoiced, category) %>%
  summarise_at(vars(mu, sigma2, sigma, n), funs(mean, var, sum)) %>%
  transmute(category,
            mean = mu_mean,
            sd = sqrt(sigma2_mean),
            n = n_sum) %>%
  bind_rows(supunsup::prior_stats %>%
              filter(source=='kronrod2012') %>%
              mutate(n = 1))

prior_lhood <- 
  prior_stats %>%
  filter(source == 'kronrod2012') %>%
  supunsup::stats_to_lhood()

prior_class <- prior_lhood %>% lhood_to_classification()


exposure_stats <- data_exp1 %>%
  group_by(vot_cond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sd_noise = sqrt(82)

exposure_lhood <- exposure_stats %>%
  group_by(vot_cond) %>%
  do(supunsup::stats_to_lhood(., sd_noise))

data_exp1 %>%
  group_by(vot_cond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=vot_cond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  geom_text(data=data.frame(vot_cond='-10, 30'), x = 10, y = 60,
            label = 'Typical Talker',
            color='black', hjust=0, vjust=0.3, size=3) +
  geom_text(data=data.frame(vot_cond='-10, 30'), x = 40, y = 50,
            label = 'Exposure\nTalker',
            hjust=0, vjust=0.8, size=3,
            lineheight=1) + 
  facet_grid(.~vot_cond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  labs(title="Experiment 1: VOT distribution conditions",
       subtitle="Similarity of exposure talker to typical talker controlled by shifting means of /b/ and /p/ distributions")
  ## scale_fill_discrete('/b/, /p/\nmean VOT') ## +
  ## theme(legend.position='none')

```

## Methods

### Participants {#sec:participants}

```{r participants-exp1}

n_subj <- data_exp1 %>% group_by(subject) %>% summarise() %>% tally()

excluded <- supunsup::supunsup_excluded %>%
  filter(supCond == 'unsupervised') %>%
  group_by(subject, bvotCond) %>% 
  summarise() %>% 
  left_join(supunsup::excludes, by="subject") %>%
  select(subject, bvotCond, exclude80PercentAcc, rank)

n_excluded <- nrow(excluded)
n_subj_repeat <- sum(!is.na(excluded$rank))
n_subj_bad <- sum(!is.na(excluded$exclude80PercentAcc))
n_both <- n_subj_repeat + n_subj_bad - n_excluded 

n_total <- n_subj + n_excluded

n_excl_cond <- excluded %>%
  group_by(bvotCond) %>%
  tally() %>%
  arrange(n)

```

`r n_total` participants were recruited via Amazon's Mechanical Turk. Participants were
paid \$2.00 for participation, which took about 20 minutes. We excluded participants
who participated more than once ($n=`r n_subj_repeat`$) or who failed to
classify VOTs reliably ($n=`r n_subj_bad`$; $n=`r n_both`$ for both reasons).
We defined reliable classification as accuracy of at least 80% at 0 and 70ms
VOT. Because some conditions had few stimuli with these VOTs, we extrapolated
participants' responses using a logistic generalized linear model (GLM).  Excluded
participants were roughly equally distributed across conditions (maximum of 
`r last(n_excl_cond$n)` in `r last(n_excl_cond$bvotCond)`ms /b/ VOT condition,
and minimum of `r first(n_excl_cond$n)` in `r first(n_excl_cond$bvotCond)`ms /b/
VOT condition). After these exclusions, data from `r n_subj` participants remained
for analysis.

### Procedure

![Example trial display (beach/peach). Listeners first click on the
    green button to play the word, then click on one picture to indicate what
    they heard.](figure_manual/beach_peach.png){#fig:beach-peach}

The procedure is based on @Clayards2008. Figure -@fig:beach-peach shows an
example trial display.  On each trial, two response option images appeared,
which corresponded to one of three /b/-/p/ minimal pairs (beach/peach,
bees/peas, or beak/peak).  Participants started each trial by clicking on a button
between the two pictures, which played the corresponding minimal pair word audio
stimulus. Participants then clicked on the picture to indicate whether they heard
the /b/ or /p/ member of the minimal pair. Participants performed 222 of these
trials, evenly divided between the three minimal pairs, in random order.

Each trial's word was synthesized with a voice onset time (VOT) that was
randomly drawn from a bimodal distribution, with low and high VOT clusters
implicitly corresponding to /b/ and /p/, respectively. This distribution defined
the *accent* that the subject heard, and each subject was pseudorandomly
assigned to one of five accent conditions ([@Fig:vot-dists-exp1]).

### Materials

The audio and visual stimuli we used were identical to those in
@Clayards2008. Three /b/-/p/ minimal pair audio continua were synthesized using
the 1988 Klatt synthesizer [@Klatt1980], by manipulating VOT in 10ms increments
(either adding voicing before the stop burst to create negative VOTs, or
aspiration after for positive VOTs). Within a /b/-/p/ continuum, the other
parameters were held constant, and modeled on natural tokens of the endpoints
(beach/peach, bees/peas, and beak/peak).

## Results

In order to assess distributional learning, each subject's classification
function is compared to two baselines, corresponding to _no learning_ and
_complete learning_.  The _no learning_ baseline is derived from the category
boundary for a _typical talker_'s VOT distributions [based on @Kronrod2016], and
the category boundary based solely on the exposure talker's distributions.  I
refer to the typical talker's boundary as the "no learning" baseline, and the
exposure talker's boundary as the "complete learning" baseline.

Each subject's classification function was estimated with a Bayesian multilevel
logistic regression, using the `brms` package [@Burkner2017] for `R`
[@RCoreTeam2017].  This approach simultaneously estimates the group-level
effects of the experimental manipulation (VOT shift condition) on the /b/-/p/
category boundary location and sharpness, and each individual participants' boundary
locations and slopes.  A multilevel approach like this has the benefit of
properly accounting for and balancing the joint uncertainty about the group- and
individual-level effects [@Gelman2007].[^shrinkage]

[^shrinkage]: One effect of using a multilevel approach is that the estimate of
    each subject's boundary is "shrunk" towards the group-level estimate.  This
    reflects the assumption that participants are drawn from the same population,
    and hence the data from one subject in a group are informative about other
    participants in that group (and vice versa).  The amount of this shrinkage
    depends on the amount of data available from each individual subject, and
    the consistency of the individual subject estimates.  In this case, each
    subject contributes sufficient data to make shrinkage mild, and an
    alternative analysis that estimates each participants' boundary with a separate
    logistic GLM produces qualitatively similar effects.

```{r exp1-regression, dependson=c("data-exp1")}

data_exp1_mod <-
  data_exp1 %>%
  select(subject, bvotCond, trial, vot, respP) %>%
  mutate(vot_s = (vot - mean(vot)) / sd(vot),
         trial_s = (trial - mean(trial)) / sd(trial))

# b_logit_exp1 <- brm(respP ~ 1 + bvotCond * vot_s * trial_s + (1 + vot_s | subject),
#                     data=data_exp1_mod, family=bernoulli(), chains=4, iter=1000)
# saveRDS(b_logit_exp1, "models/brm_logistic_exp1.rds")
b_logit_exp1 <- readRDS("models/brm_logistic_exp1.rds")

# extract fitted classification functions:
# first get trials from first, second, and third thirds
exp1_blocks <-
  data_exp1_mod %>%
  group_by(block=ntile(trial, 3)) %>%
  summarise_at(vars(trial, trial_s), mean)

# overall fit (fixed effects):
data_pred <-
  cross_df(list(vot_s = seq(min(data_exp1_mod$vot_s),
                            max(data_exp1_mod$vot_s),
                            length.out=100),
                bvotCond = unique(data_exp1_mod$bvotCond),
                block = 1:3)) %>%
  left_join(exp1_blocks, by="block") %>%
  mutate(vot = vot_s * sd(data_exp1_mod$vot) + mean(data_exp1_mod$vot)) %>%
  left_join(conditions_exp1, by="bvotCond")

expt1_class <-
  fitted(b_logit_exp1, newdata=data_pred, re_formula=NA) %>%
  as_tibble() %>%
  bind_cols(data_pred)

# by-subject fit (fixed+random effects):
data_pred_subj <-
  data_exp1_mod %>%
  group_by(subject, bvotCond) %>%
  summarise() %>%
  left_join(data_pred, by="bvotCond")

expt1_class_bysub <-
  fitted(b_logit_exp1, newdata=data_pred_subj) %>%
  as_tibble() %>%
  bind_cols(data_pred_subj)

# get x-intercept (category boundary):
#' Find x intercept with linear interpolation of two nearest points
#'
#' Respects grouping
#'
#' @param d a tibble
#' @param x (unquoted) column with the x coordinates
#' @param y (unquoted) column with the y coordinates
#' @param ytarget (default=0.5) y value to find x intercept at
find_bound <- function(d, x, y, ytarget=0.5) {
  x <- enquo(x)
  x_name <- quo_name(x)
  y <- enquo(y)
  d %>%
    arrange((!!y - ytarget)^2) %>%
    filter(row_number() <= 2) %>%
    summarise(!!x_name := (ytarget - first(!!y)) * diff(!!x)/diff(!!y) + first(!!x))
}


expt1_bounds <-
  expt1_class %>%
  group_by(vot_cond, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt1_bounds_bysub <-
  expt1_class_bysub %>%
  group_by(vot_cond, subject, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt1_bounds_fixef_samples <-
  tidybayes::linpred_draws(b_logit_exp1, data_pred, re_formula=NA) %>%
  group_by(vot_cond, block, trial, .draw) %>%
  find_bound(x=vot, y=.value)

```

```{r exp1-results, fig.width=10, fig.height=3, fig.cap="Results from experiment 1 show that distributional learning is incomplete for large shifts of VOT distributions from a typical talker's distributions"}

perfect_learning <- exposure_stats %>%
  group_by(vot_cond) %>%
  do(stats_to_lhood(.)) %>%
  lhood_to_classification()

no_learning <- prior_lhood %>%
  lhood_to_classification()

vot_limits <- data_exp1 %>% pull(vot) %>% range()

ggplot(expt1_class_bysub, aes(x=vot, color=vot_cond)) +
  geom_line(data=.%>%filter(block==3), aes(y=Estimate, group=subject), alpha=0.2) +
  facet_grid(.~vot_cond) +
  geom_line(data=perfect_learning, aes(y=prob_p), group=1, linetype="33", size=1,
            show.legend=FALSE) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype="99", color='black',
            show.legend=FALSE) +
  geom_point(data=expt1_bounds %>% filter(block==3),
             y=0.5) +
  labs(x="VOT (ms)",
       y="Probability /p/ response",
       title="Experiment 1: Classification functions",
       subtitle="Fitted with mixed effects logistic regression, predictions for final third of trials") +
  lims(x = c(-10, 70))

```

```{r expt1-boundaries, fig.cap="Category boundaries by condition.  Violins show the distribution of participants' boundaries (random effects), and the white points show the group-level boundary (fixed effects, with 95% Bayesian confidence intervals)"}

no_learning_bound <-
  no_learning %>%
  arrange((prob_p - 0.5)^2) %>%
  head(1) %>%
  pull(vot)

perfect_learning_bound <-
  perfect_learning %>%
  group_by(vot_cond) %>%
  arrange((prob_p - 0.5)^2) %>%
  filter(row_number() == 1)

# estimate uncertainty of fixed effects boundaries:
expt1_bounds_fixef <-
  expt1_bounds_fixef_samples %>%
  group_by(vot_cond, block, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))

expt1_bounds_bysub %>%
  filter(block==3) %>%
  ggplot(aes(x=vot_cond, y=vot, fill=vot_cond, color=vot_cond)) +
  geom_violin(alpha=0.5, color=NA) +
  ## geom_beeswarm(cex=2) +
  geom_pointrange(data = expt1_bounds_fixef %>% filter(block==3),
                  aes(y=mean, ymin=low, ymax=high),
                  color="white", show.legend=FALSE) +
  geom_hline(yintercept=no_learning_bound, color="black", linetype=2) +
  geom_segment(aes(x=as.numeric(vot_cond)-.5, # exposure boundaries_exp1
                   xend=as.numeric(vot_cond)+.5,
                   y=vot,
                   yend=vot,
                   color=vot_cond),
               linetype = 2, size = 1,
               data = perfect_learning_bound) +
  geom_text(data=perfect_learning_bound %>% filter(vot_cond=="20, 60"),
            aes(y=vot, color=vot_cond),
            ## x=3.5, y=41, 
            label="Complete learning\n(Exposure talker)",
            hjust = 0, vjust = 0,
            nudge_x = -0.5, nudge_y = 1) +
  geom_text(data=perfect_learning_bound %>% filter(vot_cond=="20, 60"),
            aes(color=vot_cond, x = as.numeric(vot_cond)-0.5),
            y=no_learning_bound - 1, color='black',
            ## x=3.5, y=41, 
            label="No learning\n(Typical talker)",
            hjust = 1, vjust = 0,
            ) +
  labs(title="Experiment 1: Category boundaries",
       subtitle="Compared with no learning and complete learning",
       y = "Boundary VOT (ms)",
       x = "Exposure condition (/b/, /p/ mean VOT)") +
  theme(legend.position="none") +
  coord_flip()

```

```{r bound-diff-p-vals}

# compute Bayesian p-values for differences between boundaries between
# neighboring VOT conditions.

expt1_bounds_fixef_diffs <-
  expt1_bounds_fixef_samples %>%
  group_by(.draw, block) %>%
  arrange(vot_cond) %>%
  mutate(vot_cond_next = lead(vot_cond),
         bound_diff = vot - lead(vot))

expt1_bounds_fixef_diff_stats <-
  expt1_bounds_fixef_diffs %>%
  group_by(block, vot_cond, vot_cond_next) %>%
  filter(!is.na(vot_cond_next)) %>%
  summarise_at(vars(bound_diff),
               funs(mean,
                    low=quantile(., 0.025),
                    high=quantile(., 0.975),
                    p=mean(.>0)))

ex1_pval_str <-
  expt1_bounds_fixef_diff_stats %>%
  filter(block == 3) %>%
  pull(p) %>%
  max() %>%
  daver::p_val_to_less_than()

```

```{r bound-undershoot}

# summaries of boundary undershoot
expt1_boundary_undershoot <-
  perfect_learning_bound %>%
  select(vot_cond, ideal_bound=vot) %>%
  right_join(expt1_bounds_fixef, by="vot_cond") %>%
  filter(block == 3) %>%
  mutate_at(vars(low, high, mean),
            funs(abs=(.-ideal_bound)*sign(no_learning_bound-ideal_bound),
                 perc=round(100 *
                              (.-no_learning_bound) /
                              (ideal_bound-no_learning_bound)))) %>%
  select(-ideal_bound, block, trial)
  

```


The first question is whether listeners learned anything at all from exposure to
these VOT distributions.  One way to answer this is to look at the _category
boundary_---the VOT that is ambiguous between /b/ and /p/---corresponding to the
fixed effects for each exposure condition.  Because trial number was inluded as
a regressor, trial was fixed at 184 (83% or 5/6 of the 222 total trials).  The
large white points in Figure [-@fig:expt1-boundaries] show the estimated
boundary for each condition, and the corresponding confidence intervals are the
95% posterior intervals (e.g., the 95% quantiles of the posterior samples).
Visual inspection shows that the group-level posterior distributions of the
boundaries are almost entirely non-overlapping with their
most-similar/neighboring conditions (all $`r ex1_pval_str`$[^bayes_p]), even though the
_subject_-level distributions do overlap (as shown by the colored violins).

[^bayes_p]: The $p$ values reported here are Bayesian $p$ values, or the
    posterior probability that the relevant statement holds conditional on the
    data and model.  These are determined using _samples_ from the posterior,
    by computing the proportion of those samples where the statement is true.
    In this case, the relevant condition is whether the category boundary for
    one condition is reliably less than the boundary for the condition with a
    next highest mean VOTs.

The second question is how _completely_ listeners learned the distributions they
were exposed to.  Visual inspections of the categorization functions
[@fig:exp1-results] and the category boundaries [@fig:expt1-boundaries] suggests
that listeners category boundaries (at 83% through the exposure) do _not_
perfectly correspond to the ideal category boundary for the VOT distributions
they were exposed to (thick dashed colored lines).  Specifically, listeners seem
to _undershoot_ the ideal category boundary, and their categorizations functions
lie between the ideal boundary for the exposure talker and the boundary
corresponding to a typical talker of American English (thin black colored
lines).

The undershoot is largest in absolute terms for the most shifted distributions,
with 

## Discussion

These results indicate two things.  First, like many other studies
[e.g. @Chladkova2017; @Clayards2008; @Theodore2015; @Theodore2019; @Colby2018],
I found evidence for distributional learning: listeners _do_ change how they
categorize sounds as voiced or unvoiced due to exposure to different VOT
distributions.  Second, this learning is not _complete_, in the sense that
listeners' voiced-unvoiced boundary does not exactly match the boundary implied
by the bimodal distribution they were exposed to.  Like the findings of
@Sumner2011 and @Idemaru2011, this second result suggests that distributional
learning is _constrained_ somehow.  In the following experiments, I investigate
two possible sources of these constraints.

```{r prop-in-wrong-cat}

data_exp1 %>%
  group_by(vot_cond, trueCat) %>%
  summarise(wrong=mean(ifelse(trueCat == 'b',
                              vot > no_learning_bound,
                              vot < no_learning_bound)))

```


First, it is possible that the primary constraint here is that listeners are
being asked to do _unsupervised_ distributional learning.  That is, they are
free to interpret each VOT they hear as either a /b/ or a /p/, since both
options are available for a response.  For distributions that are highly
shifted, most tokens will fall into one of the two pre-existing distributions
(see @Fig:vot-dists-exp1): in the condition with the highest positive shift
(30ms /b/, 70ms /p/), 73% of all tokens from the lower cluster would be
classified as /p/ according to a typical talker's distributions.  Thus, what I
intended listeners to treat as two separate clusters of VOTs---one for /b/ and
one for /p/---they may instead have interpreted as one large cluster in extreme
conditions, lacking any labels to tell them otherwise.  I address this
possibility in Experiment 2, by providing labels on a portion of trials.

A second possibility is that listeners are constrained by their prior experience
with other talkers.  The fact that listeners' category boundaries tend to lie
between those of a typical talker and the experimental exposure talker suggests
that this is not implausible.  To assess this possibility more rigorously, after
Experiment 2 I use a Bayesian belief-updating model of distributional learning
to investigate whether the pattern of distributional learning across conditions
is consistent with belief updating starting from common prior beliefs.

# Experiment 2: Semi-supervised distributional learning

In this experiment, some trials are presented with _label_ information, which
indicates _which_ member of the /b/-/p/ minimal pair the speaker intended to
produce.

## Methods

```{r exp2-data, dependson=c()}

data_exp2 <-
  supunsup::supunsup_clean %>%
  filter(supCond %in% c("supervised", "mixed")) %>%
  mutate(trueCat = respCategory,
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

conditions_exp2 <-
  data_exp2 %>%
  group_by(bvotCond, supCond, trueCat) %>%
  summarise(mean_vot = mean(vot)) %>%
  spread(trueCat, mean_vot) %>%
  transmute(vot_cond = paste(b, p, sep=', '),
            ideal_boundary = (b+p)/2) %>%
  ungroup() %>%
  mutate(vot_cond = factor(vot_cond, levels=levels(conditions_exp1$vot_cond)),
         supervised = fct_recode(supCond, supervised="mixed"))

n_subj_exp2 <- length(unique(data_exp2$subject))

excluded_exp2 <- supunsup::supunsup_excluded %>%
  inner_join(conditions_exp2) %>%
  group_by(subject, supCond, bvotCond, vot_cond) %>% 
  summarise() %>% 
  left_join(supunsup::excludes, by="subject") %>%
  select(subject, bvotCond, exclude80PercentAcc, rank)

n_excluded_2 <- nrow(excluded_exp2)
n_subj_repeat_2 <- sum(!is.na(excluded_exp2$rank))
n_subj_bad_2 <- sum(!is.na(excluded_exp2$exclude80PercentAcc))
n_both_2 <- n_subj_repeat_2 + n_subj_bad_2 - n_excluded_2 

n_total_2 <- n_subj_exp2 + n_excluded_2

n_excl_cond_2 <- excluded_exp2 %>%
  group_by(bvotCond) %>%
  tally() %>%
  arrange(n)

```

### Procedure

```{r exp2-label-dists, fig.width=9, fig.height=4.5, fig.cap='Distribution of labeled and unlabeled trials in Experiment 2.  These conditions have the same number of labeled and unlabeled trials, but differ in how they are assigned to VOTs.  In the "mixed" condition, most VOTs occurred in a mixture of labeled and un-labeled trials.  In the "separated" condition, each VOT always occurred as either labeled or unlabeled.  Because no difference was found between learning in these conditions, they are analyzed together.'}

data_exp2 %>%
  mutate(supCond = fct_recode(supCond, separated="supervised")) %>%
  group_by(supCond) %>%
  filter(bvotCond == 0) %>%
  filter(subject == first(subject)) %>%
  group_by(supCond, labeled, vot) %>%
  tally() %>% 
  ggplot(aes(x=vot, y=n, fill=labeled)) +
  geom_bar(stat="identity") +
  facet_grid(.~supCond) +
  scale_fill_manual(values=c("black", "gray")) +
  scale_x_continuous(breaks = c(0, 40), labels = c("/b/ mean", "/p/ mean")) +
  labs(x = "",
       y = "Count",
       title = "Experiment 2: semi-supervised learning",
       subtitle = "Distribution of labeled trials in two semi-supervised learning conditions")

```

The procedure was identical to that of Experiment 1, with one change.  Half of
the trials were _unlabeled_ as in Experiment 1: the response options were the
/b/ and /p/ ends of the continuum for that trial's word were present (e.g.,
"beach" and "peach").  The other half the trials were _labeled_: if the word was
drawn from a "beach" to "peach" continuum and the VOT was sampled from the lower
VOT cluster, than the response options could be "beach" and either "peak" or
"peas".  That is, the only response that was compatible with the _rest_ of the
word had an onset consistent with the VOT cluster that that trial was drawn
from.

Two different semi-supervised conditions were run which differed in how labeled
and unlabeled trials were distributed across VOTs (@Fig:exp2-label-dists).  No
differences were found between behavior in these conditions so they are analyzed
analyzed together.[^supconds]

[^supconds]: The two conditions are listed separately in the
    [`supunsup`](https://github.com/kleinschmidt/phonetic-sup-unsup) R data
    package.

<!-- TODO: this should go in a footnote in the introduction -->
The only other difference with Experiment 1 was that the -10ms, 30ms condition
was not included.  This is because these two experiments were originally run
concurrently as a pilot for an imaging study.  The original goal of the
supervised conditions of Experiment 2 was to see if distributional learning
could be accelerated by including some labeled trials.  The -10ms, 30ms
condition of Experiment 1 was run after analyzing data from Experiments 1 and 2
together.

### Participants

I recruited `r n_total_2 ` participants via Amazon's Mechanical Turk.  As in
Experiment 1, participants were paid $2.00 for participation.  Exclusion criteria
were the same as Experiment 1, and data from `r n_excluded_2` participants were
excluded from analysis: `r n_subj_repeat_2` for repeated participation
(including participants who had participated in Experiment 1), `r n_subj_bad_2` for
inaccurate categorization of unambiguous VOTs, with `r n_both_2` excluded for
both criteria.  This left data from `r n_subj_exp2` remaining for analysis.

## Results

```{r exp2-regression-data, dependson=c("exp2-data")}

data_exp2_mod <- supunsup::supunsup_clean %>%
  filter(labeled == "unlabeled", bvotCond != "-10") %>%
  select(subject, supCond, labeled, bvotCond, trial, vot, respP) %>%
  mutate(vot_s = (vot - mean(vot)) / sd(vot),
         trial_s = (trial - mean(trial)) / sd(trial),
         supervised = fct_recode(supCond, supervised="mixed"))

```

```{r exp2-regression, dependson=c("exp2-regression-data")}

f_noint <- respP ~ 0 + bvotCond * supervised * vot_s * trial_s +
  (1 + vot_s | subject)

## b_logit_exp2 <- brm(f_noint,
##                data = d2,
##                family = bernoulli(),
##                chains=4,
##                iter=1000)

## saveRDS(b_logit_exp2, "models/brm_logistic_sup_v_unsup.rds")
b_logit_exp2 <- readRDS("models/brm_logistic_sup_v_unsup.rds")

exp2_blocks <-
  data_exp2_mod %>%
  group_by(block=ntile(trial, 3)) %>%
  summarise_at(vars(trial, trial_s), mean)

# overall fit (fixed effects):
data_pred_exp2 <-
  cross_df(list(vot_s = seq(min(data_exp2_mod$vot_s),
                            max(data_exp2_mod$vot_s),
                            length.out=100),
                bvotCond = unique(data_exp2_mod$bvotCond),
                supervised = unique(data_exp2_mod$supervised),
                block = 1:3)) %>%
  left_join(exp2_blocks, by="block") %>%
  left_join(conditions_exp1, by="bvotCond") %>%
  mutate(vot = vot_s * sd(data_exp2_mod$vot) + mean(data_exp2_mod$vot),
         bvotCond = fct_drop(bvotCond),
         vot_cond = fct_drop(vot_cond))


data_pred_subj_exp2 <-
  data_exp2_mod %>%
  group_by(subject, supervised, bvotCond) %>%
  summarise() %>%
  left_join(data_pred_exp2)

expt2_class_bysub <-
  fitted(b_logit_exp2, newdata=data_pred_subj_exp2) %>%
  as_tibble() %>%
  bind_cols(data_pred_subj_exp2)

expt2_bounds_bysub <-
  expt2_class_bysub %>%
  group_by(vot_cond, supervised, subject, trial, block) %>%
  find_bound(x=vot, y=Estimate)

expt2_bounds_fixef_samples <-
  tidybayes::linpred_draws(b_logit_exp2, data_pred_exp2, re_formula=NA) %>%
  group_by(vot_cond, supervised, block, trial, .draw) %>%
  find_bound(x=vot, y=.value)

expt2_bounds_fixef <-
  expt2_bounds_fixef_samples %>%
  group_by(vot_cond, supervised, block, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))

```

```{r expt2-label-consistent}
label_consistent <- data_exp2 %>%
  filter(labeled == "labeled") %>%
  summarise(p = round(100*mean(respCat == trueCat))) %>%
  pull(p)

label_range <- data_exp2 %>%
  filter(labeled == "labeled") %>%
  group_by(subject) %>%
  summarise(p = round(100*mean(respCat == trueCat))) %>%
  pull(p) %>%
  range()

```


```{r expt2-bounds-sup-diff}

expt2_bounds_fixef_samples %>%
  group_by(vot_cond, block, trial) %>%
  spread(supervised, vot) %>%
  mutate(vot_diff = unsupervised - supervised) %>%
  summarise(mean = mean(vot_diff),
            low = quantile(vot_diff, 0.025),
            high = quantile(vot_diff, 0.975))


```

```{r expt2-bounds, fig.cap='Providing 50% labeled trials during distributional learning makes no difference in listener\'s category boundaries (Experiment 2, solid voilins/points) compared to purely unsupervised learning (Experiment 1, outlined violins/points, re-plotted from @fig:expt1-boundaries).  For ease of comparison with results from Experiment 1, the same axis limits are used as @fig:expt1-boundaries; this lead to exclusion from the figure of five listeners with boundaries greater than 50ms (2 in the 20ms /b/ condition, 3 in the 30ms /b/ condition).  All participants are included in the regression model from which the fixed effects boundaries are estimated (points/lines).'}

expt2_bounds_bysub %>%
  filter(block==3) %>%
  ggplot(aes(x=vot_cond, y=vot, fill=vot_cond, color=vot_cond)) +
  geom_violin(data= . %>% filter(supervised=="unsupervised"), alpha=0)+
  geom_violin(data= . %>% filter(supervised=="supervised"), alpha=0.5, color=NA)+
  geom_pointrange(data = expt2_bounds_fixef %>% filter(block==3),
                  aes(y=mean, ymin=low, ymax=high, shape=supervised),
                  position=position_dodge2(width = .2),
                  color="white", show.legend=FALSE) +
  theme(legend.position="none") +
  scale_shape_manual(values = c(16, 1)) +
  lims(y=c(10,50)) +
  labs(title="Experiment 2: Semi-supervised adaptation",
       subtitle="Compared with unsupervised (Experiment 1)",
       y = "Boundary VOT (ms)",
       x = "Exposure condition (/b/, /p/ mean VOT)") +
  coord_flip()

```

First, listeners were highly sensitive to the labels on labeled trials,
responding consistent with the label `r label_consistent`% of the time (range
across participants of `r label_range[1]`--`r label_range[2]`%).  That is, listeners
used labels to guide their responses on that trial.

Second, did listeners use labels to guide or accelerate their _learning_?  As
with Experiment 1 I assessed listeners' distributional learning via category
boundaries estimated with a Bayesian logistic mixed-effects model.  This model
included data from _both_ Experiments 1 and 2 in order to directly assess
effects of supervised learning on the strength and time course of learning.
This model included fixed effects for supervision condition, all fixed effects
from the Experiment 1 model, and all interactions thereof.  The random effects
structure was identical to Experment 1.  Because listeners' responses were so
systematically determined by the label on labeled trials, _only_ unlabeled
trials were included in the model (all trials from Experiment 1, and half of the
trial from Experiment 2).

Figure -@fig:expt2-bounds shows the by-subject and overall category boundaries
for semi-supervised (shaded violins/points) and unsupervised (outlined
violins/points) distributional learning.  As in @Fig:expt1-boundaries, the
boundaries estimated from the model for a trial 5/6 of the way to the end of the
experiment, and the points and lines show the group-level (fixed effects)
estimates and 95% credible intervals.  The intervals for unsupervised and
semi-supervised overlap for all VOT conditions, indicating that the inclusion of
labeled trials makes no reliable difference in distributional learning, at least
at the trial where these boundaries are estimated.

Similarly, the 95% credible intervals for all fixed effects coefficients
involving the supervision condition include 0, suggesting that the presence of
labeled trials makes no difference in the baseline, strength, or time course of
distributional learning.

The condition that comes the closest to showing any effect is the 30ms /b/, 70ms
/p/ VOT, which is the most extreme shift relative to standard VOT distributions.
However, the fixed effect boundaries in this condition may have been biased by
the presence of three outlier participants with unreliable responses which leads to
very shallow classification functions and very high estimated boundaries
(greater than 50ms).  These participants weren't so unreliable as to be excluded by
the pre-set exclusion criteria so they are included in the analysis here for the
sake of completeness.  Even so, there is still no significant (Bayesian
$p<0.05$) effect of supervision in any regression coefficient _or_ the estimated
boundary, and the single-subject boundary distribution for this condition
(violins in @Fig:expt2-bounds) are very similar.

## Discussion

The results of Experiment 2 suggest that labeling individual VOTs as /b/ or /p/
does very little to affect listeners' distributional learning.  Both at the
condition level (fixed effect boundaries) and the subject level (distribution of
participants' boundaries), distributional learning leads to the same shift in
category boundaries regardless of whether all trials are unlabeld (Experiment 1)
or half labeled (Experiment 2).  This in turn suggests that the discrepancy
between listeners learned boundaries and the boundary implied by the exposure
distribution is _not_ due to uncertainty about the whether any particular VOT
was intended to be a /b/ or a /p/.  This is somewhat surprising, given that in
the extremely shifted 30ms /b/, 70ms /p/ condition most of the tokens from the
/b/ distribution would normally be classified as /p/ under a typical talker's
generative model.

One possible interpretation of this result is that the way that label
information was provided made the labels somehow unavailable to update
listeners' beliefs about how phonetic categories are acoustically realized.
This is not particularly plausible, for two reasons.  The first is that a wide
variety of labeling information has been shown to be effective in phonetic
recalibration, including lexical [@Kraljic2005; @Norris2003],
visual/articulatory [@Bertelson2003], and orthographic [@Keetels2016a] labels.
The second is that listeners were highly sensitive to the labeling information
in generating their responses _on labeled trials_, it was only on _unlabeled_
trials where the presence of labels on other trials seems to make no difference.

Another possibility is that even without labels, listeners are able to pick up
on the distributions they are exposed to, and so labeling some of the trials as
a clue to which distribution they come from doesn't provide any additional
information.  If this is the case, how can the discrepancy between the
boundaries implied by those distributions and the boundaries listeners actually
learned?  Experiment 3 addresses this question via computational modeling.

# Experiment 3---Computational modeling

Experiments 1 and 2 together present a puzzle: on the one hand, distributional
learning appears to be "incomplete", with listeners using category boundaries
that are reliably different from the boundaries implied by the VOT distributions
they were exposed to.  On the other hand, providing listeners extra
information---in the form of category labels for half the VOTs they heard---did
not have any effect on distributional learning.  This suggests that listeners
_know_ the distributions that the exposure talker produces, but they just don't
_believe_ them.  Circumstantial evidence to this effect comes from the fact that
the category boundaries that listeners _do_ use appear to be a compromise
between the category boundaries derived from the VOT distributions for the
exposure talker and those produced by a typical talker of American English.

In this experiment I use a computational model to explore the possibility that
listeners start at some common starting point and update their beliefs based on
experience with the current talker.  Specifically, I use a model based on the
ideal adapter framework [@Kleinschmidt2015b], which is based on the idea that
listeners maintain uncertain beliefs about the distribution of cues
corresponding to each phonetic category.  Crucially, according to the ideal
adapter, when listeners encounter an unfamiliar talker, they initially entertain
a set of prior beliefs about the phonetic cue distributions that an unfamiliar
talker is likely to produce, which takes into account the variability across
talkers that they have previously experience, and any available socio-indexical
information that may be informative about this talker's cue distributions
[@Kleinschmidt2019].  After direct experience with that talker, a listener will
_update_ their beliefs about that talker's cue distributions, bringing them into
better alignment with the cue distributions that the talker has actually
produced.

As a result, at any given point a listener's beliefs about that talker's cue
distributions will be a _compromise_ between what they expected a priori, and
the distributions they have actually encountered.  When a listener has limited
experience with that particular talker, or especially strong, specific prior
expectations, this compromise will be especially obvious.  One prediction of the
ideal adapter is that listeners' prior expectations should be calibrated to the
level (and type) of talker variability they have actually encountered.  Because
there's relatively little variability across talkers in VOT distributions for
word-initial stops [@Kleinschmidt2019], this is exactly such a case where
listeners might be expected to bring strong prior expectations and hence show a
clear compromise between those prior expectations and the distributions that
they encounter in these experiments.



## Methods

I use a Bayesian belief updating model, where listeners begin the experiment
with a shared set of prior beliefs about the mean and variance of the VOT
distributions for /b/ and /p/, and update these beliefs according to Bayes rule
based on the VOTs they hear in the experiment.

Specifically, a listener's uncertain beliefs about the mean and variance of each
category (/b/ and /p/) are represented as Normal-Inverse Chi squared
distribution.  At the beginning of the experiment, every listener has the same
prior beliefs, expressed in the model as parameters for the _expected_ mean and
variance ($\mu_{0}$ and $\sigma^2_0$), and for the _strength_ of these beliefs
($\kappa_0$ for the mean and $\nu_0$ for the variance).  These confidence
parameters are "pseudocounts", in that they are equivalent to assuming that the
listener's expectations about the category mean is based on $\kappa_0$ prior
observations from that category, and likewise for the variance and $\nu_0$.
This prior distribution is a _conjugate prior_ because after observing $n$ data
points with sample mean $\bar x$ and variance $s^2$, the posterior distribution
is also a Normal-Inverse Chi squared, with parameters

\begin{align}
\kappa_n &= \kappa_0 + n \\
\nu_n &= \nu_0 + n \\
\mu_n &= \frac{\kappa_0\mu_0 + n\bar x}{\kappa_0 + n} \\
\sigma^2_n &= \frac{1}{\nu_n}\left(\nu_0 \sigma^2_0 + ns^2 +
  \frac{n\kappa_0}{\kappa_0 + n}(\mu_0 - \bar x)^2\right)
\end{align}

These parameter updates have an intuitive interpretation [@Kleinschmidt2015b]:
the confidence parameters are increased by the number of observations $n$ (which
is why they are called "pseudocounts"), while the new expected mean and variance
are weigthed averages of the prior expectations and the observed values, with
the weights determined by the pseudocounts and actual count,
respectively.[^var-mean]

[^var-mean]: For the variance update, there is an additional term that captures
    the possibility that a mismatch between the expected and observed mean could
    be due to a higher than expected variance.

Given updated beliefs about the mean and variance of each category the
probability of any particular VOT $x$ being /b/ vs. /p/ can be calculated by the
marginal likelihood of $x$ under the /b/ and /p/ distributions [see
@Kleinschmidt2015b for more details].  The marginal likelihood is essentially a
weighted average of the likelihood under all possible means and variance for
each category, weighted by how probable each mean/variance is based on the VOTs
observed from each category so far and the prior beliefs.  These marginal
likelihoods are then converted to a probability of responding /p/ or /b/ using
Bayes' rule.

### Assumptions

In actually implementing this model, I make a number of simplifying assumptions
for the sake of tractability.  First, and most importantly, I provide the model
with the mean and standard deviation of the two VOT distributions.  This
substantially simplifies the complexity of the learning problem: in order to
model _unsupervised_ learning, a Bayesian model needs to average predicted
behavior over distributional learning given each possible way that all the
tokens could be jointly classified, weighted by the probability of that
classification.  In an experiment with 200 trials, each of which could be either
/b/ or /p/, this ammounts to more than $10^{60}$ possibilities, an impossibly
large number to consider.  There are approximations for unsupervised or
semi-supervised belief updating, but they substantially increase the
computational complexity and exploring them is left for future work.

Second, the use of a conjugate Normal Inverse Chi-squared prior is also an
important simplification, as it means that the belief updating can be computed
analytically.  Combined with the first assumption, this means that the updated
beliefs can be computed directly from the prior parameters and the observed
count, mean, and variance of each category.

### Model fitting procedure

The free parameters of this model were the prior expected mean and variance of
/b/ and /p/, and the mean and variance prior pseudocounts (which were assumed to
be the same for /b/ and /p/ because pilot simulations suggested it was not
possible to reliably identify both).  I also included a guessing rate parameter,
which captured the fact that many listeners never reach floor or ceiling and
thus may be guessing on a proportion of trials.

The model was fit to data from Experiment 1 (unsupervised distributional
learning) using a custom Stan model [@Carpenter2017].  The trials were divided
up into six blocks of equal length (37 trials).  Responses in each block in each
distribution condition were predicted based on updated beliefs at the halfway
point of the block, using the sufficient statistics of that distribution
condition.

No "random effects" (hierarchical effects by subject) were included.  This is a
difficult choice to make.  On the one hand, there likely _are_ differences in
listeners' prior expectations and learning rates, and ignoring these ignores
important variability in the data.  On the other hand, including such random
effects by subjects introduces many additional degrees of freedom into the
model, and increases the number of data patterns it could fit.  This is
especially dangerous in this particular case since each listener only
encountered a single set of distributions, and so random effects for _subjects_
effectively introduces unconstrained flexibility in how the model captures
differences between _distributions_.  So, I opted not to include random effects
in the belief updating model, so as to make as rigorous as possible a test of
its ability to capture differences in distributional learning found above.

## Results

The belief updating model can be evaluated in a number of ways.  First, I
discuss the _outcome_ of distributional learning predicted by the model: does
the model capture the classification functions that listeners use at the end of
the experiment?  Second, I look at the _time course_ of the learning that the
model takes: does it capture the how classification functions _change_ with
increasing exposure to the experimental distributions?  Third, and finally, I
will examine whether the shared prior beliefs---the starting point for belief
updating in the model---match the distributions produced by a typical talker of
American English.

```{r infer-prior}

## options(mc.cores = parallel::detectCores())
## fit_inc <- infer_prior_beliefs(data_exp1,
##                                cue = "vot",
##                                category = "trueCat",
##                                response = "respCat",
##                                condition = "vot_cond",
##                                ranefs = "subject",
##                                n_blocks = 6,
##                                chains = 4,
##                                iter = 2000)
## saveRDS(fit_inc, "models/fit_inc.rds")

fit_inc <- readRDS("models/fit_inc.rds")

categories <-
  data_frame(cat_num = 1:2,
             category = c('b', 'p'))

prior_samples_df <-
  fit_inc %>%
  spread_draws(nu_0, kappa_0, mu_0[cat_num], sigma_0[cat_num]) %>%
  left_join(categories)

## create a data_frame with samples for updated parameters
updated_samples_df <-
  fit_inc %>%
  spread_draws(c(kappa_n, nu_n, mu_n, sigma_n)[block_num, cat_num, cond_num],
               lapse_rate[block_num]) %>%
  left_join(categories) %>%
  left_join(mutate(conditions_exp1, cond_num = as.numeric(bvotCond)))

## create a data_frame for lapsing rate samples
lapse_rate_samples <-
  fit_inc %>%
  spread_draws(lapse_rate[block_num])
```

```{r mod-class-funs, dependson=c("infer-prior")}

mod_class_funs <- 
  updated_samples_df %>%
  group_by(.draw) %>%
  nest() %>%
  sample_n(200) %>%
  unnest() %>%
  mutate(mean=mu_n, sd=sigma_n) %>%
  select(.draw, block_num, bvotCond, vot_cond, category, mean, sd) %>%
  group_by(.draw, bvotCond, vot_cond, block_num) %>%
  do(stats_to_lhood(., noise_sd=0)) %>%
  lhood_to_classification() %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = (1-lapse_rate)*prob_p + lapse_rate/2) %>%
  group_by(bvotCond, vot_cond, block_num, vot) %>%
  select(bvotCond, block_num, vot, prob_p) %>%
  summarise(prob_p_low = quantile(prob_p, 0.025),
            prob_p_high = quantile(prob_p, 0.975),
            prob_p = mean(prob_p))

prior_class_funs <-
  prior_samples_df %>%
  group_by(.draw) %>%
  nest() %>%
  sample_n(200) %>%
  unnest() %>%
  mutate(mean=mu_0, sd=sigma_0) %>%
  select(.draw, category, mean, sd) %>%
  group_by(.draw, category) %>%
  do(stats_to_lhood(., noise_sd=0)) %>%
  lhood_to_classification() %>%
  group_by(vot) %>%
  summarise(prob_p_low = quantile(prob_p, 0.025),
            prob_p_high = quantile(prob_p, 0.975),
            prob_p = mean(prob_p))

```

```{r mod-vs-behav-class-funs, fig.width=10, fig.height=3, fig.cap="Belief updating model classification functions (thin ribbons) vs. average probability /p/ response from Experiment 1, both during the final sixth of trials.  Points show the average proportion of /p/ responses, and CIs show 95% bootstrapped CIs over subjects.  Ribbons show 95% Bayesian credible interval for model posterior predictions, and the dashed black lines show classification function from the inferred prior."}

data_exp1 %>%
  filter(ntile(trial, 6) == 6) %>%
  group_by(subject, vot, vot_cond) %>%
  summarise(respP = mean(respP)) %>%
  ggplot() +
  geom_line(data = prior_class_funs,
            aes(x=vot, y=prob_p),
            color="black", linetype=2) +
  geom_ribbon(data=filter(mod_class_funs, block_num==6),
              aes(x=vot, ymin=prob_p_low, ymax=prob_p_high, fill=vot_cond),
              alpha=0.5) +
  geom_pointrange(aes(x=vot, y=respP, color=vot_cond),
                  stat="summary", fun.data="mean_cl_boot") +
  facet_grid(.~vot_cond) +
  labs(title = "Belief updating model",
       subtitle = "Classification function versus behavioral data for final 6th of trials in Experiment 1",
       x = "VOT (ms)",
       y = "Probability /p/ response")

```

```{r mod-vs-behav-class-funs-all-blocks, eval=FALSE}

# all blocks:
data_exp1 %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(subject, vot, vot_cond, block_num) %>%
  summarise(respP = mean(respP)) %>%
  ggplot() +
  geom_pointrange(aes(x=vot, y=respP, group=interaction(vot_cond, block_num),
                      color=vot_cond),
                  stat="summary", fun.data="mean_cl_boot") +
  ## geom_ribbon(data=mod_class_funs,
  ##             aes(x=vot, ymin=prob_p_low, ymax=prob_p_high, fill=vot_cond),
  ##             alpha=0.5) +
  geom_line(data=mod_class_funs,
            aes(x=vot, y=prob_p, group=interaction(vot_cond, block_num), color=vot_cond)) +
  geom_line(data = prior_class_funs,
            aes(x=vot, y=prob_p),
            color="black", linetype=2) +
  facet_grid(.~block_num)

data_exp1 %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(subject, vot, vot_cond, block_num) %>%
  summarise(respP = mean(respP)) %>%
  ggplot() +
  geom_pointrange(aes(x=vot, y=respP, group=interaction(vot_cond, block_num),
                      color=vot_cond),
                  stat="summary", fun.data="mean_cl_boot") +
  ## geom_ribbon(data=mod_class_funs,
  ##             aes(x=vot, ymin=prob_p_low, ymax=prob_p_high, fill=vot_cond),
  ##             alpha=0.5) +
  geom_line(data=mod_class_funs,
            aes(x=vot, y=prob_p, group=interaction(vot_cond, block_num), color=vot_cond)) +
  geom_line(data = prior_class_funs,
            aes(x=vot, y=prob_p),
            color="black", linetype=2) +
  facet_grid(block_num~vot_cond)


```

### Outcome of learning

First, does the belief updating model successfully capture the overall pattern
of incomplete distributional learning by the end of the experiment?  This serves
as a check whether the overall pattern of distributional learning is compatible
with an "ideal adpater" belief updating process, where every listener starts
with the same, shared prior expectations about an unfamiliar talker's VOT
distributions for /b/ and /p/.

Figure -@fig:mod-vs-behav-class-funs shows the belief-updating model's predicted
classification functions in the final sixth of trials (186--222) in the
experiment, along with the average probability of /p/ response (plus
bootstrapped 95% CI over listeners).  Qualitatively, the model provides a good
fit to the overall classification functions for each VOT condition, and the
estimated boundaries align well with the observed boundaries in each case.
<!-- TOO: maybe move this to discussion?? -->
This suggests that the pattern of incomplete distributional learning found in
Experiment 1 is _compatible_ with a belief updating model.  This is not terribly
surprising, since the category boundaries listeners were using by the end of the
experiment appeared to reflect a compromise between a typical talker and the
experimental talker's distributions, and a Bayesian belief-updating model
formalizes exactly this kind of compromise in a statistical framework.  However,
the ability of the model to _quantitatively_ capture the pattern of
distributional learning with a single set of starting beliefs serves as an
existence proof, putting this explanation of the constrained distributional
learning observed above in Experiment 1 on firmer footing.


```{r modeled-category-boundaries}

modeled_boundaries_samples <-
  updated_samples_df %>%
  group_by(.draw) %>%
  nest() %>%
  sample_n(200) %>%
  unnest() %>%
  mutate(mean=mu_n, sd=sigma_n) %>%
  select(.draw, block_num, bvotCond, vot_cond, category, mean, sd) %>%
  group_by(.draw, bvotCond, vot_cond, block_num) %>%
  do(stats_to_lhood(., xlim=c(0, 50), noise_sd=0)) %>%
  lhood_to_classification() %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = (1-lapse_rate)*prob_p + lapse_rate/2) %>%
  find_bound(vot, prob_p)

exp1_blocks6 <-
  data_exp1_mod %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(block_num) %>%
  summarise(trial = mean(trial), trial_s = mean(trial_s))


modeled_boundaries <- modeled_boundaries_samples %>%
  group_by(bvotCond, vot_cond, block_num) %>%
  summarise_at(vars(vot), funs(mean, low=quantile(., 0.025), high=quantile(., 0.975))) %>%
  left_join(exp1_blocks6, by="block_num")

```

```{r}

## re-compute expt1 bounds course with 6-tiles instead of 3-tiles

data_pred6 <-
  cross_df(list(vot_s = (seq(0, 50, by=3) - mean(data_exp1_mod$vot)) / sd(data_exp1_mod$vot),
                bvotCond = unique(data_exp1_mod$bvotCond),
                block_num = 1:6)) %>%
  left_join(exp1_blocks6, by="block_num") %>%
  mutate(vot = vot_s * sd(data_exp1_mod$vot) + mean(data_exp1_mod$vot)) %>%
  left_join(conditions_exp1, by="bvotCond")

expt1_bounds_fixef_samples6 <-
  tidybayes::linpred_draws(b_logit_exp1, data_pred6, re_formula=NA) %>%
  group_by(vot_cond, block_num, trial, .draw) %>%
  find_bound(x=vot, y=.value)

expt1_bounds_fixef6 <-
  expt1_bounds_fixef_samples6 %>%
  group_by(vot_cond, block_num, trial) %>%
  summarise(low=quantile(vot, 0.025), high=quantile(vot, 0.975), mean=mean(vot))


```

```{r}

bounds_6tile_exp1 <-
  data_exp1 %>%
  select(subject, vot_cond, trial, vot, respP) %>%
  mutate(block_num = ntile(trial, 6)) %>%
  group_by(vot_cond, block_num, vot) %>%
  summarise(respP = mean(respP)) %>%
  find_bound(vot, respP) %>%
  left_join(exp1_blocks6, by="block_num")
  

```

```{r plot-modeled-category-boundaries, fig.width=6.8, fig.height=5.3, fig.cap="Belief updating model's learning curves (ribbons, 95% credible intervals) compared with empirical category boundary, defined as the VOT where subjects' responses (linearly interpolated) are 50% /p/, 50% /b/.  Both model and empirical boundaries are estimated for each sixth of trials (blocks of 37 out of 222)."}

ggplot(modeled_boundaries, aes(x=trial, y=mean)) +
  # geom_line(aes(color=vot_cond)) +
  geom_ribbon(aes(ymin=low, ymax=high, fill=vot_cond), alpha=0.5) +
  geom_text(data = . %>% filter(block_num == 6), aes(label = vot_cond, color=vot_cond),
            hjust=0, nudge_x=6) +
  labs(x = "Trial",
       y = "/b/-/p/ boundary (ms VOT)",
       title = "Model predicted and actual learning curves",
       subtitle = "Change in /b/-/p/ category boundary with learning") +
  lims(x = c(NA, 222* 1.01)) +
  theme(legend.position = "hide") +
  geom_point(data = bounds_6tile_exp1,
             aes(y=vot, color=vot_cond)) +
  geom_line(data = bounds_6tile_exp1,
            aes(y=vot, color=vot_cond))
  ## geom_pointrange(data = expt1_bounds_fixef6,
  ##                 aes(y=mean, ymin=low, ymax=high, color=vot_cond)) +
  ## geom_line(data = expt1_bounds_fixef6,
  ##           aes(y=mean, color=vot_cond))
  ## geom_point(data = expt1_bounds_s,
  ##            aes(y=vot, color=vot_cond)) +
  ## geom_line(data = expt1_bounds_s,
  ##           aes(y=vot, color=vot_cond))


# alternative figure: same style as boundary plots from experiments
## ggplot(modeled_boundaries_samples,
##        aes(x = vot_cond, y = vot, fill=vot_cond, alpha = block_num,
##            group=interaction(block_num, vot_cond))) +
##   geom_violin(position="identity", color=NA) +
##   geom_violin(data = expt1_bounds_fixef_samples,
##               aes(alpha=2*block, color=vot_cond, group=interaction(block, vot_cond)),
##               fill=NA, position="identity") +
##   coord_flip()

```

### Learning curves

Next, how well does the belief-updating model capture the learning curve, or
change in category boundaries with further experience?  Figure
-@fig:plot-modeled-category-boundaries shows the belief-updating predicted
learning curves, compared with the empircal boundaries (points/lines) estimated
for each condition in each sixth of the experiment (blocks of 37 trials).  The
belief updating model qualitatively captures the change in category boundaries,
both within each condition and across conditions.  Boundaries start off quite
similar at the beginning of the experiment, and gradually diverge as listeners
gain more exposure to the particular VOT distributions of their condition.

Comparing these modeled learning curves with the behavioral data directly is not
straightforward.  Figure -@fig:plot-modeled-category-boundaries shows one
estimate of the changing category boundaries, estimated directly from the data
in each sixth of the experiment (e.g., the VOT where the average proportion-/p/
response curve crosses 50% in each condition).  Other reasonable possibilities
include a boundary calculated from the regression model reported in Experiment 1
(which includes a linear change in intercept, or /p/ bias, over trials, as well
as a linear change in the slope of the VOT boundary), or a more complex
regression model using a spline term in each condition to capture non-linear
changes in the offset (but not the slope).

All of these methods agree qualitatively on where the boundaries are _later_ in
the experiment, when listeners make very few errors or random responses.  Where
they disagree (with each other, and with the belief udpating model) is in the
estimated boundaries for the early trials, where listeners make many random or
guessing responses, even on unambiguous VOTs.  In the belief updating model,
this is captured by the "lapse rate" or the proportion of trials on which the
simulated subjects simply guess randomly, which is allowed to vary over trials
(Figure -@fig:lapse-rate-by-block).  The inferred lapse rate is nearly 12% in
the first sixth of trials (1-37), while for the remainder of the experiment it
is closer to 5%.  Models that don't take into such guessing into account are
subject to mis-estimate the boundary [location and slope @Clayards2008;
@Wichmann2001], and so for this reason I chose the more interpetable---if
somewhat noisier---direct estimate of the boundary over the estimates from
either of the regression models for comparison with the belief updating model.

```{r lapse-rate-by-block, fig.width=5.8, fig.height=3.9, fig.cap="Lapse (guess) rate inferred in the belief updating model for each 1/6th of trials in Experiment 1.  The inferred lapse rate starts rather high (nearly 12%) but decreases to around 5% after the first 1/6 of trials (1--37)"}

lapse_rate_samples %>%
  summarise_at(vars(lapse_rate),
               funs(mean, low=quantile(., 0.025), high=quantile(., 0.975))) %>%
  left_join(exp1_blocks6, by="block_num") %>%
  ggplot(aes(x=trial, y=mean, ymin=low, ymax=high)) +
  geom_pointrange() +
  labs(y="Guessing rate",
       title="Inferred guessing rate for belief updating model",
       subtitle="Means and 95% credible intervals") +
  lims(y=c(0, NA),
       x=c(0, NA))

```

### Inferred prior beliefs

Finally, what are the prior beliefs that the model infers listeners are bringing
to this task?  There are two relevant aspects of the inferred prior beliefs: the
_content_ of those beliefs, and their _strength_.  The strength corresponds to how
confident the modeled listener is in their prior beliefs, which is represented
in the model in the prior confidence pseudocounts for the means ($\kappa_0$) and
variance ($\nu_0$).  These parameters correspond to the number of observations
required to get to a point where the listener's updated beliefs in equal parts
informed by their prior expectations and recent experience.

```{r confidence-params, fig.width=5.6, fig.height=4.4, fig.cap="Sampled values of prior confidence parameters from belief updating model fit to Experiment 1.  Upper left shows values where $\\kappa_0 < \\nu_0$ (\"shifting\"), and lower right shows values where $\\nu_0 < \\kappa_0$ (\"expanding\").  Red stars show approximate MAP values for each kind of solution (individual sample with the highest posterior)."}

confidence_summaries <-
  prior_samples_df %>%
  filter(category=="b") %>%
  ungroup() %>%
  summarise_at(vars(kappa_0, nu_0), funs(mean, low=quantile(., 0.025), high=quantile(., 0.975))) %>%
  mutate_all(funs(formatC(signif(., digits=2), digits=2, format="fg", preserve.width="none")))

p_shift <- round(with(prior_samples_df, mean(nu_0 > kappa_0)), 2)

confidence_maps <- tidybayes::spread_draws(fit_inc, kappa_0, nu_0, lp__) %>%
  group_by(kappa_0 > nu_0) %>%
  arrange(desc(lp__)) %>%
  filter(row_number() == 1)

prior_samples_df %>%
  ggplot(aes(x=kappa_0, y=nu_0)) +
  #geom_density2d() +
  geom_point(alpha=0.05) +
  scale_x_log10(TeX("Pseudocount for mean ($\\kappa_0$)")) +
  scale_y_log10(TeX("Pseudocount for variance ($\\nu_0$)")) +
  coord_equal() +
  geom_abline() +
  geom_point(data = confidence_maps, size=10, shape="*", color="red")

```

Based on the data from Experiment 1, listeners are highly confident in their
prior beliefs.  The expected prior pseudocount for the means was
$\kappa_0 = `r confidence_summaries$kappa_0_mean`$ (95% credible interval of
$[`r confidence_summaries['kappa_0_low']`,
`r confidence_summaries['kappa_0_high']`]$).  This is nearly twice the number of
trials in the experiment from each cluster (111 each, for a total of 222),
meaning that by the end of the experiment the model's beliefs about the mean
VOTs for /b/ and /p/ are still primarily determined by the prior beliefs.
Similarly, the expected prior pseudocount for the variances 
$\nu_0 = `r confidence_summaries$nu_0_mean`$ (95% credible interval of 
$[`r confidence_summaries["nu_0_low"]`,
`r confidence_summaries["nu_0_high"]`]$), which is much larger than the number
of trials from each cluster.

Together, these parameters suggest that listeners' have strong prior beliefs
about the VOT distributions associated with /b/ and /p/, and moreover that their
beliefs in the _variance_ of VOTs is stronger than their beliefs in the _means_.
In turn, this suggests that the distributional learning observed in Experiment 1
is driven mostly by changes in listeners' beliefs about the underlying means,
with only small contributions from changes in the variance (e.g., a relaxation
in their standards for what counts as a good /b/ or /p/).

However, as in previous work with similar belief updating models
[@Kleinschmidt2015b], the posterior distribution of these parameters was
multimodal, where one mode puts more confidence in the means ($\kappa_0 >
\nu_0$) and adapts mostly by updating beliefs in the variance, while the other
puts more in the variances ($\nu_0 > \kappa_0$) and adapts by shifting.
Nevertheless, the "shifting" solution is preferred, with $\nu_0 < \kappa_0$ in
the majority of MCMC samples (Bayesian $p$ value of `r p_shift`).


```{r inferred-beliefs}

prior_samples_df %>%
  gather(param, value, mu_0, sigma_0) %>%
  ggplot(aes(x=value)) +
  geom_density() +
  facet_grid(param ~ category, scales="free")

prior_samples_df %>% ggplot(aes(x=kappa_0, y=mu_0, color=category)) + geom_point(alpha=0.1)
  

```



## Discussion

The results of the belief updating modeling show that the pattern of constrained
distributional learning from Experiment 1 can be explained as belief updating
starting from a shared set of prior beliefs.  Furthermore, the model suggests
that these prior beliefs are relatively strong, with listeners behaving as if
they'd already observed $2--5\times$ more VOT samples than they would encounter
in the experiment before it even started.  This is in stark contrast to the
results of a simplar belief updating model of recalibration
[@Kleinschmidt2015b], which found that listeners have relatively _weak_ prior
beliefs. <!-- TODO: why the discrepancy -->




However, there are a few important caveats to these conclusions.  First and most
importantly, they are contingent on the choice of the prior distribution.  For
mathematical convenience, this model uses a Normal-Inverse $\chi^2$
distribution.  This is a standard choice a conjugate prior for a normal
distribution when there is uncertainty in the mean and variance [@Gelman2003].
The use of a conjugate prior makes this modeling computationally tractable and
leads to interpretable parameters [@Kleinschmidt2015b].  It also imposes strong
assumptions about the _relationship_ between the uncertainty that listeners have
about the mean and variance.  Of particular relevance to these results, under
such a prior the uncertainty about the mean is proportional to the
variance.[^normal-prior] <!-- TODO: finish this thought -->

[^normal-prior]: Specifically, the prior distribution of the mean $\mu$ is
    _conditional_ on the variance, 
    $\mu | \sigma^2 \sim \mathrm{Normal}(\mu_0, \sigma^2 / \kappa_0)$.



* learning may flatten off and assymptote with more exposure, reflecting
  discounting of less recent trials (Dave and Emily's paper, Giraud paper, ).
  This is still _compatible_ with belief updating, it just suggests that
  listeners are assuming a different implicit generative model, which allows for
  the fact that a talker's distributions may drift, or change slowly, making
  recent observations more informative about the _current_ distributions than
  are observations from many minutes, hours, days, etc. ago.



# Experiment 4
