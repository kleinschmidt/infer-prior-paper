---
title: |
  Supplementary information for "What constrains distributional learning
  in adults"
author: Dave F. Kleinschmidt
bibliography: /home/dave/Documents/papers/zotero.bib
output:
  html_document:
    code_folding: hide
    keep_md: true
    md_extensions: +implicit_figures
    pandoc_args:
    - --filter
    - pandoc-crossref
  pdf_document:
    md_extensions: +implicit_figures
    fig_crop: false
    keep_tex: true
    latex_engine: xelatex
    citation_package: biblatex
    dev: cairo_pdf
    pandoc_args:
    - --filter
    - pandoc-crossref
mainfont: "CMU Serif"
reference-section-title: "References"
---


This is supplementary information for the paper "What constrains distributional
learning in adults?", submitted for publication.  A preprint version of the
paper and the RMarkdown source for both the manuscript and this supplementary
information can be found at this OSF repository: 
[osf.io/3wdp2/](https://osf.io/3wdp2/).


```{r knitr-setup, warning=FALSE, message=FALSE, error=FALSE, echo=FALSE, results='hide'}

library(knitr)
opts_chunk$set(warning = FALSE,
               message = FALSE,
               error = FALSE,
               cache=TRUE,
               results = "hide",
               echo=opts_knit$get("rmarkdown.pandoc.to") != 'latex')

options(digits=2)

## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})
## Produce markdown-formatted table captions with anchors for cross-refs.
## Requires pandoc-tablenos to parse the IDs. Refer to tables
## in text with {@tbl:label} or @tbl:label.
## Based partly on http://stackoverflow.com/a/18672268
##
## (see https://github.com/tomduck/pandoc-tablenos)
knit_hooks$set(tbl.cap = function(before, options, envir) {
  if(!before){
    paste0('\n\nTable: ', options$tbl.cap,
           ' {#tbl:', options$label, '}', sep = '')
  }
})

```

```{r preamble, cache=FALSE}

library(tidyverse)
library(dplyr)
library(glue)
library(magrittr)
library(beliefupdatr)
library(supunsup)

library(rstan)
library(brms)
library(tidybayes)
library(ggridges)

library(kableExtra)
options(knitr.kable.NA="")

library(cowplot)
theme_set(cowplot::theme_cowplot() +
            theme(plot.title=element_text(hjust=0)))
library(latex2exp)

## devtools::install_github('kleinschmidt/daver')
library(daver)
## devtools::install_github('kleinschmidt/phonetic-sup-unsup')
library(supunsup)
## devtools::install_github('kleinschmidt/beliefupdatr')
library(beliefupdatr)

```

```{r}

## clean names from regression models

beta_name_subs <- list(
  c(":", " : "),
  c("vot_s", "VOT"),
  c("trial_s", "Trial"),
  c("bvotCond", "/b/ VOT="),
  c("supervisedunsupervised", "unsupervised"),
  c("vot_cond(-?[0-9]0)([0-9]0)", "VOT /b/=\\1,/p/=\\2"),
  c("vot_condM", "vot_cond-") # I think str_replace_multi starts at the end...so this happens first
  )

clean_beta <- function(x) str_replace_multi(x, beta_name_subs, replace_all = TRUE)

gather_betas <- function(fit) {
  beta_levels <- rownames(fixef(fit))
  fit %>%
    gather_draws(`b_.*`, regex=TRUE) %>%
    ungroup() %>%
    mutate(.variable = factor(clean_beta(str_replace(.variable, "b_", "")),
                              levels = rev(clean_beta(beta_levels))))
}

scale_beta_gt0 <- 
  scale_fill_gradient2("p(β > 0)",
                       midpoint=0.5,
                       mid=gray(0.9),
                       low=scales::muted("blue"),
                       high=scales::muted("red"))

summary_to_tibble <- function(x) {
  x %>%
    as.data.frame() %>%
    tibble::rownames_to_column(var="Coefficient") %>%
    transmute(Coefficient=clean_beta(Coefficient), Estimate, `l-95% CI`, `u-95% CI`)
}

fixef_ranef_tables <- function(fit) {
  fit_summary <- summary(fit)

  named <- c(
    list("Fixed effects (β)"=fit_summary$fixed),
    fit_summary$random %>% set_names(., glue("Random effects: {names(.)}"))
  )

  map(named, summary_to_tibble)
}

multi_kable <- function(named, ...) {
  named %>%
    bind_rows() %>%
    kable(...) %>%
    pack_rows(index = map(named, nrow))
}

```

# Experiment 1

The Bayesian logistic regression model for Experiment 1 was fit using `brms`
[@Burkner2017], based on the formula:

```
respP ~ 1 + bvotCond * vot_s * trial_s + (1 + vot_s | subject)
```

`respP` is 0 for a click on the /b/-initial response picture and 1 for the
/p/-initial, `bvotCond` is the experimental condition (exposure distributions),
dummy coded as 0 or 1, and `vot_s` and `trial_s` are the stimulus VOT and the
trial number, each standardized to have mean 0 and variance 1.

This formula generates fixed effects for all main effects of condition, VOT, and
trial, all two-way interactions, and the three-way interaction.

```{r expt-1-model-table, results="asis"}

b_logit_exp1 <- readRDS("models/brm_logistic_exp1.rds")
exp1_max_rhat <- format(max(rhat(b_logit_exp1)), digits=3)

b_logit_exp1 %>%
  fixef_ranef_tables() %>%
  multi_kable(caption="Bayesian mixed-effects logistic regression coefficients from Experiment 1") %>%
  kable_styling(full_width = FALSE, bootstrap_options = "condensed")

```

Table 1 shows the fixed-effects regression coefficients for the fixed effects
and standard deviations/correlations for the random effects, along with the 95%
credible intervals for each parameter.  All $\hat{R} < `r exp1_max_rhat`$
indicating good convergence.  [Figure @fig:expt-1-model-plot] visualizes the
posterior distributions of the fixed effects coefficients, with color indicating
how reliably they are greater than or less than zero.

```{r expt-1-model-plot, fig.width=9.4, fig.height=5.7, fig.cap="Posterior distributions for fixed-effects coefficients in Experiment 1.  Red indicates coefficients that are reliably greater than zero, and blue indicates reliably less than zero."}

b_logit_exp1 %>%
  gather_betas() %>%
  group_by(.variable) %>%
  mutate(p_gt_zero = mean(.value > 0)) %>%
  ggplot(aes(x=.value, y=.variable, fill=p_gt_zero)) +
  geom_density_ridges(relative_scale = 2) +
  scale_beta_gt0 +
  geom_vline(xintercept = 0, linetype=3) +
  labs(x = "β (log-odds of /p/ response)",
       y = "")

```


# Experiment 2

The regression model for Experiment 2 was another Bayesian logistic
mixed-effects regression model, fit with `brms` [@Burkner2017], using the
following formula:

```
respP ~ bvotCond * supervised * vot_s * trial_s + (1 + vot_s | subject)
```

This includes a main effect and all interactions with `supervised`, which codes
whether a given subject was in the supervised (0) or unsupervised (1)
condition.  All other variables were coded as in Experiment 1.

Only unlabeled trials were used to estimate the model, since responses to
labeled trials were nearly always consistent with the label and hence
uninformative about the listener's classification function at that point.

## Prior for Bayes Factors

In order to calculate Bayes factors for the null hypothesis of no effect of
supervision vs. the alternative for some possibly non-zero effect via the
Savage-Dickey method [@Wagenmakers2010], I had to specify a proper prior on beta
coefficients.  By default, `brms` uses an improper (uniform) prior over the
whole real number line, which does not assign a finite, positive probability to
any value.  I chose a Student's $t$ distributions with 3 degrees of freedom,
mean 0, and scale of 1 (`student_t(3, 0, 1)` in the `brms` syntax).  This
particular prior is a good choice in this case because it has heavy tails and
therefore does not meaningfully constrain parameter estimates in the presence of
sufficient data, yet still assigns a reasonably high prior probability to 0,
which corresponds to the null hypothesis of zero effect of the corresponding
parameter.  This is important for the Savage-Dickey method because it uses the
ratio between the prior and posterior probability assigned to the parameter
value corresponding to the null hypothesis as a means of estimating the Bayes
factor (e.g., support of the evidence for or against the null hypothesis).  If
the prior is so spread out that it assigns very low probability to the null
hypothesis parameter value, then any data that leads to an estimate anywhere
near the null hypothesis parameter value will be interpreted as evidence in
favor of the null.  In this case, using a prior that assigns a relatively high
probability to the null hypothesis parameter value creates a relatively
_conservative_ test of support for the null hypothesis.

```{r}

b_logit_exp2 <- readRDS("models/b_logit_sup_v_unsup_w_prior.rds")

hyps <-
  b_logit_exp2 %>%
  fixef() %>%
  rownames() %>%
  tibble(beta = .,
         clean_beta = clean_beta(.),
         hyp = glue("{x} = 0", x=.),
         hyp_paren = glue("({x}) = 0", x=.))

hyps_test <-
  hypothesis(b_logit_exp2, hypothesis=hyps$hyp)

format_bf <- function(x) {
  ifelse(
    x > 100, ">100",
    ifelse(
      x < 0.01, "<0.01",
      signif(x, digits=2)
    )
  )
}

hyps_tbl <-
  hyps_test$hypothesis %>%
  inner_join(hyps, by=c("Hypothesis"="hyp_paren")) %>%
  mutate(`BF (β=0)` = format_bf(Evid.Ratio))

```

## Results

```{r expt-2-model-table, results="asis"}

exp2_max_rhat <- format(max(rhat(b_logit_exp2)), digits=3)

ex2_summary <- summary(b_logit_exp2)

fixef_ranef_tables(b_logit_exp2) %>%
  map(left_join, select(hyps_tbl, Coefficient=clean_beta, `BF (β=0)`)) %>%
  multi_kable(caption="Bayesian mixed-effects logistic regression coefficients from Experiment 2, along with Bayes Factors for null hypothesis of β=0") %>%
  kable_styling(full_width = FALSE, bootstrap_options = "condensed")

```

Table 2 shows the fixed effects coefficients and random effects standard
deviations and correlations, along with their 95% credible intervals.  As
before, all $\hat{R} < `r exp2_max_rhat`$ indicating good convergence of the
sampler.  [Figure @fig:expt-2-model-plot] shows the posterior distributions of
the fixed effects, with color indicating the Bayes Factor evidence for or
against the null hypothesis of $\beta = 0$.  The left panel shows the "basic"
fixed effects for exposure distribution, VOT, and trial, along with the
intercept.  Because the default treatment coding was used, these correspond to
the corresponding effects in the supervised conditions (Experiment 2 data).  The
right panel shows the corresponding interactions of the `unsupervised` predictor
with these terms, which correspond to the change in the size or direction of
these effects between Experiment 2 and the unsupervised Experiment 1.  All of
these are clustered around zero, and moreover the Bayes Factors are all greater
than 1, indicating evidence in favor of the null hypothesis of zero differences
between supervised and unsupervised distributional learning in these
experiments.

```{r expt-2-model-plot, fig.width=15.0, fig.height=4.8, fig.cap="Posterior distributions for fixed-effects coefficients in Experiment 2.  Color shows the Bayes Factor from the Savage-Dickey test for the null hypothesis that $\\beta=0$: values greater than 1 (shown in blue) indicate evidence in favor of the null (no effect), while values less than 1 (in red) indicate evidence against the null (any non-zero effect; many very small Bayes Factors have been squished to 0.01 for the purposes of visualization).  Note that VOT and trial are centered (mean 0) and scaled (variance 1).  The left panel shows the effects for all terms not involving the `unsupervised` predictor (compare with effects from Experiment 1 in Figure -@fig:expt-1-model-plot), while the right panel shows the corresponding interactions between these terms and the `unsupervised` predictor (e.g., change in effects between supervised conditions from Experiment 2 and unsupervised Experiment 1)."}

# hypothesis can generate negative values for very small BFs
bf_cleanup <- function(x) {
  ifelse(abs(x) < 1e-10, abs(x), x)
}

b_logit_exp2 %>%
  gather_betas() %>%
  group_by(.variable) %>%
  mutate(p_gt_zero = mean(.value > 0),
         unsup = ifelse(str_detect(.variable, "unsupervised"),
                        "Unsupervised-supervised differences",
                        "Main effects (supervised baseline)")) %>%
  left_join(transmute(hyps_tbl, .variable=factor(clean_beta, levels=levels(.$.variable)), BF=Evid.Ratio)) %>%
  ggplot(aes(x=.value, y=.variable, fill=bf_cleanup(BF))) +
  stat_density_ridges(from=-3, to=5, color="gray50") +
  scale_fill_gradient2("BF(β=0)", midpoint=0,
                       limits=exp(c(-2,1) * log(12.5)),
                       oob = scales::squish,
                       trans = "log10") +
  geom_vline(xintercept = 0, linetype=3) +
  labs(x = "β (log-odds of /p/ response)",
       y = "",
       title = "Regression coefficients from Experiment 2") +
  facet_wrap(.~unsup, scales="free_y")

```

# Experiment 4

As with the previous experiments, the data from the 70-trial test phase was
analyzed with a Bayesian mixed-effects logistic regression via `brms`
[@Burkner2017].  Recall that this test phase followed a 222-trial exposure phase
(as in Experiments 1 and 2), and that the change from exposure to test was
implicit, un-cued, and not in any way easily detectable by the listeners.

The model was identical to that from Experiment 1, with the exception that the
variable coding for exposure condition was called `vot_cond` since it needed to
include both the /b/ and /p/ mean VOT since they varied independently and some
conditions with different /b/ means had the same /p/ mean (and vice versa).  As
before, VOT and trial number were centered and scaled to have unit variance.

```
respP ~ 1 + vot_cond * vot_s * trial_s + (1 + vot_s | subject)
```

One important difference in interpreting the regression coefficients is that the
trial number was with respect to the 70 trials of the *test* phase, rather than
the exposure phase as in Experiments 1 and 2.  Thus, while the main effects and
interactions with `trial_s` in the earlier models indicate learning or build-up
effects, here they capture how listeners behavior changed over the course of the
post-exposure test block.  So the opposite signs of the main effects of VOT
condition (mostly negative, indiciating stronger /b/-bias or higher boundaries
for conditions with higher VOT conditions as expected) and the corresponding VOT
condition-by-trial interactions (mostly positive) means that over the course of
the test phase, the differences between conditions got smaller.  That is, the
opposite signs of the interactions with trial and their corresponding main
effects show evidence of "unlearning", or "re-learning" during the test phase,
where listeners' category boundaries converge after 70 trials of exposure to the
same flat VOT distribution (from -10ms VOT to 50ms VOT).

```{r expt4, results="asis"}

b_logit_exp4 <- readRDS("models/brm_logistic_expt4.rds")

b_logit_exp4 %>%
  fixef_ranef_tables() %>%
  multi_kable(caption = "Bayesian mixed-effects logistic regression coefficients from Experiment 4") %>%
  kable_styling(full_width = FALSE, bootstrap_options = "condensed")

```

```{r expt-4-model-plot, fig.width=9.4, fig.height=5.7, fig.cap="Posterior distributions for fixed-effects coefficients in Experiment 4.  Red indicates coefficients that are reliably greater than zero, and blue indicates reliably less than zero."}

b_logit_exp4 %>%
  gather_betas() %>%
  group_by(.variable) %>%
  mutate(p_gt_zero = mean(.value > 0)) %>%
  ggplot(aes(x=.value, y=.variable, fill=p_gt_zero)) +
  geom_density_ridges(relative_scale = 2) +
  scale_beta_gt0 +
  geom_vline(xintercept = 0, linetype=3) +
  labs(x = "β (log-odds of /p/ response)",
       y = "")

```
