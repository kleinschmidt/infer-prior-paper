
* todos
** follow-up modeling analyses

*** DONE re-analyse results with incremental model

    I've run this model but I haven't looked at the results, buried in
    ~beliefupdatr~ somewhere?  IN particular, want to know what the inferred prior
    looks like.

*** TODO joint fits to expts 1 and 3 (two mean)

    This might be hard because of the different structures ot these
    datasets...1 is just exposure, but 3 has a separate post-test.

*** TODO how much does assuming labels known hurt us?

    run forward simulations based on inferred prior with unsupervised algorithm
    (particles).  In lieu of actually doing the full unsupervised prior inferring,
    see how far off we are.

*** TODO additional models

    control model: just change lapse rate (maybe)

    use production means/variances and just fit confidence.  how well does it
    explain adaptation?

** TODO analyze results with (lapsing) Bayesian logistic regression

   that way I can get good estimates of the boundary uncertainty both at the
   subject level and across conditions/experiments.  Especially important to
   compare between the supervised and unsupervised conditions.

*** DONE fit brms logistic regression to experiments 1 and 2

    include: intercept, vot, supervised/unsupervised, exposure condition (and
    trial?) 

*** DONE extract subject boundaries from logistic regression fits
    [[file:models/brms_regressions.R::expt1_bounds_bysub%20<-][subject boundaries]]

*** DONE plot subject curves and overall curves
    [[file:models/brms_regressions.R::expt1_bounds%20%25>%25][plot at block 3]]
*** TODO fit brms logistic regression to experiment 3

*** TODO (maybe) fix lapsing logistic regression to expts 1, 2, and 3
   
* more literature to consider

** TODO L2 learning

   Baese Berk stuff?  (I think some of that is on dist learning of prevoiced VOT
   categories)

** TODO Diehl papers
** TODO Holt follow-ups to Idemaru and Holt 2011
** Chladkova et al. 2017: unsupervised "recalibration"

   Do lexically driven re-tuning with non-words.  Shifted i/e and good e/i.

* Outline

** Intro
  
*** Lots of flexibility in adult language

    recal, dist learning, etc.

*** what _are_ the constraints?

*** goal

    systematically explore a range of distributions that vary in their similarity
    to a typical talker's distributions; see what (English-speaking) adults
    manage to learn and what they do not.

*** preview

    people have a harder time adapting to distributions that deviate a lot from
    what's typical of American English.  This happens even in the presence of
    labeling information that tells them which VOTs are /b/ and which are /p/.

    The pattern of adaptation across conditions is consistent with statistically
    optimal distributional learning starting from a shared set of prior beliefs,
    and the shared prior beliefs are reasonably similar to what a typical talker
    of American English produces.
    
** Experiment 1
   
   A bunch of VOT distributions which vary in terms of similarity to standard
   American English VOT distributions.  Only going to vary the means of these
   distributions (keep things tractable).

*** Results

*** Discussion    

    Distributional learning is incomplete: in more extreme conditions, produced
    boundaries don't get all the way to the ideal boundary for the exposure
    distributions.  

    Why?

    One possible explanation: "shrinkage" towards a typical talker's boundary.

    Another possible explanation: this is an unsupervised task, so maybe
    listeners aren't picking up on the underlying distributions.  Test that in
    Experiment 2

** Experiment 2

   Same distributions, but half the trials are labeled.  Always one voiced and
   one voiceless, but on labeled trials the options are not minimal pairs, so
   only one is a plausible option.

*** Results

    Are boundaries different with supervision?

    Still don't get all the way there, even with supervised teaching signal.

*** Discussion

    Suggests that the constraint comes from something about the distributions
    themselves.  As above, one possible source is prior experience with a
    typical talker.  Explore this possibility with modeling.

** Modeling

   two questions: is the pattern of adaptation consistent with distributional
   learning starting from a single (shared) set of prior beliefs?  And if so,
   are those shared prior beliefs similar to what a typical talker of American
   English produces?

   Model belief updating process.  Because we didn't find large differences
   between the supervised and unsupervised conditions, we're going to treat this
   as a supervised learning process because it's /much/ simpler to model.


** Experiment 3

   

* notes/snippets

  One of the basic facts about human language is that a neurotypical human
  infant can learn any of the many and varied languages spoken on Earth.  That
  is, the language system is incredibly flexible during development.  However,
  at a certain point people lose the ability to learn other languages to
  native-like proficiency.  Nevertheless, recent research has shown that adults
  still remain the ability to learn new variants of their _native_ language.
  That is, the language system seems to remain plastic at least into adulthood,
  even if the form of this plasticity changes.
  
  One of the most remarkable forms of plasticity in adult speech perception is
  /distributional learning/.  Listeners adjust their representations of phonetic
  categories with mere exposure to distributions of acoustic stimuli.  This
  occurs without explicit instruction or labels.

  Suggests that listeners are closely attuned to the statistical patterns in
  their language environment, and pick up on changes in these contingencies to
  more effectively recover a talker's intended message from noisy, variable, and
  ambiguous acoustic signals.

  A comprehensive understanding of distributional learning requires that we
  understand not only how and when it does happen but also how and when it does
  _not_ happen, or is only incomplete.  That goal of this paper is to
  systematically probe the limits of distributional learning for speech in one
  phonetic system: word-initial stop voicing in American English.


  This paper addresses the /limits/ of distributional learning in adults.  What
  sort of speech statistics can adult speakers of American English /not/ learn?


  
