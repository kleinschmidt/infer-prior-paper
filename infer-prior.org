#+STARTUP: indent

* todos
** TODO prepare for submission
*** DONE title and abstract
*** DONE author note
acknoledge diss committee, funding (Florian R1, GRFP/NRSA)
*** DONE formatting for Cognition
*** TODO final copy-editing
*** TODO cover letter
** DONE finalize data analysis
*** DONE follow-up modeling analyses

**** DONE re-analyse results with incremental model

I've run this model but I haven't looked at the results, buried in
~beliefupdatr~ somewhere?  IN particular, want to know what the inferred prior
looks like.

**** joint fits to expts 1 and 3 (two mean)

This might be hard because of the different structures ot these
datasets...1 is just exposure, but 3 has a separate post-test.

**** how much does assuming labels known hurt us?

run forward simulations based on inferred prior with unsupervised algorithm
(particles).  In lieu of actually doing the full unsupervised prior inferring,
see how far off we are.

**** additional models

control model: just change lapse rate (maybe)

use production means/variances and just fit confidence.  how well does it
explain adaptation?

**** DONE consider only analyzing positive VOT shifts
only have positive shfits for supervised ones...and maybe it simplifies
things somewhat.

***** considerations
the issue is that the data from the other experiment starts to look weird
then since there's no negative VOT shifts...but negative VOTs are weird
anyway so maybe it's best to just stay away from those altogether.

but then it feels a little icky to withold data just because it's
conceptually tricky/makes for a less clean story.  I mean it's probably
fine to just say "okay negative VOTs are weird but leaving that aside let's
see what happens".  THen again it also doesn't go AGAINST any story I'm
trying to tell, it's just not super clear what relevance it has to
anything.

***** Update
[[file:models/inc_model.R::#'%20Okay%20what%20am%20I%20seeing%20here?%20the%20fit%20looks%20_more_%20like%20a%20"scale"%20solution][summary and plots]]

Okay, so I did this.  And the inferred prior changes from a "shift" to a
"scale" (less confidence in variance than mean).  The fit to all five
conditions actually has like 80/20 split in the samples, so it's not
*super* surprising but it's a little troubling that the "headline" changes
so much.

I kind of wish I didn't know this; now I have to decide which to report.
It seems pretty clear that no matter _what_ I'll have to add the caveat
that the fits are different.  But I'm not so sure what to make of the
difference.  Is it actually important?  Just a basic instability in the
model?
    
**** TODO (FUTURE) fit belief updating model to supervised
To be able to make the contrast with aba-ada recal.
*** DONE analyze results with Bayesian logistic regression

that way I can get good estimates of the boundary uncertainty both at the
subject level and across conditions/experiments.  Especially important to
compare between the supervised and unsupervised conditions.

**** DONE fit brms logistic regression to experiments 1 and 2

include: intercept, vot, supervised/unsupervised, exposure condition (and
trial?) 

**** DONE extract subject boundaries from logistic regression fits
[[file:models/brms_regressions.R::expt1_bounds_bysub%20<-][subject boundaries]]

**** DONE plot subject curves and overall curves
[[file:models/brms_regressions.R::expt1_bounds%20%25>%25][plot at block 3]]
*** (maybe) fix lapsing logistic regression to expts 1, 2, and 3
Use mixture distribution in brm.
*** DONE compute savage-dickey bayes factor for sup vs. unsup
can do this using BRMS, as long as you use ~sample_prior=TRUE~.  Then use
~hypothesis(varname = 0)~ etc.  (from this [[https://vuorre.netlify.com/post/2017/03/21/bayes-factors-with-brms/][blog post]])
**** DONE WIP in [[file:models/brms_regressions.R::b_logit_sup_v_unsup_w_prior <- brm(f2_int,][brms_regression.R]]
**** DONE write up in text
** TODO edit intro
mostly done here...need to incorporate a bit more literature I think though.
** DONE modeling ("experiment" 3)
*** DONE Results: write up inferred beliefs.
*** DONE discussion of this experiment
** DONE experiment 4:
*** DONE get data in and plot
*** DONE run BRM analysis
*** DONE generate predictions from model for data
*** DONE write up:
**** DONE intro, results, dicusssion
** DONE write discussion
*** DONE outline structure
*** DONE revise lead in
This maybe could be a lot more concise...dont' necessarily need to
recapitulate the whole series of studies.
*** DONE read over existing discussion materials and make a plan
<2020-01-16 Thu>
I've made a good if messy start on the general discussion, and moved a lot
of materials to the discussion for expt 4.  So I need to go read that and
see what it all needs.
** DONE regression tables for an appendix/supplementary material
** DONE speed of learning in expt 2
just look at whether there's an interaction between trial and supervision.
** TODO editing misc
*** DONE I vs. we
*** DONE what about things babies are bad at/don't lose?
"babies are bad at some things and some things never get constrained, cf clicks
Xhosa, ng (narayan), danish phonemes, so i might back off the claim that
acquisition is unconstrained or give some caveats"
*** DONE when does the "three cluster" stuff come in?
Elika thought that in model, exp 4, and gen disc was too much.
"cut/smooth/etc"

It seems out of place in discussion of modeling results...

I put it in the GD. and pointer in the expt 4 discussion
** DONE quantify model goodness-of-fit
Even if this is something as simple as log-likelihood.  talk about
"quantitative fit" to the data but there's nothing quantitative about it!!
   
Can do LOO for this I think, I think I'm getting something reasonable by just
using ~loo~ on the talker-level log-likelihood.  but need to be careful that
we're comparing apples to apples (I think we are, since I'm not collapsing
across VOTs but not sure...).

Okay I've double checked that and it looks reasonable...trouble is that the
LOO values are maybe too variable to compare across models (and all the
baseline GLM models I've fit have pareto k params that are high so not
great).  but it's just a matter of what baseline model we're using for the
comparison.  the model clearly does a good job in absolute terms, it's just a
question of whether it does a good job relative to particular baselines.
Here are some baselines that seem reasonable:
*** DONE Fit baseline models
- null model (intercept only)
- logistic regression (linear trial effects, like the data analysis)
- lapsing logistic regression (linear trial effects; variable lapse rate)
*** MAYBE fit additional baseline models
Like a static logistic regression sort of model...
*** DONE incorporate into text
overall LL, LOOIC/elpd.

   
** TODO (FUTURE) what does belief updating recover from asymetic dists?
generate belief updating data given asymmetric priors (e.g., realistic
priors) and see whether the model can recover it.  maybe leave this for a
separate paper (with unsupervised learning etc.)
* TODO more literature to consider
** DONE "what is optimal" (benefit/cost/accuracy tradeoff)
@Tavoni2019, related to changes in boundaries contrasted with error rates in
discussion of Experiment 4.
** DONE Cheyenne's dissertation
** DONE Mechanisms of learning
@Harmon2019: follow up of the Idemaru and Holt studies trying to tease apart
error-driven and reinforcement learnig in these supervised cue-reweighting
designs.  They MENTION distributional learning in their overview of
frameworks but don't seem to talk much more about it...
   
they basically rule out dist learning as a mechanism because there's no
modality sensitivity without worrying about whether you'd expect to find
any.  really need to do that supervised vs. unsupervised DL paper...
** Bozena's paper with Roger
Esp. 2014
** Jessamyn Schertz papers
Korean VOT learning etc. @Schertz2016a
** DONE L2 learning

Baese Berk stuff?  (I think some of that is on dist learning of prevoiced VOT
categories)

Japanese r-l
** TODO Diehl papers
** TODO Holt follow-ups to Idemaru and Holt 2011
** DONE Chladkova et al. 2017: unsupervised "recalibration"

Do lexically driven re-tuning with non-words.  Shifted i/e and good e/i.
** DONE Schweinhart, Shafto, and Essock 2017
Adapt to orientation distributions, basically the same finding as Idemaru and
Holt (2011).
** Language learning review anon
Focused I think on learning _non-native_ contrasts via DL...
"""
The literature suggests that adults, too, can track the probability
distributions of sounds in the ambient language (Maye & Gerken, 2001;
Hayes-Harb, 2007; Clayards, Tanenhaus, Aslin, & Jacobs, 2008; Goudbeek,
Cutler, & Smits, 2008; Escudero, Benders, & Wanrooij, 2011). For instance,
Maye and Gerken (2001) showed that listeners exposed to a bimodal
distribution along the voice-onset time continuum between [d] and [d̥ ]
(i.e. prevoiced and voiceless unaspirated alveolar plosives) subsequently
discriminated this non-native contrast better than listeners exposed to a
unimodal distribution on the same continuum. However, a number of recent
studies failed to find the expected distributional training effects (Wanrooij
et al., 2014a; Wanrooij, de Vos, & Boersma, 2015; partly Ong, Burnham, &
Escudero, 2015). In principle, adults are sensitive to distributional
statistics across modalities (Love, 2003; Garrido, Tang, Taylor, Rowe, &
Mattingley, 2016)
"""

I've incorporated some of these refs...
** DONE Cristia meta-analysis of dist learning acquisition papers
this is @Cristia2018a

put this in the intro?  where I talk about the fact that infants and adults
both show evidence of distributional learning
* DONE decide which data goes in this paper
One of the issues I keep coming back to is the question of *which data to
discuss in this paper*.  A priori, my strong preferences is *all of it*.

There are two main sticking points for me:
1. The -10 /b/ VOT condition was added after the fact, and there's only
   unsupervised data on that one.  So if the supervised data is included,
   there's a discrepancy.
2. If the -10 condition is excluded from the belief updating model, the
   inferred prior changes, in a rather dramatic way: switches from a "shift"
   (lower mean confidence) to a "scale" (lower var confidence) solution.

   I actually don't think *1.* is that big of a deal: learning in the large
   positive shifts is also incomplete, so the supervised conditions still provide
   a reasonable test of the hypothesis that what's blocking learning is the lack
   of labels.

   And *2.* is only a problem inasmuch as the goal is to draw strong conclusions
   based on the content of the inferred prior.  I think it's really better as a
   sanity check here: _can_ the constraints we see be explained by belief
   updating based on a common starting point??  Either way the answer is gonna be
   yes (there's no reason that the fits to a restricted subset of the data would
   fit _worse_ on those data than a fit to a superset).

   So there's no reason to give up my prior preferences to include all the data.
   Just need to be honest about how the data was collected and the caveats about
   the interpretation of the inferred prior.  But there are already _lots_ of
   caveates needed there anyway (assuming direct access to the statistical
   properties of the clusters/labels are known, constraints on the confidence
   parameters being equal across categories, etc.).  So.

   *decision*: experiment 1 is all conditions.  experiment 2 is all supervised
   (which doesn't include -10 shift).  modeling is all conditions from
   experiment 1.  experiment 3 is all experiment 3 conditions
* DONE incorporate "Experiment 4"
It's not 100% clear to me whether this study adds anything to the paper, but I
think it does affect my conclusions about the /modeling/ part so it's
important to include.  Specifically, it makes me less confident in the
specific prior parameters that are inferred and suggests that there are
*other* constraints that are not captured by that model...maybe the choice of
prior is not quite right, maybe the form of the input data leads to that bias,
maybe the ... who knows. 

What was the goal of this experiment?  There are two:
1. test what kind of distributional learning happens for even more extreme
   shifts, and shifts that involve only one of the clusters
2. see whether the prior beliefs that are most consistent with the
   distributional learning in experiment 1 can effectively predict the kind of
   learning that happens with very different distributions.  (a strong
   out-of-sample test).

   What have we learned from the experiments thus far?  Constraints on
   distributional learning can't be attributed to lack of knowledge about the
   intended categories...are consistent with belief udpating from prior
   beliefs...what questions remain then?  well one question is: we've looked at a
   rather narrow range of variation in distributions in some respects (equal
   variance, just shifting the means around)...we know people are sensitive to
   other sorts of differences in distributions (like cue reliability)...and
   people might also have prior expectations that don't match the structure we've
   assumed (two clusters, vary independently), because 1. prevoicing
   [@Lisker1964; @Goldrick2013] and 2. correlation between means across category
   [@Chodroff2017; but didn't find any correlation for /b/ and /p/ so...].  So if
   this is the case, then we might expect to find that these prior beliefs DON'T
   capture the real constraints...so let's try a stronger test.
* Outline

** Intro
  
*** Lots of flexibility in adult language

recal, dist learning, etc.

*** what _are_ the constraints?

*** goal

systematically explore a range of distributions that vary in their similarity
to a typical talker's distributions; see what (English-speaking) adults
manage to learn and what they do not.

In this paper I explore a simple hypothesis: people a harder time learning
distributions that are less similar to the distributions that they've
previously encountered.  
    
*** preview

people have a harder time adapting to distributions that deviate a lot from
what's typical of American English.  This happens even in the presence of
labeling information that tells them which VOTs are /b/ and which are /p/.

The pattern of adaptation across conditions is consistent with statistically
optimal distributional learning starting from a shared set of prior beliefs,
and the shared prior beliefs are reasonably similar to what a typical talker
of American English produces.
    
** Experiment 1
   
A bunch of VOT distributions which vary in terms of similarity to standard
American English VOT distributions.  Only going to vary the means of these
distributions (keep things tractable).

*** Results

*** Discussion    

Distributional learning is incomplete: in more extreme conditions, produced
boundaries don't get all the way to the ideal boundary for the exposure
distributions.  

Why?

One possible explanation: "shrinkage" towards a typical talker's boundary.

Another possible explanation: this is an unsupervised task, so maybe
listeners aren't picking up on the underlying distributions.  Test that in
Experiment 2

** Experiment 2

Same distributions, but half the trials are labeled.  Always one voiced and
one voiceless, but on labeled trials the options are not minimal pairs, so
only one is a plausible option.

*** Results

Are boundaries different with supervision?

Still don't get all the way there, even with supervised teaching signal.

*** Discussion

Suggests that the constraint comes from something about the distributions
themselves.  As above, one possible source is prior experience with a
typical talker.  Explore this possibility with modeling.

** Modeling

two questions: is the pattern of adaptation consistent with distributional
Learning starting from a single (shared) set of prior beliefs?  And if so,
are those shared prior beliefs similar to what a typical talker of American
English produces?

Model belief updating process.  Because we didn't find large differences
between the supervised and unsupervised conditions, we're going to treat this
as a supervised learning process because it's /much/ simpler to model.


** Experiment 3

** Discussion   

*** Ideal adapter: learn where to be flexible
Under this interpretation, people don't become less flexible _per se_ as
they enter adulthood.  Rather, they become _smarter_ about where to be
flexible, because this allows them to learn more efficiently.

* notes/snippets

** Another intro
One of the basic facts about human language is that a neurotypical human
infant can learn any of the many and varied languages spoken on Earth.  That
is, the language system is incredibly flexible during development.  However,
at a certain point people lose the ability to learn other languages to
native-like proficiency.  Nevertheless, recent research has shown that adults
still remain the ability to learn new variants of their _native_ language.
That is, the language system seems to remain plastic at least into adulthood,
even if the form of this plasticity changes.
  
One of the most remarkable forms of plasticity in adult speech perception is
/distributional learning/.  Listeners adjust their representations of phonetic
categories with mere exposure to distributions of acoustic stimuli.  This
occurs without explicit instruction or labels.

Suggests that listeners are closely attuned to the statistical patterns in
their language environment, and pick up on changes in these contingencies to
more effectively recover a talker's intended message from noisy, variable, and
ambiguous acoustic signals.

A comprehensive understanding of distributional learning requires that we
understand not only how and when it does happen but also how and when it does
_not_ happen, or is only incomplete.  That goal of this paper is to
systematically probe the limits of distributional learning for speech in one
phonetic system: word-initial stop voicing in American English.


This paper addresses the /limits/ of distributional learning in adults.  What
sort of speech statistics can adult speakers of American English /not/ learn?

** DONE Intro AGAIN
What are all the things in play here?

*** DONE flesh out text here and paste into intro

*** acquisition: lots of flexibility there!
A basic fact of human language is that any typically developing human infant
can learn any human language.  Human languages vary dramatically at every
level, including the basic sound systems they use, and the human language
faculty must be flexible enough to deal with this substantial
cross-linguistic variability.  The first stages of language acquistion are
characterized by initial flexibility, which declines over development as the
particulars of the native language are acquired.  For instance, as infants
become better at discriminating linguistically important sounds in their
native language, they simultaneously /lose/ the ability to discriminate
sounds that are important for other languages but not their native language
[@Best1995; @Kuhl1992; @Werker1984].  Ultimately, people become sufficiently
inflexible over development that they generally struggle to learn another
language in adulthood [@Hartshorne2018].

*** remarkable flexibility in adult language
However, adult listeners still need to deal with substantial variability
/within/ their native language, as talkers differ in how they realize the
phonetic categories of the language using acoustic cues [e.g. @Allen2003;
@Newman2001; @Clopper2005].  Accordingly, adult listeners flexibly adapt to
unfamiliar talkers in a wide variety of contexts.  At one extreme,
perception of heavily accented non-native talkers becomes faster and more
accurate with just a few minutes of exposure [@Clarke-Davidson2004;
@Bradlow2008; @Baese-berk2013].  At the other extreme, listeners recalibrate
representations of individual phonetic categories based on subtle changes in
single segments in otherwise unaccented talkers [@Kraljic2006; @Norris2003;
@Bertelson2003].
    
*** distributional learning
Both acquisition and adaptation have been theorized to be forms of
distributional learning.  First, computational modeling shows that
both acquisition [@McMurray2010; @Vallabha2007; @Feldman2013; but see
@Hitczenko2018] and rapid adaptation [@Kleinschmidt2015b] can be treated as
forms of distributional learning.  At some level, acquisition simply /is/ a
problem of distributional learning, in the sense that ((( computational
level analysis??  it IS a problem of distributional learning )))

Second, both adults and infants are sensitive to distributional properties
of speech.  One set of findings shows that listeners (both infants and
adults) become more sensitive after exposure to a bimodal distribution of an
acoustic cue (like length, voice-onset time, vowel formant frequencies,
etc.) compared with exposure to a unimodal distribution [e.g. @Escudero2011;
Goudbeek2008; @Maye2000; @Maye2002; @Feldman2013b].  Another set of findings
shows that adult listeners can adapt to changes in the means and/or
variances of the cue distributions for known phonetic categories [e.g.,
@Clayards2008; @Theodore2015; @Theodore2019; @Colby2018; @Chladkova2017].
What both of these sets of findings have in common is that listeners pick up
on the distributions of cues without any explicit instruction about the
itended category label associated with each token.  For example,
@Clayards2008 had listeners listen to /b/-/p/ minimal pair words (e.g.,
"beach/peach") with different voice-onset times (VOT), and click on a
matching picture to indicate which member of the minimal pair they heard.
On every trial, the VOT was drawn from one of two bimodal distributions,
which had clusters with the same means but different variances across
subjects.  Listeners in the high-variance condition produced shallower
categorization functions, reflecting greater uncertainty associated with the
wider range of VOTs they heard for each cluster.
    
*** clear constraints on what adults can learn
If both acquisition and adaptation can be treated as forms of distributional
learning, and both infants and adults are sensitive to distributional
information, what distinguishes acquisition from adaptation?  For one, it
seems that distributional learning in adults is /constrained/.  Adult
listeners struggle to learn new categories that are not present in their
native language.  For instance, Japanese listeners struggle to discriminate
the English /r/-/l/ contrast, which corresponds a single category in their
native language [@Goto1971; @Miyawaki1975].  Long-term naturalistic exposure
is not sufficient to achieve good discrimination of this contrast, even
after convserational competence has been achieved [@Takagi1995].  While
perception of this contrast can be improved somewhat by training, it
requries extensive training and these listeners seldom achieve native-like
levels of performance [@Bradlow1997].

There are also apparent constraints on the ability of adult listeners to
adapt to variations in the distributions associated with native language
categories.  For instance, @Idemaru2011 tested how well listeners adapt to
distributions of two cues that distinguish voicing (e.g., /b/ vs. /p/),
voice onset time (VOT, the primary cue to voicing) and the pitch of the
following vowel (f0, a secondary cue).  These two cues are typically
positively correlated in English, with /p/ corresponding to high VOT and
high f0, and /b/ to low values of both cues [@Kohler1982]. In one condition,
listeners were exposed to a talker who produced a positively correlated
distribution of these cues. During a post-test, these listeners used f0 to
categorize stops with ambiguous VOTs. In another condition, listeners heard
a talker who produced an *un*correlated distribution, where f0 is
uninformative. In contrast to the listeners in the first condition, during
the post test these listeners _ignored_ f0 even for ambiguous VOTs. This
effect is consistent with the idea that listeners are rationally integrating
multiple cues to voicing, weighing them based on how informative they are
[@Ernst2004; @Clayards2008; @Bejjanki2011]. However, listeners in a third
condition who were exposed to a talker who produced an *anti*correlated
distribution did _not_ follow the predictions of rational cue integration.
Despite the fact that f0 was just as informative for this accent as for the
positively correlated accent, listeners _ignored_ f0 as a cue to
voicing. This suggests that these listeners have ruled out the possibility
of a reversed mapping between f0 and voicing (/b/ vs. /p/), possibly perhaps
American English talkers typically do not typically produce it [e.g.,
@House1953].  Likewise, @Sumner2011 found that listeners had trouble
adapting to a talker who produced VOT distributions for /b/ and /p/ that had
_substantially_ lower means (approximately -60ms and 0ms, respectively) than
a typical talker [approximately 0--10ms and 60ms VOT; @Lisker1964].

*** goals: systematically probe constraints on distributional learning 
So on the one hand, distributional learning provides a unifying theoretical
perspective on flexibility in language acquisition and adaptation.  On the
other hand, it highlights an important difference between these two basic
kinds of plasticity in the language system.  From the lens of distributional
learning, one major difference between acquisition and adaptation is that
distributional learning in adulthood appears to be /constrained/, while it
is relatively /unconstrained/ during acquisition.

However, we lack a clear understanding of the nature and source of those
constraints.  There are a number of other differences between the learning
problems posed by acquisition and adaptation, even if both are forms of
distributional learning.  First, distributional learning in infancy is, at
least initially, almost entirely unsupervised, meaning that there is very
little information about whether any two observed acoustic cue values come
from the same cluster (category) or different ones.  Adults have a great
deal of circumstantial evidence from the lexicon, pragmatic context,
phonotactics, etc. which provides /some/ information about the intended
category for a particular cue value.  This makes the distributional 
problem of adaptation at least semi-supervised.  
# where does the constraint come from here?
Second, when adapting to an
unfamiliar talker, adults have a great deal of prior experience with /other/
talkers which they could use to narrow down the possible distributions they
ought to expect [@Kleinschmidt2015].  Both of these factors might contribute
to constraints on adult distributional learning.  For the first, if adult
adaptation typically operates in a /supervised/ setting, the fully
unsupervised setting of a typical distributional learning experiment might
not provide enough information, leading to reduced learning.  For the
second, if the distributions encountered in an experiment fall far enough
outside the range of what a listener expects based on their prior
experience, they may struggle to adapt [@Kleinschmidt2015].

The goal of this paper is to systematically probe the constraints on
distributional learning in American English-speaking adults.  Experiment 1
tests the ability of American English listeners to change their
classification of word initial stop voicing based on experience with a range
of distributions of voice-onset time (VOT).  I find that distributional
learning is more complete when the experimental distributions are more
similar to those of a typical American English talker, suggesting that prior
experience with other talkers may constrain distributional learning.
Experiment 2 tests another possible constraint on distributional learning,
which is the absence of /labels/, which could lead to uncertainty about
whether the bimodal distribution really corresponds to the standard English
categories of voiced and voiceless stops of /b/ and /p/.  Surprisingly,
telling listeners whether a particular VOT was intended to be a voiced /b/
or a voiceless /p/ on half of the trials has no effect on the speed or
completeness of distributional learning.  Experiment 3 uses a Bayesian
belief-updating model to test whether the constraints observed in Experiment
1 can be explained as belief updating starting from a common set of prior
beliefs that is shared by all of the subjects.

Together, these results show that distributional learning in adults /is/
constrained, and these constraints are at least consistent with belief
updating starting from a set of prior beliefs about the VOT distributions
that a typical talker of American English will produce.
    
**** preview
Developmental trajectory may better be thought of as a *change* in the kind
of flexibility that listeners have, rather than a *loss* of plasticity.

** for discussion
are these results anti-bayesian?  no...learning is inference, guided by prior
experience.

*** need to balance stability and flexibility
if you are too stable, can't deal with changes (e.g., unfamiliar talkers and
accents).  but if you're too flexible, then you end up overly sensitive to
meaningless variation that doesn't predict what's going to happen in the
future.

I think this is better for the *discussion*: why are these constraints
there?  For the introduction we want at most to gesture towards this
story...

*** cline between language learning and adaptation
cite @Pajak2016: hierarchical inference under uncertainty.

* Feedback 
** from Meghan

#+begin_src text
     I read through the paper and I enjoyed reading it. I have some minor comments
     attached. Two other minor points is that you might want to cite Cheyene Munson's
     thesis (attached) for evidence of shifts in distributions leading to shifts in
     boundaries. Also Schrieber, Onishi & Clayards (2013) was the first case of using
     the paradigm that Colby, Clayards & Baum used so it would be good to cite that
     since it's not the same co-authors. My only major point (using my reviewer's
     hat) would be that there isn't any discussion of alternative frameworks or
     learning models. This makes it harder to evaluate how much weight to put on the
     fact that the data are compatible with a bayesian belief updating model. its
     probably outside the scope of this paper to evaluate exactly what an error
     driven learning model would look like or reinforcement learning or whatever, but
     maybe you can at least point to that literature? I assume you know about this
     paper: Harmon, Z., Idemaru, K., & Kapatsinski, V. (2019). Learning
     mechanisms in cue reweighting. Cognition, 189, 76-88.
#+end_src
** from Elika

#+begin_src text 

   ,* seems like you want to allude to the ‘unsupervised’ nature of dl early in
     intro but don’t say that directly until a bit later

   ,* some typos in text and cut off things in fig’s throughout, just fyi, i’m not
     listing them

   ,* notion of training vs. life experience doesn't quite get captured (what you're
     doing in your study vs. what life learning is like)--you do come back to this
     a bit with exp4 and disc, but i might plant a see od if in intro

   ,* babies are bad at some things and some things never get constrained, cf clicks
     Xhosa, ng (narayan), danish phonemes, so i might back off the claim that
     acquisition is unconstrained or give some caveats

✔  ,* dan swingley and eric theissen and noami f would all argue i think that the
     WORD(even if you don't understand it) it occurs is acts as a cue so “there is
     very little information about whether any two observed acoustic cue values
     come from the same cluster (category) or different ones “ is a little hard to
     say

   ,* in case relevant, seedlings infants get the vast majority of their input from
     just mom/ mom+dad so they don’t actually have TONS of cross talker experience
     when they’re showing good phonemic rep’s

   ,* feels a little funny to have a mini results summary overall before exp 1

   ,* intro in general reads quite smoothly and clearly!



✔  ,*fig 3: you never tell us what the colored/black dotted lines are or what the
    dot is or what the traces are exactly (you can sort this out from the following
    figure a bit though)

   ,* i vs. we: make a choice…feels like a style/register shift when you get to
     experiments, can’t put my finger on it..

✔  ,* this feels funnily worded “what I intended listeners to treat as ...“

   ,* exp2: clever design!

   ,* can you say something slightly bland and stats-y to assuage readers’ fears
     that you’ve collapsed the two labeling conditions?

   ,* i don’t quite follow the rationale for why -10ms,30ms was dropped.  e.g. which
     set of data pilot vs. current are you talking about in the last sentence of
     the -10ms,30ms cond pg.7

✔  ,* why bigger N for exp 2 (esp with fewer cond’s?)

   ,* ooh rpackage nice

✔  ,* awk wording: “to guide their responses on that trial. “

   ,* exp 2: i found the outlier description a little confusing; could you put those
     points on your graph separately?do the results change if you actually remove
     them? oh i see they are the 3 more horizontal lines on fig 6 purple panel;
     this wasn’t immediately clear to me

   ,* sometimes helpful to include model specification in pseudocode; some ppl like
     to see model estimates in table or graph, i’m agnostic bc i get way more out
     of the graphs you have an believe you did the rest right, but others may be
     more skeptical than me:)

   ,* haha you’ve anthropomorphized your model “an impossibly large number to
     consider. “

✔  ,* “can be computed analytically “ as opposed to?

✔  ,* after the last sentence in ‘model fitting procedure’ you could gently remind
     people that they’re welcome to do whatever they like with the code and dat you
     provide in they want to take a different tack with REs

   ,* the modeling section was a little hard to follow for me, as a non-modeler, so
     depending on your audience that comment may be irrelevant. but a take-home
     sentence at the end of teach subsection of results for exp 3 might help. also
     you spent a lot more time talking about the learning pattern for the model
     than within the exp 1 and 2 where you just say ‘we’re looking at the 5/6 mark
     and thereafter’; this may be just fine, but flagging it in case you want to
     foreshadow above that you’ll come back to that in exp 3

   ,* oy, do you really wanna through the ‘there are 3 clusters’ stuff in there?
     feels like it muddies the waters a bit to bring it up at this late stage (plus
     psychological reality for Ss, effect of their being to ‘letters’, etc.?) oh
     actually you mention this in two places, exp 4 and exp 3 disc,…uh you also
     mention it again in the GenDisc, cut/smooth/etc.

✔  ,* you only warned readers about 3 experiments, tbh i got a little bleary-eyed
     trying to keep straight what was going on by exp 4. but i think there’s some
     redundancy in discussion for this study and the setup (e.g. the role of
     testing midway)

✔  ,* this is a little trippy for me in the context of your stems: “In order to
     effectively adapt to an unfamiliar talker’s accent, a listener needs to have
     some reasonably good estimate of the amount and kind of talker variability
     they should expect, which is directly related to the distribution of talkers’
     accents8 that exist in their environment “ bc you’re not really altering the
     categories in ways that actual talkers (or those with a particular accent do),
     right? i’d smooth that point or acknowledge it or something

   ,* pretty unrelated but remind me to tell you about my postdoc Federica’s f32
     which is centered on between vs. within talker variability in our corpus and
     in studies with 8 month olds in the lab

   ,* flow-wise: i might do exp 1:3 and then the model

   ,* also, tbh, the model could be it’s own separate paper, giving you two
     shorter-sweeter-more-focused papers, but i see why you might not want to do
     that

   ,* don’t end on a caveat pre-conclusion, end on a strength of what you’ve done
     that we didn’t know before!
#+end_src


#+begin_example

#+end_example
** Reviews
*** Initial submission

Letter:

#+begin_quote
Dear Dr. Kleinschmidt,

I have received reviews from two expert reviewers and I have read your paper
myself with interest. The topic addressed here is an interesting one, and the
results are intriguing. However, the reviewers identified multiple points of
weakness in the paper that need to be addressed in a revision.

Both reviewers clearly spent a lot of time with your paper to generate these
thoughtful reviews. They both offer constructive criticism for how to tighten
and clarify the theoretical framing of the paper to illustrate the novelty and
impact. Better engagement with the existing literature, including how the
present findings contrast with different theoretical perspectives, will go a
long way here. The reviewers also ask for clarifications regarding
methodological choices. There are a number of other details the reviewers raise
that I will not repeat here but that need to be addressed. I encourage you to
take their advice into account as you revise as demonstrating clear and strong
theoretical impact will be critical to a successful revision.

If and when you submit the revised manuscript, please include a letter with a
detailed response to reviews in which you respond point-by-point to each of the
reviewers’ comments, explaining how (or why not) you addressed each comment in
the revised manuscript.

Please bear in mind the following standard caveat if and when you revise the
paper: Inviting resubmission does not entail that the next version, or any
subsequent version, will be accepted for publication. It is a policy at
Cognition to avoid a protracted editorial process that may in any case end,
eventually, in rejection. Moreover — and this is a standard caveat — the
clarifications that result from the revision may reveal new issues that would
subsequently preclude publication.

Sincerely, Sarah Brown-Schmidt Associate Editor Cognition


To submit a revision, please go to https://www.editorialmanager.com/cognit/ and
login as an Author.

Your username is: dave.f.kleinschmidt@gmail.com

If you need to retrieve password details, please go to:
https://www.editorialmanager.com/cognit/l.asp?i=343655&l=81FGB13N

NOTE: Upon submitting your revised manuscript, please upload the source files
for your article. We cannot accommodate PDF manuscript files for production
purposes. We also ask that when submitting your revision, you follow the journal
formatting guidelines. For additional details regarding acceptable file formats,
please refer to the Guide for Authors at:
http://www.elsevier.com/journals/Cognition/0010-0277/guide-for-authors

When submitting your revised paper, we ask that you include the following items:

Response to Reviewers (mandatory)

This should be a separate file labeled "Response to Reviewers" that carefully
addresses, point-by-point, the issues raised in the comments appended below. You
should also include a suitable rebuttal to any specific request for change that
you have not made. Mention the page, paragraph, and line number of any revisions
that are made.

Manuscript and Figure Source Files (mandatory)

We cannot accommodate PDF manuscript files for production purposes. We also ask
that when submitting your revision you follow the journal formatting
guidelines. Figures and tables may be embedded within the source file for the
submission as long as they are of sufficient resolution for Production.For any
figure that cannot be embedded within the source file (such as *.PSD Photoshop
files), the original figure needs to be uploaded separately. Refer to the Guide
for Authors for additional information.
http://www.elsevier.com/journals/Cognition/0010-0277/guide-for-authors

Data References (mandatory)

This journal requires you to cite underlying or relevant datasets in your
manuscript by citing them in your text and including a data reference in your
Reference List. Data references should include the following elements: author
name(s), dataset title, data repository, version (where available), year, and
global persistent identifier. Add [dataset] immediately before the reference so
we can properly identify it as a data reference. The [dataset] identifier will
not appear in your published article. See the Data Archiving Policy here
https://www.elsevier.com/journals/cognition/0010-0277/guide-for-authors

On your Main Menu page is a folder entitled "Submissions Needing Revision". You
will find your submission record there.

For further assistance, please visit our customer support site at
http://help.elsevier.com/app/answers/list/p/7923. Here you can search for
solutions on a range of topics, find answers to frequently asked questions and
learn more about EM via interactive tutorials. You will also find our 24/7
support contact details should you need any further assistance from one of our
customer support representatives.

Include interactive data visualizations in your publication and let your readers
interact and engage more closely with your research. Follow the instructions
here: https://www.elsevier.com/authors/author-services/data-visualization to
find out about available data visualization options and how to include them with
your article.

Yours sincerely,

Sarah Brown-Schmidt, PhD Associate Editor Cognition

Reviewers' comments:

Reviewer #1: This paper examines adult adaptation to VOT distributions of /b/
and /p/ and the role of prior linguistic experience in constraining that
adaptation. The paper includes three behavioral experiments of VOT adaptation
and a computational model of these results. The results are largely discussed in
terms of distributional learning — a process that is shared between adults and
children, but still differs with respect to the degree of constraints or
supervision in the process. That is, child language acquisition involves
unsupervised distributional learning, whereas adult adaptation involves
supervised distributional learning. Overall, I thought the paper was
well-written and the statistical analyses quite sophisticated. The computational
Bayesian modeling section also provides a nice proof of concept as to the
importance of prior experience. I was less convinced by the impact and novelty
of the paper, and did not fully understand the motivations for some of the
experiments. With further revision, I may be otherwise convinced about the
overall contribution of the study and model.

It would be very helpful to clearly situate this paper's contribution in the
distributional learning literature. This may help establish the contribution and
novelty. At the moment, the paper comes across as almost too high level: "prior
experience constrains distributional learning" and "distributional learning is
necessary for acquisition and adaptation". I think there are three branches of
issues to address here: 1) more engagement with what is meant by prior
experience, 2) clear differentiation from previous distributional learning
experiments, and 3) is it really saying much to equate acquisition with
adaptation when the difference between supervised vs unsupervised learning is so
massive?

With respect to #1 and #3 (more engagement with what is meant by prior
experience, and relating adult distributional learning to child acquisition),
the paper currently dances around what the constraints are. I think it would be
beneficial to discuss how phonological structure and the phonetic instantiation
of that likely constrains adult learning. That is, there seem to be a finite set
of categories and limited variation within and between those categories.

Having clearly defined constraints on the distributions is a key aspect of what
differentiates child acquisition from adult adaptation. It seemed to me that
understanding the constraints and therefore the constitution of the prior was a
main goal of the paper. The discussion of how this relates to child acquisition
seemed slightly distracting and a little lofty (though perhaps with other
details this point could stay in).

With respect to #2, the paper does begin to situate itself, but the paper's
contribution and difference from previous VOT adaptation, recalibration and
distributional learning papers could be made more explicit, in both the
behavioral and computational domains. Relatedly, how does this model differ from
that of Kleinschmidt & Jaeger (2015)?

Another aspect of the paper which should be addressed is the current order of
experiments and transitions between them. The paper currently presents three
behavioral experiments and one cognitive model in which the cognitive model
occurs third in order, but then was discussed briefly again in the final
experiment. I would recommend introducing the model either first or last.

Relatedly, while I followed the motivation for the first experiment of
adaptation, I did not fully follow the motivations for Experiment 2 (labeled)
and Experiment 4 (lead VOT vs long lag VOT). For Experiment 2, if participants
were simply labeling according to the typical talker's distribution, wouldn't
you get a flat response curve? Why isn't that ruled out by the data already? For
Experiment 4, I can at least understand at a high level why one might want to
understand adaptation to lead vs long lag VOT, but I did not fully grasp how
this related to a specific research question.

In addition, while there is a section on this all the way at the end, I still
had some questions on the methodological differences between Experiment 1
and 4. I wonder if it might help to raise these differences earlier; I think
these differences may have also contributed to my confusion about why these
experiments were grouped together. My main questions involved how the VOT
boundaries were estimated in each of the experiments? Specifically, is it the
case that in Experiment 1 they were never tested on ambiguous VOTs? How then do
you know the boundary has shifted? If this is just an estimated boundary, which
I believe it is, then will it look like the boundary is higher for people in the
long condition just because those people are getting more long VOT values than
people in the short VOT conditions?

Other comments and questions Experiment 1
- p. 11: clarify what is meant by 'each condition'
- I also find it very interesting that listeners don't fully adapt but rather
  undershoot, but this might also make sense because if a speaker were to make a
  phonologically voiced stop greater than 20 ms or perhaps 35 ms, it would be
  articulatorily different, and namely, aspirated. This could be a hard
  articulatory constraint on the phonologically voiced stops that they cannot be
  aspirated. Alternatively, we would have to consider the possibility that
  languages could theoretically distinguish mildly aspirated with strongly
  aspirated stops. I don't think such a language exists, so it might be more
  likely to consider the case phonologically voiced stops simply can't be
  aspirated and perhaps a listener is aware of this.
- The undershoot with respect to the low VOT boundary is reminiscent of Nielsen
  (2007: Implicit phonetic imitation is constrained by phonemic contrast, ICPhS)
  in which phonetic imitation of voiceless stops might also have a hard lower
  boundary.
- Figure 1: Typical talker / Exposure talker labels confusing. I might suggest
  removing the labels from the panel and instead writing out what the lines and
  histograms are in the caption.
- p. 13 Why use the 5/6 point of the 222 total trials?
- Relatedly, how many ambiguous VOTs (e.g., btw. 20 and 40 ms) are they actually
  responding to?

Experiment 2
- as clarification, how were the labeled and unlabeled trials mixed together, if
  at all?
- no beta coefficients?

Experiment 3 - model
- clarification: where does 10^60 come from?
- p. 26: what is meant by a constant probability of responding /p/ for the null
  model: what is that probability? how is it chosen?
- Doesn't Experiment 2 suggest that the guessing rate is pretty low? The
  ambiguous VOTs might involve some guessing (which gets back to the question of
  how many truly ambiguous VOTs are people actually hearing?).
- Can you elaborate on the baseline model? What was the training data, and is
  the difference the lack of a specified prior?
- It would be helpful for understanding to create a figure of the models and
  include a description of their predicted behaviors.
- p. 27: description of parameter differences in parentheses could be
  highlighted earlier on in the section
- Figure 8: I'm not seeing the ribbons here. Are the points the observed data?
- p. 32 - 33: how is the expected prior pseudo count calculated?
- p. 33: where are the MCMC samples coming from? More background/explanation
  needs to be provided here
- Can you elaborate on Kronrod et al. (2016), and provide what the estimated
  parameters were
- For clarification, note that the ada/aba contrast is made in formant
  transitions (end of p. 36)

Experiment 4
- See comment about distinguishing methodology above

General Discussion
- why not implement the fully ideal adapter model that classifies and adapts
  tokens?
- This question might relate to the one on methodology of boundary estimation,
  but how much stock should we be putting in to the meaning of this boundary for
  understanding internal category structure? Some previous studies have also
  used goodness ratings (e.g., Miller, 1994: On the internal structure of
  phonetic categories: A progress report, Cognition) to demonstrate that even if
  a boundary does not change, a listener's representation of the category
  nevertheless changes for a given talker. This could also be the case for some
  of your listeners when no boundary shift was detected.

Typos
- p. 13: some typo with fig. 3 and fig. 4
- section 3.3. typo: unlabeled
- p. 25: directly/approximately seems contradictory
- p. 33: typo: "along with ."
- p. 45: missing period at end
- p. 47: "a final Experiment 4"





Reviewer #2: I'm happy to review "What constrains distributional learning for
adults?" for Cognition. This manuscript presents the results of four experiments
that aim to identify factors that influence the degree to which listeners modify
the mapping to speech sounds given exposure to statistical (i.e.,
distributional) regularities in speech input. Specifically, this work tests the
hypothesis that distributional learning in adults is fundamentally constrained
by prior experience with the cue-sound mappings produced by other talkers.

In experiment 1, listeners were exposed to one of five "accents." Each accent
consisted of bimodal VOT input (VOTs specifying the /b/ and /p/ categories);
across accents, the VOT input was shifted in VOT space to have relatively
shorter, relatively longer, or relatively consistent values with a "typical"
talker. Listeners heard tokens from their respective accent in the context of a
category identification task in which they picked which of two minimal-pair
pictures (i.e., picture of beach or peach) matched the auditory stimulus on each
trial. The results (1) showed evidence of distributional learning in that the
category boundary between listener groups differed in line with the exposure
accent and (2) evidence of incomplete adaptation in that listeners' category
boundaries fell intermediate to those expected for a typical talker and the
boundary that would optimally separate the /b/ and /p/ categories for each
accent.

Experiment 2 was similar to experiment 1 except that only four accents were
tested and the picture mapping protocol was modified to provide supervision on
learning for half of the trials. Supervision was provided in the form of label
information, implemented by modifying the two picture response options on each
trial to only have one picture consistent with the continuum stimulus heard on
that trial (e.g., pictures of beach/peas instead of beach/peach or
bees/peas). The results of experiment 2 patterned as experiment 1, and there was
no evidence to suggest that learning was differentially influenced by the
inclusion of disambiguating label information on half of the trials.

Experiment 3 showed used a computational model to show that both the learning
outcome and learning curves (i.e., learning over time) observed in experiments 1
and 2 could be captured by a belief-updating framework in which listeners
adapt/learning from a shared set of prior beliefs (i.e., expectations of a
typical talker) - at least for the specific parameters used to instantiate the
model tested here.

Experiment 4 followed a design to experiment 1, with three key
differences. First, some of the accents showed more substantial deviation from a
typical talker (by presenting negative VOTs for the voiced category, i.e.,
prevoicing). Second, the means of the /b/ and /p/ categories varied
independently from that of a typical talker (e.g., an accent could have VOTs for
/b/ that were substantially shorter without concomitant substantially shorter
VOTs for /p/). Third, learning was assessed independent of exposure trials by
the inclusion of 70 trials after exposure consisting of a flat frequency
distribution of VOTs that were novel for most of the accent conditions. The
behavioral results showed diminished distributional learning for the accents
that most deviated from the typical talker (as compared to predicted learning
from the model tested in experiment 3).

My evaluation is that this work is extremely well-executed from a methodological
standpoint, clearly grounded in theory, and presents stable results that
subsequently advance theory by pinpointing constraints on distributional
learning in adults (namely, their a priori expectations of cue-sound
relationships formed by extensive experience with other talkers' speech). The
manuscript is extremely well-written. I learned a lot from the carefully crafted
introduction and discussion, and was pleased to see such care taken to highlight
potential limitations of the belief-updating model tested here and exciting
avenues for future research. As is reads now, this work will make an important
contribution to the literature and I'm excited to see it in print. A few minor
comments that came to mind in reading this:

1. A table that shows the exact number of included participants in each accent
   condition for each experiment would be very useful to the reader. Also, some
   context on the sample sizes would be useful. Were these based on a priori
   power analyses? Were these samples of convention/convenience?

2. Regarding the implementation of semi-supervised learning: The use of labels
   here was really interesting in that the labels provided information only for
   the non-critical portion of the stimulus. That is, the disambiguating
   information concerned the vowel+coda portion (e.g., /iz/ for stimuli drawn
   from the /biz/-/piz/ continuum). Supervision could of course take many forms
   (e.g., feedback after response) and I wonder why this type of supervision was
   implemented given that it could be implicitly down-weighting
   attention/sensitivity to word-initial VOT. Put to the extreme, I wouldn't be
   surprised if accuracy on labeled trials was at ceiling in this paradigm even
   if the VOT + (say) 10 ms of the vowel were masked or, perhaps, even if the
   VOT + entire vowel portion were masked. I suppose the current data provide
   some evidence that intial VOTs weren't completely ignored (otherwise the
   learning in experiment 2 would be less than, not equal to, learning in
   experiment 1). More context on the hypothesized mechanism at work for this
   type of supervision (i.e., labels that disambiguate based on aspects of the
   signal tertiary to the manipulated input) and a discussion of the potential
   role for other types of supervisory signals (e.g., feedback) would be useful.

3. I can see the utility in analyzing experiment 2 data only for non-labeled
   trials, but I also think that the reader would benefit from learning about
   the results when all trials are included. If this analysis were presented
   first in experiment 2, then the report of labels driving responses 98% of the
   time (which is what I imagine you expected them to do) logically suggests
   seeing whether the same patterns are observed only for trials in which labels
   were not provided.

4. Regarding experiment 4 (which is a super exciting and very clever
   manipulation, by the way, great to know that learning in this paradigm can be
   tested independently of exposure!): Can you confirm that the 10 repetitions
   of the 7 VOTs were presented in cycles (e.g., 1 repetition of all 7, then a
   second repetition of all 7, and so on)? I'm guessing this is the case, but I
   didn't see it stated explicitly. My understanding is that the model used for
   analyzing the behavioral data include time as a fixed effect in order to
   evaluate whether learning from exposure was attenuated during the 70 test
   trials (given exposure to the flat frequency distribution at test). But I
   didn't see any formal report of whether that was in fact the case in the main
   text (it was alluded to as supplementary material, but I was not able to
   download that for some reason). The decision to make conclusions in
   experiment 4 based on derived boundaries 1/6 of the way through the test
   trials would be strengthened by empirical support that additional learning
   (or unlearning) is occurring during the 70 test trials.

5. Regarding the null effects of supervision in experiment 2, I'd be keen to
   hear speculation regarding potential interactions between supervision and
   specific input distributions to be learned. For example, no supervision was
   provided in experiment 4, which presented the most extreme accents (for a few
   reasons). Might the role of supervision have been different for these input
   distributions? It may be the case that supervisory signals have a different
   role depending on the exact nature of the to-be-learned signal that reflects
   graded contributions of previous experience/expectations and online evidence.

Thank you for the opportunity to read this important paper. I believe that the
few minor points raised above could be addressed very straightforwardly and
doing so would make me even more excited to see this work in print.

Signed: Rachel M. Theodore
#+end_quote

**** R1
Primary concern is with /impact and novelty/, motivations for experiments.

"clearly situate contribution in the dl literature".  comes across as too high
level.

three issues:
1. more engagement with what is meant by prior experience 
2. clear differentiation from previous distributional learning experiments
3. is it really saying much to equate acquisition with adaptation when the
   difference between supervised vs unsupervised learning is so massive?

"I think it would be beneficial to discuss how *phonological structure* and the
*phonetic instantiation* of that likely constrains adult learning"

order and motivation of experiments, "recommend either introducing the model
first or last"

methodological issues with how bounaries are estimated (are there actual
ambiguous stimuli tested, and how many; 

***** TODO survey sources of constraint
what's the role of phonetic/phonological structure (number of categories, types
of cues, etc.)

look up the Cho paper on VOT and see if there are langauges that violate that
"partially aspirated" constraint (like Seoul Korean)?  And the chodroff and
wilson to get a sense of the variation across talkers

****** Schertz and Kang (2016)
production study with a couple dozen speakers from Seoul and Chinese Korean.
Find that fortis stop is intermediate between lenis and aspirated in terms of
VOT. and that these /production/ differences are reflected in their /perception/
as well (see also Schertz et al. 2016, APP)

***** TODO differeniate from previous distributional learning experiments
"in both the behavioral and computational domains".

#+begin_quote
It would be very helpful to clearly situate this paper's contribution in the
distributional learning literature. This may help establish the contribution and
novelty. At the moment, the paper comes across as almost too high level: "prior
experience constrains distributional learning" and "distributional learning is
necessary for acquisition and adaptation". I think there are three branches of
issues to address here: 1) more engagement with what is meant by prior
experience, 2) clear differentiation from previous distributional learning
experiments, and 3) is it really saying much to equate acquisition with
adaptation when the difference between supervised vs unsupervised learning is so
massive?
#+end_quote

***** TODO clarify motivation for later experiments

***** TODO clarify methodology:
boundary estimation, experiment 4 vs. earlier experiments, number of ambiguous
trials.

***** TODO ordering of sections
'recommend model either first or last' but I'm trying to minimize dependency
length.

***** TODO misc stuff
**** R2 
overall positive...

***** TODO Include table for subject counts in each experiment

***** TODO discuss type/source of feedback
explicit vs. implicit.  does it lead to down-weighting of the VOT?  (no,
empirically, but also you don't know trial-to-trial what is going to be
important).

***** TODO analyze labeled data too in expt 2?
no because you'd end up with boundaries that exactly match the input
distributions (or have to throw in an interaction term) so it's just going to
confuse things I think...

***** TODO experiment 4 methods/unlearning
how are trials distributed during posttest?

***** TODO interaction between supervision and distributions
yes I'd have thought so too but the distributions from Expt 1 are pretty extreme
too.  Also see that sumner paper: even with supervision you still don't get
shifts!  need a specific order of trials.
