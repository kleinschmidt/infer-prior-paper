
* todos
** follow-up modeling analyses

*** DONE re-analyse results with incremental model

    I've run this model but I haven't looked at the results, buried in
    ~beliefupdatr~ somewhere?  IN particular, want to know what the inferred prior
    looks like.

*** joint fits to expts 1 and 3 (two mean)

    This might be hard because of the different structures ot these
    datasets...1 is just exposure, but 3 has a separate post-test.

*** how much does assuming labels known hurt us?

    run forward simulations based on inferred prior with unsupervised algorithm
    (particles).  In lieu of actually doing the full unsupervised prior inferring,
    see how far off we are.

*** additional models

    control model: just change lapse rate (maybe)

    use production means/variances and just fit confidence.  how well does it
    explain adaptation?

*** DONE consider only analyzing positive VOT shifts
    only have positive shfits for supervised ones...and maybe it simplifies
    things somewhat.

**** considerations
     the issue is that the data from the other experiment starts to look weird
     then since there's no negative VOT shifts...but negative VOTs are weird
     anyway so maybe it's best to just stay away from those altogether.

     but then it feels a little icky to withold data just because it's
     conceptually tricky/makes for a less clean story.  I mean it's probably
     fine to just say "okay negative VOTs are weird but leaving that aside let's
     see what happens".  THen again it also doesn't go AGAINST any story I'm
     trying to tell, it's just not super clear what relevance it has to
     anything.

**** Update
     [[file:models/inc_model.R::#'%20Okay%20what%20am%20I%20seeing%20here?%20the%20fit%20looks%20_more_%20like%20a%20"scale"%20solution][summary and plots]]

     Okay, so I did this.  And the inferred prior changes from a "shift" to a
     "scale" (less confidence in variance than mean).  The fit to all five
     conditions actually has like 80/20 split in the samples, so it's not
     *super* surprising but it's a little troubling that the "headline" changes
     so much.

     I kind of wish I didn't know this; now I have to decide which to report.
     It seems pretty clear that no matter _what_ I'll have to add the caveat
     that the fits are different.  But I'm not so sure what to make of the
     difference.  Is it actually important?  Just a basic instability in the
     model?
    
** TODO analyze results with (lapsing) Bayesian logistic regression

   that way I can get good estimates of the boundary uncertainty both at the
   subject level and across conditions/experiments.  Especially important to
   compare between the supervised and unsupervised conditions.

*** DONE fit brms logistic regression to experiments 1 and 2

    include: intercept, vot, supervised/unsupervised, exposure condition (and
    trial?) 

*** DONE extract subject boundaries from logistic regression fits
    [[file:models/brms_regressions.R::expt1_bounds_bysub%20<-][subject boundaries]]

*** DONE plot subject curves and overall curves
    [[file:models/brms_regressions.R::expt1_bounds%20%25>%25][plot at block 3]]
*** TODO fit brms logistic regression to experiment 3

*** TODO (maybe) fix lapsing logistic regression to expts 1, 2, and 3
   
* more literature to consider

** TODO L2 learning

   Baese Berk stuff?  (I think some of that is on dist learning of prevoiced VOT
   categories)

** TODO Diehl papers
** TODO Holt follow-ups to Idemaru and Holt 2011
** Chladkova et al. 2017: unsupervised "recalibration"

   Do lexically driven re-tuning with non-words.  Shifted i/e and good e/i.
** Schweinhart, Shafto, and Essock 2017
   Adapt to orientation distributions, basically the same finding as Idemaru and
   Holt (2011).
** Language learning review anon
   Focused I think on learning _non-native_ contrasts via DL...
   """
   The literature suggests that adults, too, can track the probability
   distributions of sounds in the ambient language (Maye & Gerken, 2001;
   Hayes-Harb, 2007; Clayards, Tanenhaus, Aslin, & Jacobs, 2008; Goudbeek,
   Cutler, & Smits, 2008; Escudero, Benders, & Wanrooij, 2011). For instance,
   Maye and Gerken (2001) showed that listeners exposed to a bimodal
   distribution along the voice-onset time continuum between [d] and [dÌ¥ ]
   (i.e. prevoiced and voiceless unaspirated alveolar plosives) subsequently
   discriminated this non-native contrast better than listeners exposed to a
   unimodal distribution on the same continuum. However, a number of recent
   studies failed to find the expected distributional training effects (Wanrooij
   et al., 2014a; Wanrooij, de Vos, & Boersma, 2015; partly Ong, Burnham, &
   Escudero, 2015). In principle, adults are sensitive to distributional
   statistics across modalities (Love, 2003; Garrido, Tang, Taylor, Rowe, &
   Mattingley, 2016)
   """
* DONE decide which data goes in this paper
  One of the issues I keep coming back to is the question of *which data to
  discuss in this paper*.  A priori, my strong preferences is *all of it*.

  There are two main sticking points for me:
  1. The -10 /b/ VOT condition was added after the fact, and there's only
     unsupervised data on that one.  So if the supervised data is included,
     there's a discrepancy.
  2. If the -10 condition is excluded from the belief updating model, the
     inferred prior changes, in a rather dramatic way: switches from a "shift"
     (lower mean confidence) to a "scale" (lower var confidence) solution.

  I actually don't think *1.* is that big of a deal: learning in the large
  positive shifts is also incomplete, so the supervised conditions still provide
  a reasonable test of the hypothesis that what's blocking learning is the lack
  of labels.

  And *2.* is only a problem inasmuch as the goal is to draw strong conclusions
  based on the content of the inferred prior.  I think it's really better as a
  sanity check here: _can_ the constraints we see be explained by belief
  updating based on a common starting point??  Either way the answer is gonna be
  yes (there's no reason that the fits to a restricted subset of the data would
  fit _worse_ on those data than a fit to a superset).

  So there's no reason to give up my prior preferences to include all the data.
  Just need to be honest about how the data was collected and the caveats about
  the interpretation of the inferred prior.  But there are already _lots_ of
  caveates needed there anyway (assuming direct access to the statistical
  properties of the clusters/labels are known, constraints on the confidence
  parameters being equal across categories, etc.).  So.

  *decision*: experiment 1 is all conditions.  experiment 2 is all supervised
  (which doesn't include -10 shift).  modeling is all conditions from
  experiment 1.  experiment 3 is all experiment 3 conditions
* Outline

** Intro
  
*** Lots of flexibility in adult language

    recal, dist learning, etc.

*** what _are_ the constraints?

*** goal

    systematically explore a range of distributions that vary in their similarity
    to a typical talker's distributions; see what (English-speaking) adults
    manage to learn and what they do not.

    In this paper I explore a simple hypothesis: people a harder time learning
    distributions that are less similar to the distributions that they've
    previously encountered.  
    
*** preview

    people have a harder time adapting to distributions that deviate a lot from
    what's typical of American English.  This happens even in the presence of
    labeling information that tells them which VOTs are /b/ and which are /p/.

    The pattern of adaptation across conditions is consistent with statistically
    optimal distributional learning starting from a shared set of prior beliefs,
    and the shared prior beliefs are reasonably similar to what a typical talker
    of American English produces.
    
** Experiment 1
   
   A bunch of VOT distributions which vary in terms of similarity to standard
   American English VOT distributions.  Only going to vary the means of these
   distributions (keep things tractable).

*** Results

*** Discussion    

    Distributional learning is incomplete: in more extreme conditions, produced
    boundaries don't get all the way to the ideal boundary for the exposure
    distributions.  

    Why?

    One possible explanation: "shrinkage" towards a typical talker's boundary.

    Another possible explanation: this is an unsupervised task, so maybe
    listeners aren't picking up on the underlying distributions.  Test that in
    Experiment 2

** Experiment 2

   Same distributions, but half the trials are labeled.  Always one voiced and
   one voiceless, but on labeled trials the options are not minimal pairs, so
   only one is a plausible option.

*** Results

    Are boundaries different with supervision?

    Still don't get all the way there, even with supervised teaching signal.

*** Discussion

    Suggests that the constraint comes from something about the distributions
    themselves.  As above, one possible source is prior experience with a
    typical talker.  Explore this possibility with modeling.

** Modeling

   two questions: is the pattern of adaptation consistent with distributional
   Learning starting from a single (shared) set of prior beliefs?  And if so,
   are those shared prior beliefs similar to what a typical talker of American
   English produces?

   Model belief updating process.  Because we didn't find large differences
   between the supervised and unsupervised conditions, we're going to treat this
   as a supervised learning process because it's /much/ simpler to model.


** Experiment 3

** Discussion   

*** Ideal adapter: learn where to be flexible
    Under this interpretation, people don't become less flexible _per se_ as
    they enter adulthood.  Rather, they become _smarter_ about where to be
    flexible, because this allows them to learn more efficiently.

* notes/snippets

** Another intro
   One of the basic facts about human language is that a neurotypical human
   infant can learn any of the many and varied languages spoken on Earth.  That
   is, the language system is incredibly flexible during development.  However,
   at a certain point people lose the ability to learn other languages to
   native-like proficiency.  Nevertheless, recent research has shown that adults
   still remain the ability to learn new variants of their _native_ language.
   That is, the language system seems to remain plastic at least into adulthood,
   even if the form of this plasticity changes.
  
   One of the most remarkable forms of plasticity in adult speech perception is
   /distributional learning/.  Listeners adjust their representations of phonetic
   categories with mere exposure to distributions of acoustic stimuli.  This
   occurs without explicit instruction or labels.

   Suggests that listeners are closely attuned to the statistical patterns in
   their language environment, and pick up on changes in these contingencies to
   more effectively recover a talker's intended message from noisy, variable, and
   ambiguous acoustic signals.

   A comprehensive understanding of distributional learning requires that we
   understand not only how and when it does happen but also how and when it does
   _not_ happen, or is only incomplete.  That goal of this paper is to
   systematically probe the limits of distributional learning for speech in one
   phonetic system: word-initial stop voicing in American English.


   This paper addresses the /limits/ of distributional learning in adults.  What
   sort of speech statistics can adult speakers of American English /not/ learn?

** Framing
   What are all the things in play here?

*** acquisition: lots of flexibility there!
    babies can learn any language, and they do it without labeled data.

    ...gradually lose some of that flexibility.  harder to learn new phonetic
    categories, lose the ability to discriminate non-native phonetic contrasts.

*** remarkable flexibility in adult language
    however, more recent work has shown that adult listeners nevertheless
    maintain a great deal of flexibility /within/ their native language.
    
    adapt to unfamiliar talkers/accents with very little exposure.  accent
    adaptation (increase in comprehension accuracy and speed) CITE.
    recalibration: rapid changes in category boundaries with a few labeled
    examplars
    
*** distributional learning
    both acquisition and adaptation have been theorized to be forms of
    distributional learning.

    both adults and infants are sensitive to the distributional properties of
    speech [@Maye2000; @Maye2002]

    e.g. bimodal input distribution, better able to discriminate [@Maye2000;
    @Pajak2014; @Perfors2010].

    in adults, sensitive to changes in the underlying distributions of phonetic
    cues associated with phonetic categories.  vowels, stop consonants,
    (fricatives?).  pick up on both the /mean/ and /variance/ of distributions.

    Even recalibration can be treated as a form of distributional learning,
    where listeners update their beliefs about the underlying distributions of
    cues based on labeled examples from phonetic categories [@Kleinschmidt2015;
    @Kleinschmidt2016].
    
*** clear constraints on what adults can learn
    Despite this flexibility, there are clear constraints on what adults can
    learn.  Listeners struggle to learn new categories that are not present in
    their native language.  For instance, Japanese listeners struggle to
    discriminate the English /r/-/l/ contrast, which corresponds a single
    category in their native language [@Goto1971; @Myawaki1975].  While
    perception of this contrast can be improved somewhat by training, the amount
    of training is extensive and these listeners seldom achieve native levels of
    performance [@Bradlow1997]

    e.g. japanese r-l [@Goto1971; @Myawaki1975; @Bradlow1997], prevoicing (Melissa BB)?

*** goals: systematically probe constraints on distributional learning 
    from the lens of distributional learningp, the major difference between
    acquisition and adaptation is that distributional learning in adulthoot
    is---in many cases---/constrained/, while it is relatively /unconstrained/
    during acquisition.  However, we lack a clear understanding of the nature
    and source of those constraints.  

    There are some clues: (( paragraph from diss chapter on @Idemaru2011 and
    @Sumner2011? ))

    The goal of this paper is to systematically probe the constraints on
    distributional learning in English-speaking adults.  Experiment 1 tests the
    ability of American English listeners to change their classification of word
    initial stop voicing based on experience with a range of distributions of
    voice-onset time (VOT).  I find that distributional learning is more
    complete when the experimental distributions are more similar to those of a
    typical American English talker, suggesting that prior experience with other
    talkers may constrain distributional learning.  Experiment 2 tests another
    possible constraint on distributional learning, which is the absence of
    /labels/, which could lead to uncertainty about whether the bimodal
    distribution really corresponds to the standard English categories of voiced
    and voiceless stops of /b/ and /p/.  Surprisingly, telling listeners whether
    a particular VOT was intended to be a voiced /b/ or a voiceless /p/ on half
    of the trials has no effect on the speed or completeness of distributional
    learning.  Experiment 3 uses a Bayesian belief-updating model to test
    whether the constraints observed in Experiment 1 can be explained as belief
    updating starting from a common set of prior beliefs that is shared by all
    of the subjects.

    Together, these results show that distributional learning in adults /is/
    constrained, and these constraints are at least consistent with belief
    updating starting from a set of prior beliefs about the VOT distributions
    that a typical talker of American English will produce.
    
**** preview
     Developmental trajectory may better be thought of as a *change* in the kind
     of flexibility that listeners have, rather than a *loss* of plasticity.

*** need to balance stability and flexibility
    if you are too stable, can't deal with changes (e.g., unfamiliar talkers and
    accents).  but if you're too flexible, then you end up overly sensitive to
    meaningless variation that doesn't predict what's going to happen in the
    future.

    I think this is better for the *discussion*: why are these constraints
    there?  For the introduction we want at most to gesture towards this
    story...

** for discussion
   are these results anti-bayesian?  no...learning is inference, guided by prior
   experience.
